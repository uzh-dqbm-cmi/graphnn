{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://scikit-learn.org/stable/_images/grid_search_workflow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score, matthews_corrcoef, precision_recall_curve, auc\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import SGD\n",
    "\n",
    "import skorch\n",
    "from skorch import NeuralNetClassifier\n",
    "from skorch.callbacks import EpochScoring\n",
    "from skorch.callbacks import TensorBoard\n",
    "from skorch.helper import predefined_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import configurations (file paths, etc.)\n",
    "import yaml\n",
    "try:\n",
    "    from yaml import CLoader as Loader, CDumper as Dumper\n",
    "except ImportError:\n",
    "    from yaml import Loader, Dumper\n",
    "    \n",
    "configFile = '../cluster/data/medinfmk/ddi/config/config.yml'\n",
    "\n",
    "with open(configFile, 'r') as ymlfile:\n",
    "    cfg = yaml.load(ymlfile, Loader=Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathInput = cfg['filePaths']['dirRaw']\n",
    "pathOutput = cfg['filePaths']['dirProcessed']\n",
    "# path to store python binary files (pickles)\n",
    "# in order not to recalculate them every time\n",
    "pathPickles = cfg['filePaths']['dirProcessedFiles']['dirPickles']\n",
    "pathRuns = cfg['filePaths']['dirProcessedFiles']['dirRuns']\n",
    "pathPaperScores = cfg['filePaths']['dirRawFiles']['paper-individual-metrics-scores']\n",
    "datasetDirs = cfg['filePaths']['dirRawDatasets']\n",
    "DS1_path = str(datasetDirs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(input_fea, input_lab, seperate=False):\n",
    "    offside_sim_path = input_fea\n",
    "    drug_interaction_matrix_path = input_lab\n",
    "    drug_fea = np.loadtxt(offside_sim_path,dtype=float,delimiter=\",\")\n",
    "    interaction = np.loadtxt(drug_interaction_matrix_path,dtype=int,delimiter=\",\")\n",
    "    \n",
    "    train = []\n",
    "    label = []\n",
    "    tmp_fea=[]\n",
    "    drug_fea_tmp = []\n",
    "            \n",
    "    for i in range(0, (interaction.shape[0]-1)):\n",
    "        for j in range((i+1), interaction.shape[1]):\n",
    "            label.append(interaction[i,j])\n",
    "            drug_fea_tmp_1 = list(drug_fea[i])\n",
    "            drug_fea_tmp_2 = list(drug_fea[j])\n",
    "            if seperate:\n",
    "                 tmp_fea = (drug_fea_tmp_1,drug_fea_tmp_2)\n",
    "            else:\n",
    "                 tmp_fea = drug_fea_tmp_1 + drug_fea_tmp_2\n",
    "            train.append(tmp_fea)\n",
    "\n",
    "    return np.array(train), np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_array_format(data):\n",
    "    formated_matrix1 = []\n",
    "    formated_matrix2 = []\n",
    "    for val in data:\n",
    "        formated_matrix1.append(val[0])\n",
    "        formated_matrix2.append(val[1])\n",
    "    return np.array(formated_matrix1), np.array(formated_matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_labels(labels, encoder=None, categorical=True):\n",
    "    if not encoder:\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(labels)\n",
    "        y = encoder.transform(labels).astype(np.int32)\n",
    "    if categorical:\n",
    "        y = np_utils.to_categorical(y)\n",
    "    return y, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_names(labels, encoder=None, categorical=True):\n",
    "    if not encoder:\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(labels)\n",
    "    if categorical:\n",
    "        labels = np_utils.to_categorical(labels)\n",
    "    return labels, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStratifiedKFoldSplit(X,y,n_splits):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, random_state=42)\n",
    "    return skf.split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NDD(nn.Module):\n",
    "    def __init__(self, D_in=1096, H1=300, H2=400, D_out=2, drop=0.5):\n",
    "        super(NDD, self).__init__()\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(D_in, H1) # Fully Connected\n",
    "        self.fc2 = nn.Linear(H1, H2)\n",
    "        self.fc3 = nn.Linear(H2, D_out)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self._init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if(isinstance(m, nn.Linear)):\n",
    "                m.weight.data.normal_(0, 0.05)\n",
    "                m.bias.data.uniform_(-1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateSimilarityDFSingleMetric(df, sim_type, metric, value):\n",
    "    df.loc[df['Similarity'] == sim_type, metric ] = round(value,3)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateSimilarityDF(df, sim_type, AUROC, AUPR, F1, Rec, Prec):\n",
    "    df = updateSimilarityDFSingleMetric(df, sim_type, 'AUC', AUROC)\n",
    "    df = updateSimilarityDFSingleMetric(df, sim_type, 'AUPR', AUPR)\n",
    "    df = updateSimilarityDFSingleMetric(df, sim_type, 'F-measure', F1)\n",
    "    df = updateSimilarityDFSingleMetric(df, sim_type, 'Recall', Rec)\n",
    "    df = updateSimilarityDFSingleMetric(df, sim_type, 'Precision', Prec)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNetParamsStr(net, str_hidden_layers_params, net_params_to_print=[\"max_epochs\", \"batch_size\"]):\n",
    "    net_params = [val for sublist in [[x,net.get_params()[x]] for x in net_params_to_print] for val in sublist]\n",
    "    net_params_str = '-'.join(map(str, net_params))\n",
    "    return(net_params_str+str_hidden_layers_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeReplicatedIndividualScoresCSV(net, df, destination, str_hidden_layers_params):\n",
    "    filePath = destination + \"replicatedIndividualScores_\" + getNetParamsStr(net, str_hidden_layers_params) + \".csv\"\n",
    "    df.to_csv(path_or_buf = filePath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNDDClassifier(D_in, H1, H2, D_out, drop, Xy_test):\n",
    "    model = NDD(D_in, H1, H2, D_out, drop)\n",
    "    \n",
    "    net = NeuralNetClassifier(\n",
    "        model,\n",
    "        criterion=nn.CrossEntropyLoss,\n",
    "        max_epochs=20,\n",
    "        optimizer=SGD,\n",
    "        optimizer__lr=0.01,\n",
    "        optimizer__momentum=0.9,    \n",
    "        optimizer__weight_decay=1e-6,    \n",
    "        optimizer__nesterov=True,    \n",
    "        batch_size=200,\n",
    "        callbacks=callbacks,\n",
    "        # Shuffle training data on each epoch\n",
    "        iterator_train__shuffle=True,\n",
    "        device=device,\n",
    "        train_split=predefined_split(Xy_test),\n",
    "    )\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgMetrics(AUROC, AUPR, F1, Rec, Prec, kfold_nsplits):\n",
    "    AUROC /= kfold_nsplits\n",
    "    AUPR /= kfold_nsplits\n",
    "    F1 /= kfold_nsplits\n",
    "    Rec /= kfold_nsplits\n",
    "    Prec /= kfold_nsplits\n",
    "    return AUROC, AUPR, F1, Rec, Prec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paperIndividualScores = pd.read_csv(pathPaperScores)\n",
    "\n",
    "df_replicatedIndividualScores = df_paperIndividualScores.copy()\n",
    "# Copy scores table and set them to 0\n",
    "for col in df_replicatedIndividualScores.columns:\n",
    "    if col != 'Similarity':\n",
    "        df_replicatedIndividualScores[col].values[:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "soft = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6288\u001b[0m       \u001b[32m0.6758\u001b[0m        \u001b[35m0.6280\u001b[0m  2.5137\n",
      "      2        \u001b[36m0.6216\u001b[0m       0.6758        0.6286  2.4360\n",
      "      3        \u001b[36m0.6162\u001b[0m       0.6758        0.6333  2.3867\n",
      "      4        \u001b[36m0.6096\u001b[0m       0.6758        0.6331  2.3735\n",
      "      5        \u001b[36m0.6029\u001b[0m       \u001b[32m0.6803\u001b[0m        \u001b[35m0.6245\u001b[0m  2.4230\n",
      "      6        \u001b[36m0.5970\u001b[0m       0.6765        \u001b[35m0.6192\u001b[0m  2.4200\n",
      "      7        \u001b[36m0.5963\u001b[0m       0.6780        0.6232  2.3889\n",
      "      8        \u001b[36m0.5921\u001b[0m       \u001b[32m0.6917\u001b[0m        \u001b[35m0.6114\u001b[0m  2.3374\n",
      "      9        0.5940       0.6810        0.6205  2.7024\n",
      "     10        \u001b[36m0.5917\u001b[0m       0.6792        0.6158  2.5809\n",
      "     11        \u001b[36m0.5903\u001b[0m       0.6775        0.6162  2.4978\n",
      "     12        0.5954       0.6759        0.6237  2.5894\n",
      "     13        0.5979       0.6831        0.6178  2.4727\n",
      "     14        0.5922       0.6809        0.6214  2.5226\n",
      "     15        0.5949       0.6761        0.6365  2.4232\n",
      "     16        0.5947       0.6762        0.6125  2.4262\n",
      "     17        0.5946       0.6758        0.6197  2.3998\n",
      "     18        0.5958       0.6793        0.6167  2.3995\n",
      "     19        0.5998       0.6811        0.6144  2.4107\n",
      "     20        0.6008       0.6819        0.6224  2.3924\n",
      "0 chem 0.5157355805151324 0.41326225324747734 0.0807944465869649 0.04312030462076773 0.6396946564885496\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6307\u001b[0m       \u001b[32m0.6758\u001b[0m        \u001b[35m0.6283\u001b[0m  2.4161\n",
      "      2        \u001b[36m0.6213\u001b[0m       0.6758        \u001b[35m0.6239\u001b[0m  2.4540\n",
      "      3        \u001b[36m0.6149\u001b[0m       0.6758        \u001b[35m0.6237\u001b[0m  2.3920\n",
      "      4        \u001b[36m0.6084\u001b[0m       0.6753        0.6256  2.2019\n",
      "      5        \u001b[36m0.6029\u001b[0m       0.6758        \u001b[35m0.6223\u001b[0m  2.2788\n",
      "      6        \u001b[36m0.6014\u001b[0m       0.6738        0.6237  2.5128\n",
      "      7        \u001b[36m0.6003\u001b[0m       0.6755        0.6264  2.2556\n",
      "      8        0.6016       0.6720        0.6232  2.4560\n",
      "      9        \u001b[36m0.5966\u001b[0m       0.6756        \u001b[35m0.6188\u001b[0m  2.4751\n",
      "     10        0.5967       0.6742        0.6214  2.4354\n",
      "     11        \u001b[36m0.5925\u001b[0m       0.6758        0.6230  2.4036\n",
      "     12        0.5984       0.6758        0.6310  2.8152\n",
      "     13        0.5990       \u001b[32m0.6760\u001b[0m        0.6241  2.3451\n",
      "     14        0.6010       0.6758        0.6220  2.4839\n",
      "     15        0.5972       0.6743        0.6248  2.4173\n",
      "     16        0.5981       0.6758        0.6453  2.4128\n",
      "     17        0.5977       \u001b[32m0.6772\u001b[0m        \u001b[35m0.6183\u001b[0m  2.4119\n",
      "     18        0.6021       0.6758        0.6206  2.5250\n",
      "     19        0.6038       0.6761        0.6312  2.4166\n",
      "     20        0.5990       0.6764        \u001b[35m0.6179\u001b[0m  2.3922\n",
      "1 chem 1.0170387492469586 0.792816075499752 0.08755258707437018 0.046516414531233924 1.3131640442436516\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6312\u001b[0m       \u001b[32m0.6758\u001b[0m        \u001b[35m0.6270\u001b[0m  2.3983\n",
      "      2        \u001b[36m0.6209\u001b[0m       0.6758        \u001b[35m0.6258\u001b[0m  2.4249\n",
      "      3        \u001b[36m0.6149\u001b[0m       0.6758        \u001b[35m0.6252\u001b[0m  2.6246\n",
      "      4        \u001b[36m0.6085\u001b[0m       0.6758        0.6380  2.5128\n",
      "      5        \u001b[36m0.6020\u001b[0m       0.6758        0.6364  2.5620\n",
      "      6        \u001b[36m0.5967\u001b[0m       0.6753        0.6475  3.1285\n",
      "      7        \u001b[36m0.5944\u001b[0m       \u001b[32m0.6766\u001b[0m        \u001b[35m0.6205\u001b[0m  3.2054\n",
      "      8        \u001b[36m0.5939\u001b[0m       0.6678        0.6321  2.8036\n",
      "      9        0.5959       0.6760        0.6293  2.7025\n",
      "     10        0.5981       \u001b[32m0.6767\u001b[0m        0.6345  2.4349\n",
      "     11        0.5973       0.6759        0.6264  2.5231\n",
      "     12        0.5992       0.6755        0.6361  2.4585\n",
      "     13        0.6026       0.6755        0.6304  2.4883\n",
      "     14        0.5983       0.6759        \u001b[35m0.6184\u001b[0m  2.4408\n",
      "     15        0.5987       0.6747        0.6228  2.4155\n",
      "     16        0.5951       0.6648        0.6412  2.4168\n",
      "     17        0.5958       0.6653        0.6366  2.4680\n",
      "     18        0.5970       0.6722        0.6247  2.4395\n",
      "     19        \u001b[36m0.5923\u001b[0m       0.6497        0.6511  2.5575\n",
      "     20        0.5956       0.6762        0.6449  2.4493\n",
      "2 chem 1.518140284001404 1.1162930353482083 0.09389984342162652 0.04970669959864156 1.9210071814985534\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6274\u001b[0m       \u001b[32m0.6758\u001b[0m        \u001b[35m0.6284\u001b[0m  2.4471\n",
      "      2        \u001b[36m0.6200\u001b[0m       0.6758        0.6317  2.4101\n",
      "      3        \u001b[36m0.6134\u001b[0m       0.6755        0.6311  2.4334\n",
      "      4        \u001b[36m0.6051\u001b[0m       0.6738        0.6433  2.4104\n",
      "      5        \u001b[36m0.5965\u001b[0m       \u001b[32m0.6769\u001b[0m        0.6323  2.4252\n",
      "      6        \u001b[36m0.5936\u001b[0m       0.6635        0.6464  2.4315\n",
      "      7        \u001b[36m0.5933\u001b[0m       0.6546        0.6505  2.4112\n",
      "      8        0.5950       0.6725        0.6532  2.3817\n",
      "      9        0.5963       0.6756        0.6750  2.3818\n",
      "     10        0.5967       0.6764        0.6456  2.4022\n",
      "     11        0.5967       0.6688        0.6469  2.4150\n",
      "     12        0.5934       0.6763        0.6443  2.4469\n",
      "     13        0.5943       0.6576        0.6599  2.3892\n",
      "     14        0.5975       0.6664        0.6546  2.4476\n",
      "     15        0.5996       0.6762        0.6836  2.5596\n",
      "     16        0.6009       0.6755        0.6463  2.3688\n",
      "     17        0.5936       \u001b[32m0.6777\u001b[0m        0.6829  2.3792\n",
      "     18        \u001b[36m0.5929\u001b[0m       0.6655        0.6564  2.3875\n",
      "     19        \u001b[36m0.5906\u001b[0m       0.6759        0.7201  2.8458\n",
      "     20        0.5930       0.6741        0.6589  2.3879\n",
      "3 chem 2.038653460667467 1.4857140458304658 0.2368822995619774 0.13358032314500362 2.405261489282273\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6290\u001b[0m       \u001b[32m0.6759\u001b[0m        \u001b[35m0.6237\u001b[0m  2.3589\n",
      "      2        \u001b[36m0.6236\u001b[0m       0.6753        \u001b[35m0.6229\u001b[0m  2.3865\n",
      "      3        \u001b[36m0.6176\u001b[0m       0.6662        0.6253  2.3774\n",
      "      4        \u001b[36m0.6106\u001b[0m       0.5910        0.6644  2.3860\n",
      "      5        \u001b[36m0.6028\u001b[0m       0.5174        0.7305  2.3376\n",
      "      6        \u001b[36m0.5939\u001b[0m       0.4798        0.7819  2.4196\n",
      "      7        \u001b[36m0.5932\u001b[0m       0.4001        0.9972  2.3848\n",
      "      8        \u001b[36m0.5904\u001b[0m       0.4330        0.8442  2.3803\n",
      "      9        0.5965       0.5847        0.6677  2.6298\n",
      "     10        0.5975       0.5533        0.7006  2.4327\n",
      "     11        0.5961       0.6191        0.6523  2.3571\n",
      "     12        0.6008       0.5436        0.6983  2.3796\n",
      "     13        0.6010       0.6248        0.6508  2.4042\n",
      "     14        0.6003       0.5661        0.7196  2.3860\n",
      "     15        0.5984       0.5510        0.7518  2.3789\n",
      "     16        0.6043       0.6032        0.6949  2.3919\n",
      "     17        0.6024       0.5184        0.8207  2.7406\n",
      "     18        0.6009       0.3958        1.3406  2.5029\n",
      "     19        0.5996       0.4931        1.0146  2.4450\n",
      "     20        0.5987       0.4994        0.9194  2.5245\n",
      "4 chem 2.571539365169989 1.8418094442968753 0.6854344089254935 0.7617194750593717 2.754084077302392\n",
      "chem 0.5143078730339978 0.36836188885937504 0.1370868817850987 0.15234389501187434 0.5508168154604783\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6305\u001b[0m       \u001b[32m0.6758\u001b[0m        \u001b[35m0.6299\u001b[0m  2.4204\n",
      "      2        \u001b[36m0.6270\u001b[0m       0.6758        \u001b[35m0.6275\u001b[0m  2.4296\n",
      "      3        \u001b[36m0.6126\u001b[0m       \u001b[32m0.6914\u001b[0m        \u001b[35m0.6127\u001b[0m  2.4034\n",
      "      4        \u001b[36m0.5757\u001b[0m       \u001b[32m0.7095\u001b[0m        \u001b[35m0.5941\u001b[0m  2.3615\n",
      "      5        \u001b[36m0.5262\u001b[0m       0.7087        \u001b[35m0.5822\u001b[0m  2.3619\n",
      "      6        \u001b[36m0.4861\u001b[0m       \u001b[32m0.7155\u001b[0m        0.5848  2.5026\n",
      "      7        \u001b[36m0.4596\u001b[0m       \u001b[32m0.7192\u001b[0m        0.5868  2.4302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      8        \u001b[36m0.4431\u001b[0m       \u001b[32m0.7288\u001b[0m        0.5839  2.6835\n",
      "      9        \u001b[36m0.4297\u001b[0m       0.7266        0.5955  2.4403\n",
      "     10        \u001b[36m0.4185\u001b[0m       \u001b[32m0.7294\u001b[0m        0.5916  2.6645\n",
      "     11        \u001b[36m0.4137\u001b[0m       \u001b[32m0.7331\u001b[0m        0.5960  2.5649\n",
      "     12        \u001b[36m0.4067\u001b[0m       0.7291        0.6038  2.5339\n",
      "     13        \u001b[36m0.4002\u001b[0m       \u001b[32m0.7380\u001b[0m        0.5907  2.4311\n",
      "     14        \u001b[36m0.3957\u001b[0m       \u001b[32m0.7386\u001b[0m        0.5876  2.4629\n",
      "     15        \u001b[36m0.3919\u001b[0m       0.7340        0.6194  2.5578\n",
      "     16        \u001b[36m0.3882\u001b[0m       \u001b[32m0.7411\u001b[0m        0.6015  2.4309\n",
      "     17        \u001b[36m0.3853\u001b[0m       0.7305        0.6290  2.4157\n",
      "     18        \u001b[36m0.3827\u001b[0m       0.7362        0.6128  2.3665\n",
      "     19        \u001b[36m0.3793\u001b[0m       0.7374        0.6263  2.4221\n",
      "     20        \u001b[36m0.3751\u001b[0m       \u001b[32m0.7464\u001b[0m        0.5941  2.5221\n",
      "0 target 0.6565979518614613 0.6137976370772845 0.5063636363636363 0.4012555315426572 0.6860812950906211\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6339\u001b[0m       \u001b[32m0.6758\u001b[0m        \u001b[35m0.6296\u001b[0m  2.5247\n",
      "      2        \u001b[36m0.6269\u001b[0m       \u001b[32m0.6761\u001b[0m        \u001b[35m0.6274\u001b[0m  2.4505\n",
      "      3        \u001b[36m0.6132\u001b[0m       \u001b[32m0.6820\u001b[0m        \u001b[35m0.6173\u001b[0m  2.3955\n",
      "      4        \u001b[36m0.5791\u001b[0m       \u001b[32m0.6935\u001b[0m        \u001b[35m0.6079\u001b[0m  2.4003\n",
      "      5        \u001b[36m0.5284\u001b[0m       0.6931        \u001b[35m0.6072\u001b[0m  2.4414\n",
      "      6        \u001b[36m0.4888\u001b[0m       \u001b[32m0.7108\u001b[0m        0.6139  2.3731\n",
      "      7        \u001b[36m0.4611\u001b[0m       0.7078        0.6142  2.5071\n",
      "      8        \u001b[36m0.4424\u001b[0m       \u001b[32m0.7127\u001b[0m        0.6203  2.4785\n",
      "      9        \u001b[36m0.4290\u001b[0m       \u001b[32m0.7160\u001b[0m        0.6188  2.5479\n",
      "     10        \u001b[36m0.4181\u001b[0m       \u001b[32m0.7238\u001b[0m        0.6113  2.3623\n",
      "     11        \u001b[36m0.4121\u001b[0m       0.7214        \u001b[35m0.6040\u001b[0m  2.4057\n",
      "     12        \u001b[36m0.4061\u001b[0m       0.7238        0.6151  2.4488\n",
      "     13        \u001b[36m0.3976\u001b[0m       \u001b[32m0.7253\u001b[0m        0.6102  2.3704\n",
      "     14        \u001b[36m0.3945\u001b[0m       0.7241        0.6397  2.1343\n",
      "     15        \u001b[36m0.3902\u001b[0m       0.7231        0.6298  2.4010\n",
      "     16        \u001b[36m0.3859\u001b[0m       0.7212        0.6594  2.4401\n",
      "     17        \u001b[36m0.3822\u001b[0m       0.7174        0.6625  2.4309\n",
      "     18        \u001b[36m0.3783\u001b[0m       \u001b[32m0.7311\u001b[0m        0.6166  2.4637\n",
      "     19        \u001b[36m0.3760\u001b[0m       0.7231        0.6613  2.4405\n",
      "     20        \u001b[36m0.3724\u001b[0m       0.7262        0.6707  2.4583\n",
      "1 target 1.3024412966014167 1.173412095169509 1.0033912762484192 0.8185654008438819 1.3004752344845605\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6304\u001b[0m       \u001b[32m0.6758\u001b[0m        \u001b[35m0.6297\u001b[0m  2.4269\n",
      "      2        \u001b[36m0.6251\u001b[0m       \u001b[32m0.6769\u001b[0m        \u001b[35m0.6275\u001b[0m  2.4600\n",
      "      3        \u001b[36m0.6106\u001b[0m       \u001b[32m0.6805\u001b[0m        \u001b[35m0.6156\u001b[0m  2.4143\n",
      "      4        \u001b[36m0.5753\u001b[0m       \u001b[32m0.6850\u001b[0m        \u001b[35m0.5936\u001b[0m  2.4181\n",
      "      5        \u001b[36m0.5247\u001b[0m       \u001b[32m0.6952\u001b[0m        \u001b[35m0.5889\u001b[0m  2.5317\n",
      "      6        \u001b[36m0.4830\u001b[0m       0.6900        0.6182  2.4339\n",
      "      7        \u001b[36m0.4573\u001b[0m       0.6866        0.6430  2.3662\n",
      "      8        \u001b[36m0.4411\u001b[0m       0.6821        0.6529  2.4321\n",
      "      9        \u001b[36m0.4289\u001b[0m       0.6921        0.6452  2.4003\n",
      "     10        \u001b[36m0.4198\u001b[0m       0.6884        0.6834  2.5057\n",
      "     11        \u001b[36m0.4119\u001b[0m       0.6791        0.7170  2.4434\n",
      "     12        \u001b[36m0.4055\u001b[0m       0.6829        0.7006  2.3935\n",
      "     13        \u001b[36m0.4013\u001b[0m       0.6855        0.7290  2.4676\n",
      "     14        \u001b[36m0.3964\u001b[0m       0.6904        0.7144  2.4560\n",
      "     15        \u001b[36m0.3910\u001b[0m       0.6886        0.7445  2.4915\n",
      "     16        \u001b[36m0.3870\u001b[0m       0.6871        0.7393  2.4180\n",
      "     17        \u001b[36m0.3847\u001b[0m       0.6895        0.7409  2.3899\n",
      "     18        \u001b[36m0.3809\u001b[0m       0.6851        0.7748  2.3959\n",
      "     19        \u001b[36m0.3770\u001b[0m       0.6861        0.7702  2.4097\n",
      "     20        \u001b[36m0.3736\u001b[0m       0.6925        0.7441  2.4298\n",
      "2 target 1.9282059676677608 1.6768551059429964 1.4823648236144604 1.2546053308634353 1.8317605009422406\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6364\u001b[0m       \u001b[32m0.6758\u001b[0m        \u001b[35m0.6293\u001b[0m  2.4017\n",
      "      2        \u001b[36m0.6273\u001b[0m       \u001b[32m0.6759\u001b[0m        \u001b[35m0.6261\u001b[0m  2.3816\n",
      "      3        \u001b[36m0.6162\u001b[0m       \u001b[32m0.6824\u001b[0m        \u001b[35m0.6109\u001b[0m  2.3763\n",
      "      4        \u001b[36m0.5800\u001b[0m       \u001b[32m0.7026\u001b[0m        \u001b[35m0.5898\u001b[0m  2.4155\n",
      "      5        \u001b[36m0.5268\u001b[0m       0.6921        0.5929  2.8006\n",
      "      6        \u001b[36m0.4822\u001b[0m       0.7024        0.5941  2.4030\n",
      "      7        \u001b[36m0.4519\u001b[0m       0.6765        0.6381  2.3629\n",
      "      8        \u001b[36m0.4332\u001b[0m       0.6701        0.6685  2.3696\n",
      "      9        \u001b[36m0.4202\u001b[0m       0.6634        0.7055  2.4248\n",
      "     10        \u001b[36m0.4098\u001b[0m       0.6511        0.7577  2.4520\n",
      "     11        \u001b[36m0.4027\u001b[0m       0.6535        0.7359  2.3934\n",
      "     12        \u001b[36m0.3956\u001b[0m       0.6447        0.7983  2.6149\n",
      "     13        \u001b[36m0.3887\u001b[0m       0.6394        0.8275  2.3855\n",
      "     14        \u001b[36m0.3831\u001b[0m       0.6462        0.8429  2.6484\n",
      "     15        \u001b[36m0.3784\u001b[0m       0.6371        0.9017  2.4043\n",
      "     16        \u001b[36m0.3758\u001b[0m       0.6433        0.8878  2.4184\n",
      "     17        \u001b[36m0.3706\u001b[0m       0.6340        0.9292  2.4162\n",
      "     18        \u001b[36m0.3687\u001b[0m       0.6419        0.8960  2.6499\n",
      "     19        \u001b[36m0.3632\u001b[0m       0.6295        0.9576  2.3991\n",
      "     20        \u001b[36m0.3631\u001b[0m       0.6339        0.9802  2.3706\n",
      "3 target 2.5342157010349764 2.1365850423598314 1.9649934004305638 1.7814140166718122 2.2770458175741126\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6315\u001b[0m       \u001b[32m0.6759\u001b[0m        \u001b[35m0.6288\u001b[0m  2.5556\n",
      "      2        \u001b[36m0.6281\u001b[0m       0.6759        \u001b[35m0.6247\u001b[0m  2.8183\n",
      "      3        \u001b[36m0.6199\u001b[0m       \u001b[32m0.6789\u001b[0m        \u001b[35m0.6105\u001b[0m  2.9269\n",
      "      4        \u001b[36m0.5912\u001b[0m       \u001b[32m0.7099\u001b[0m        \u001b[35m0.5857\u001b[0m  2.8891\n",
      "      5        \u001b[36m0.5372\u001b[0m       0.7082        \u001b[35m0.5837\u001b[0m  3.0151\n",
      "      6        \u001b[36m0.4893\u001b[0m       0.7048        0.6039  2.9010\n",
      "      7        \u001b[36m0.4601\u001b[0m       0.7014        0.6250  2.5023\n",
      "      8        \u001b[36m0.4403\u001b[0m       0.7017        0.6403  2.5202\n",
      "      9        \u001b[36m0.4284\u001b[0m       0.6964        0.6711  2.5970\n",
      "     10        \u001b[36m0.4164\u001b[0m       0.6969        0.6907  2.7417\n",
      "     11        \u001b[36m0.4081\u001b[0m       0.6861        0.7330  2.6388\n",
      "     12        \u001b[36m0.4014\u001b[0m       0.6856        0.7374  2.7932\n",
      "     13        \u001b[36m0.3964\u001b[0m       0.6938        0.7238  2.6544\n",
      "     14        \u001b[36m0.3899\u001b[0m       0.6868        0.7842  2.4384\n",
      "     15        \u001b[36m0.3858\u001b[0m       0.6834        0.8123  2.4543\n",
      "     16        \u001b[36m0.3817\u001b[0m       0.6877        0.8032  2.5203\n",
      "     17        \u001b[36m0.3775\u001b[0m       0.6924        0.8125  2.4176\n",
      "     18        \u001b[36m0.3734\u001b[0m       0.6836        0.8182  2.6137\n",
      "     19        \u001b[36m0.3718\u001b[0m       0.6797        0.8576  2.4086\n",
      "     20        \u001b[36m0.3678\u001b[0m       0.6914        0.7992  2.7819\n",
      "4 target 3.172201881073872 2.6345852962944813 2.4702092566334204 2.267416486824138 2.803056734361458\n",
      "target 0.6344403762147743 0.5269170592588963 0.4940418513266841 0.4534832973648276 0.5606113468722916\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6230\u001b[0m       \u001b[32m0.6799\u001b[0m        \u001b[35m0.6139\u001b[0m  2.3958\n",
      "      2        \u001b[36m0.6044\u001b[0m       \u001b[32m0.6882\u001b[0m        \u001b[35m0.6073\u001b[0m  2.4283\n",
      "      3        \u001b[36m0.5910\u001b[0m       0.6860        \u001b[35m0.6041\u001b[0m  2.3906\n",
      "      4        \u001b[36m0.5764\u001b[0m       \u001b[32m0.6893\u001b[0m        0.6062  2.4075\n",
      "      5        \u001b[36m0.5629\u001b[0m       \u001b[32m0.6993\u001b[0m        0.6042  2.3715\n",
      "      6        \u001b[36m0.5544\u001b[0m       \u001b[32m0.7020\u001b[0m        0.6073  2.4660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        \u001b[36m0.5462\u001b[0m       \u001b[32m0.7043\u001b[0m        \u001b[35m0.6019\u001b[0m  2.4270\n",
      "      8        \u001b[36m0.5391\u001b[0m       \u001b[32m0.7045\u001b[0m        0.6069  2.4622\n",
      "      9        \u001b[36m0.5353\u001b[0m       0.6958        0.6115  2.4142\n",
      "     10        \u001b[36m0.5310\u001b[0m       \u001b[32m0.7084\u001b[0m        0.6057  2.4120\n",
      "     11        \u001b[36m0.5283\u001b[0m       \u001b[32m0.7148\u001b[0m        \u001b[35m0.5942\u001b[0m  2.4603\n",
      "     12        \u001b[36m0.5249\u001b[0m       \u001b[32m0.7157\u001b[0m        \u001b[35m0.5929\u001b[0m  2.3829\n",
      "     13        \u001b[36m0.5214\u001b[0m       0.7133        0.5993  2.3863\n",
      "     14        \u001b[36m0.5197\u001b[0m       \u001b[32m0.7164\u001b[0m        0.5950  2.3901\n",
      "     15        \u001b[36m0.5179\u001b[0m       \u001b[32m0.7172\u001b[0m        0.5932  2.3744\n",
      "     16        \u001b[36m0.5158\u001b[0m       0.7131        0.5989  2.3756\n",
      "     17        \u001b[36m0.5135\u001b[0m       0.7148        0.5965  2.3782\n",
      "     18        \u001b[36m0.5125\u001b[0m       \u001b[32m0.7186\u001b[0m        \u001b[35m0.5826\u001b[0m  2.4157\n",
      "     19        \u001b[36m0.5113\u001b[0m       \u001b[32m0.7221\u001b[0m        0.5899  2.4187\n",
      "     20        \u001b[36m0.5100\u001b[0m       0.7215        0.5835  2.3807\n",
      "0 transporter 0.6147286663639795 0.5504395027319215 0.4199847126676395 0.3110013378614799 0.6465554129225503\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6194\u001b[0m       \u001b[32m0.6860\u001b[0m        \u001b[35m0.6100\u001b[0m  2.3514\n",
      "      2        \u001b[36m0.6028\u001b[0m       0.6839        0.6107  2.1568\n",
      "      3        \u001b[36m0.5897\u001b[0m       \u001b[32m0.6885\u001b[0m        \u001b[35m0.6055\u001b[0m  2.1396\n",
      "      4        \u001b[36m0.5735\u001b[0m       \u001b[32m0.6960\u001b[0m        \u001b[35m0.5996\u001b[0m  2.2133\n",
      "      5        \u001b[36m0.5607\u001b[0m       \u001b[32m0.6990\u001b[0m        \u001b[35m0.5975\u001b[0m  2.1573\n",
      "      6        \u001b[36m0.5502\u001b[0m       \u001b[32m0.7058\u001b[0m        \u001b[35m0.5931\u001b[0m  2.1985\n",
      "      7        \u001b[36m0.5438\u001b[0m       0.6957        0.5978  2.1891\n",
      "      8        \u001b[36m0.5390\u001b[0m       \u001b[32m0.7064\u001b[0m        0.5960  2.1403\n",
      "      9        \u001b[36m0.5339\u001b[0m       0.6988        0.5937  2.1098\n",
      "     10        \u001b[36m0.5297\u001b[0m       0.6964        \u001b[35m0.5898\u001b[0m  2.2351\n",
      "     11        \u001b[36m0.5253\u001b[0m       0.6683        0.6025  2.4854\n",
      "     12        \u001b[36m0.5249\u001b[0m       0.6872        0.6053  2.3927\n",
      "     13        \u001b[36m0.5213\u001b[0m       0.7060        0.5976  2.5715\n",
      "     14        \u001b[36m0.5183\u001b[0m       0.6930        0.5917  2.4526\n",
      "     15        \u001b[36m0.5167\u001b[0m       0.6771        0.6041  2.4136\n",
      "     16        \u001b[36m0.5146\u001b[0m       \u001b[32m0.7087\u001b[0m        \u001b[35m0.5892\u001b[0m  2.4982\n",
      "     17        \u001b[36m0.5122\u001b[0m       0.6989        0.5918  2.3511\n",
      "     18        \u001b[36m0.5118\u001b[0m       0.6913        0.5998  2.4092\n",
      "     19        \u001b[36m0.5107\u001b[0m       \u001b[32m0.7155\u001b[0m        0.5942  2.3557\n",
      "     20        \u001b[36m0.5082\u001b[0m       \u001b[32m0.7188\u001b[0m        \u001b[35m0.5880\u001b[0m  2.7809\n",
      "1 transporter 1.2426401121835307 1.093150694585189 0.8799360134493901 0.6803540187300607 1.2559967442181863\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6233\u001b[0m       \u001b[32m0.6833\u001b[0m        \u001b[35m0.6099\u001b[0m  2.4246\n",
      "      2        \u001b[36m0.6063\u001b[0m       \u001b[32m0.6917\u001b[0m        \u001b[35m0.6028\u001b[0m  2.3945\n",
      "      3        \u001b[36m0.5951\u001b[0m       \u001b[32m0.6926\u001b[0m        0.6037  2.6206\n",
      "      4        \u001b[36m0.5790\u001b[0m       0.6895        0.6100  2.6140\n",
      "      5        \u001b[36m0.5641\u001b[0m       \u001b[32m0.7012\u001b[0m        0.6220  2.4576\n",
      "      6        \u001b[36m0.5530\u001b[0m       \u001b[32m0.7016\u001b[0m        0.6231  2.3631\n",
      "      7        \u001b[36m0.5449\u001b[0m       \u001b[32m0.7056\u001b[0m        0.6196  2.5658\n",
      "      8        \u001b[36m0.5392\u001b[0m       \u001b[32m0.7068\u001b[0m        0.6453  2.6902\n",
      "      9        \u001b[36m0.5339\u001b[0m       0.7044        0.6362  2.4337\n",
      "     10        \u001b[36m0.5309\u001b[0m       \u001b[32m0.7089\u001b[0m        0.6392  2.4128\n",
      "     11        \u001b[36m0.5269\u001b[0m       0.7078        0.6558  2.3784\n",
      "     12        \u001b[36m0.5236\u001b[0m       \u001b[32m0.7101\u001b[0m        0.6446  2.4140\n",
      "     13        \u001b[36m0.5209\u001b[0m       0.7078        0.6468  2.3762\n",
      "     14        \u001b[36m0.5187\u001b[0m       0.7053        0.6630  2.4718\n",
      "     15        \u001b[36m0.5166\u001b[0m       0.7079        0.6447  2.4009\n",
      "     16        \u001b[36m0.5140\u001b[0m       \u001b[32m0.7108\u001b[0m        0.6459  2.5604\n",
      "     17        \u001b[36m0.5124\u001b[0m       0.7063        0.6770  2.4233\n",
      "     18        \u001b[36m0.5116\u001b[0m       0.7036        0.6637  2.4547\n",
      "     19        \u001b[36m0.5090\u001b[0m       \u001b[32m0.7138\u001b[0m        0.6516  2.4885\n",
      "     20        \u001b[36m0.5078\u001b[0m       0.7131        0.6623  2.5358\n",
      "2 transporter 1.8492750130027322 1.5861169227585712 1.2870242021000142 0.9841514870844912 1.8727957412929879\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6178\u001b[0m       \u001b[32m0.6856\u001b[0m        \u001b[35m0.6164\u001b[0m  2.3849\n",
      "      2        \u001b[36m0.6013\u001b[0m       \u001b[32m0.6933\u001b[0m        \u001b[35m0.6107\u001b[0m  2.4029\n",
      "      3        \u001b[36m0.5838\u001b[0m       0.6874        0.6118  2.3818\n",
      "      4        \u001b[36m0.5644\u001b[0m       0.6857        0.6150  2.4492\n",
      "      5        \u001b[36m0.5491\u001b[0m       0.6727        0.6243  2.4943\n",
      "      6        \u001b[36m0.5388\u001b[0m       0.6762        0.6361  2.3841\n",
      "      7        \u001b[36m0.5301\u001b[0m       0.6809        0.6269  2.4000\n",
      "      8        \u001b[36m0.5248\u001b[0m       0.6714        0.6410  2.4906\n",
      "      9        \u001b[36m0.5193\u001b[0m       0.6615        0.6606  2.4322\n",
      "     10        \u001b[36m0.5143\u001b[0m       0.6513        0.6738  2.4108\n",
      "     11        \u001b[36m0.5109\u001b[0m       0.6620        0.6657  2.4048\n",
      "     12        \u001b[36m0.5063\u001b[0m       0.6573        0.6806  2.3940\n",
      "     13        \u001b[36m0.5034\u001b[0m       0.6660        0.6518  2.5498\n",
      "     14        \u001b[36m0.5003\u001b[0m       0.6605        0.6834  2.4240\n",
      "     15        \u001b[36m0.4977\u001b[0m       0.6668        0.6815  2.4967\n",
      "     16        \u001b[36m0.4956\u001b[0m       0.6550        0.7062  2.3850\n",
      "     17        \u001b[36m0.4935\u001b[0m       0.6661        0.6967  2.4214\n",
      "     18        \u001b[36m0.4934\u001b[0m       0.6696        0.6981  2.4897\n",
      "     19        \u001b[36m0.4907\u001b[0m       0.6661        0.7037  2.4519\n",
      "     20        \u001b[36m0.4887\u001b[0m       0.6565        0.7279  2.3343\n",
      "3 transporter 2.418387252293 2.0014080407969264 1.6640059278654746 1.3047236801481938 2.330278484720804\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6219\u001b[0m       \u001b[32m0.6818\u001b[0m        \u001b[35m0.6139\u001b[0m  2.4362\n",
      "      2        \u001b[36m0.6019\u001b[0m       0.6674        0.6300  2.3988\n",
      "      3        \u001b[36m0.5848\u001b[0m       0.6366        0.6796  2.4167\n",
      "      4        \u001b[36m0.5675\u001b[0m       0.6350        0.7225  2.4941\n",
      "      5        \u001b[36m0.5521\u001b[0m       0.6357        0.8000  2.3577\n",
      "      6        \u001b[36m0.5429\u001b[0m       0.6381        0.8228  2.4201\n",
      "      7        \u001b[36m0.5349\u001b[0m       0.6402        0.8942  2.4412\n",
      "      8        \u001b[36m0.5289\u001b[0m       0.6421        0.8686  2.4895\n",
      "      9        \u001b[36m0.5246\u001b[0m       0.6440        0.8745  2.4154\n",
      "     10        \u001b[36m0.5198\u001b[0m       0.6426        0.9010  2.4480\n",
      "     11        \u001b[36m0.5166\u001b[0m       0.6473        0.9013  2.5851\n",
      "     12        \u001b[36m0.5148\u001b[0m       0.6395        0.9526  2.3792\n",
      "     13        \u001b[36m0.5113\u001b[0m       0.6446        0.9305  2.3908\n",
      "     14        \u001b[36m0.5092\u001b[0m       0.6443        1.0148  2.4037\n",
      "     15        \u001b[36m0.5069\u001b[0m       0.6458        1.0076  2.3767\n",
      "     16        \u001b[36m0.5048\u001b[0m       0.6450        1.0221  2.4215\n",
      "     17        \u001b[36m0.5035\u001b[0m       0.6448        0.9945  2.3930\n",
      "     18        \u001b[36m0.5021\u001b[0m       0.6459        0.9896  2.3862\n",
      "     19        \u001b[36m0.5006\u001b[0m       0.6450        0.9621  2.4089\n",
      "     20        \u001b[36m0.4993\u001b[0m       0.6489        1.0177  2.3784\n",
      "4 transporter 3.02415606510492 2.4184980679052934 2.1354804156115574 1.7878443059201166 2.790655103002483\n",
      "transporter 0.6048312130209841 0.4836996135810587 0.4270960831223115 0.3575688611840233 0.5581310206004966\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6170\u001b[0m       \u001b[32m0.6847\u001b[0m        \u001b[35m0.6076\u001b[0m  2.3989\n",
      "      2        \u001b[36m0.5982\u001b[0m       \u001b[32m0.6927\u001b[0m        \u001b[35m0.6012\u001b[0m  2.3779\n",
      "      3        \u001b[36m0.5728\u001b[0m       \u001b[32m0.6988\u001b[0m        \u001b[35m0.5986\u001b[0m  2.3953\n",
      "      4        \u001b[36m0.5462\u001b[0m       \u001b[32m0.7186\u001b[0m        \u001b[35m0.5815\u001b[0m  2.3788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m0.5234\u001b[0m       \u001b[32m0.7194\u001b[0m        0.5890  2.4436\n",
      "      6        \u001b[36m0.5090\u001b[0m       \u001b[32m0.7249\u001b[0m        \u001b[35m0.5780\u001b[0m  2.5653\n",
      "      7        \u001b[36m0.4974\u001b[0m       0.7246        \u001b[35m0.5772\u001b[0m  2.4674\n",
      "      8        \u001b[36m0.4906\u001b[0m       0.7174        0.6089  2.4034\n",
      "      9        \u001b[36m0.4824\u001b[0m       0.7148        0.6068  2.3777\n",
      "     10        \u001b[36m0.4770\u001b[0m       0.7215        \u001b[35m0.5733\u001b[0m  2.3916\n",
      "     11        \u001b[36m0.4737\u001b[0m       0.7184        0.5771  2.3716\n",
      "     12        \u001b[36m0.4681\u001b[0m       0.7241        0.5911  2.5683\n",
      "     13        \u001b[36m0.4656\u001b[0m       0.7193        0.5812  2.5440\n",
      "     14        \u001b[36m0.4619\u001b[0m       0.7136        0.5896  2.4756\n",
      "     15        \u001b[36m0.4601\u001b[0m       \u001b[32m0.7261\u001b[0m        0.5861  2.8922\n",
      "     16        \u001b[36m0.4574\u001b[0m       0.7183        0.5787  2.7993\n",
      "     17        \u001b[36m0.4549\u001b[0m       0.7210        0.5915  2.5410\n",
      "     18        \u001b[36m0.4529\u001b[0m       0.7208        0.6122  2.4961\n",
      "     19        \u001b[36m0.4523\u001b[0m       \u001b[32m0.7278\u001b[0m        0.5925  2.4297\n",
      "     20        \u001b[36m0.4501\u001b[0m       0.7264        0.5849  2.3991\n",
      "0 enzyme 0.6220517469501661 0.5673921355285172 0.43526170798898073 0.3252032520325203 0.6579221320008328\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6211\u001b[0m       \u001b[32m0.6640\u001b[0m        \u001b[35m0.6223\u001b[0m  2.3826\n",
      "      2        \u001b[36m0.5966\u001b[0m       \u001b[32m0.6691\u001b[0m        \u001b[35m0.6203\u001b[0m  2.4921\n",
      "      3        \u001b[36m0.5736\u001b[0m       \u001b[32m0.6812\u001b[0m        \u001b[35m0.6182\u001b[0m  2.4414\n",
      "      4        \u001b[36m0.5489\u001b[0m       \u001b[32m0.6841\u001b[0m        \u001b[35m0.6070\u001b[0m  2.5522\n",
      "      5        \u001b[36m0.5262\u001b[0m       \u001b[32m0.6989\u001b[0m        \u001b[35m0.6054\u001b[0m  2.4973\n",
      "      6        \u001b[36m0.5100\u001b[0m       0.6869        0.6116  2.3296\n",
      "      7        \u001b[36m0.4995\u001b[0m       \u001b[32m0.7023\u001b[0m        \u001b[35m0.5955\u001b[0m  2.6548\n",
      "      8        \u001b[36m0.4901\u001b[0m       0.6892        0.6085  2.5071\n",
      "      9        \u001b[36m0.4831\u001b[0m       0.6993        0.6130  2.3163\n",
      "     10        \u001b[36m0.4782\u001b[0m       0.7001        0.6077  2.5790\n",
      "     11        \u001b[36m0.4741\u001b[0m       0.6880        0.6259  2.3848\n",
      "     12        \u001b[36m0.4703\u001b[0m       \u001b[32m0.7035\u001b[0m        0.6126  2.6073\n",
      "     13        \u001b[36m0.4667\u001b[0m       0.6924        0.6244  2.4415\n",
      "     14        \u001b[36m0.4640\u001b[0m       0.6890        0.6253  2.4661\n",
      "     15        \u001b[36m0.4614\u001b[0m       \u001b[32m0.7128\u001b[0m        0.6097  2.4304\n",
      "     16        \u001b[36m0.4589\u001b[0m       0.6893        0.6254  2.7028\n",
      "     17        \u001b[36m0.4565\u001b[0m       0.6940        0.6239  2.4877\n",
      "     18        \u001b[36m0.4550\u001b[0m       0.7102        0.6187  2.3595\n",
      "     19        \u001b[36m0.4521\u001b[0m       0.6957        0.6257  2.3965\n",
      "     20        \u001b[36m0.4519\u001b[0m       0.6922        0.6431  2.5427\n",
      "1 enzyme 1.2405543488148332 1.0778651756165194 0.8980629654112651 0.7342801276114027 1.190692538112614\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6230\u001b[0m       \u001b[32m0.6815\u001b[0m        \u001b[35m0.6103\u001b[0m  2.4680\n",
      "      2        \u001b[36m0.6051\u001b[0m       \u001b[32m0.6869\u001b[0m        \u001b[35m0.6045\u001b[0m  2.3797\n",
      "      3        \u001b[36m0.5855\u001b[0m       0.6815        \u001b[35m0.6023\u001b[0m  2.4556\n",
      "      4        \u001b[36m0.5608\u001b[0m       0.6778        0.6079  2.4868\n",
      "      5        \u001b[36m0.5383\u001b[0m       0.6820        0.6121  2.4455\n",
      "      6        \u001b[36m0.5203\u001b[0m       0.6796        0.6209  2.4279\n",
      "      7        \u001b[36m0.5075\u001b[0m       0.6854        0.6313  2.3974\n",
      "      8        \u001b[36m0.4999\u001b[0m       \u001b[32m0.6893\u001b[0m        0.6427  2.3944\n",
      "      9        \u001b[36m0.4945\u001b[0m       \u001b[32m0.7011\u001b[0m        0.6380  3.0003\n",
      "     10        \u001b[36m0.4858\u001b[0m       0.6871        0.6632  2.3937\n",
      "     11        \u001b[36m0.4824\u001b[0m       0.6800        0.6555  2.4183\n",
      "     12        \u001b[36m0.4789\u001b[0m       0.6829        0.6681  2.3321\n",
      "     13        \u001b[36m0.4746\u001b[0m       0.6908        0.6562  2.4264\n",
      "     14        \u001b[36m0.4714\u001b[0m       0.6887        0.6870  2.4502\n",
      "     15        \u001b[36m0.4701\u001b[0m       0.6847        0.6790  2.4536\n",
      "     16        \u001b[36m0.4659\u001b[0m       0.6948        0.6841  2.4129\n",
      "     17        \u001b[36m0.4652\u001b[0m       0.6917        0.6860  2.4588\n",
      "     18        \u001b[36m0.4619\u001b[0m       \u001b[32m0.7047\u001b[0m        0.6969  2.7676\n",
      "     19        \u001b[36m0.4612\u001b[0m       0.6850        0.7077  2.4588\n",
      "     20        \u001b[36m0.4586\u001b[0m       0.6935        0.7005  2.4113\n",
      "2 enzyme 1.8534861853294373 1.5616386331842413 1.346204218188876 1.1182463723371412 1.7287658002989423\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6251\u001b[0m       \u001b[32m0.6810\u001b[0m        \u001b[35m0.6131\u001b[0m  2.3798\n",
      "      2        \u001b[36m0.6023\u001b[0m       \u001b[32m0.6882\u001b[0m        \u001b[35m0.6081\u001b[0m  2.6086\n",
      "      3        \u001b[36m0.5790\u001b[0m       \u001b[32m0.6911\u001b[0m        \u001b[35m0.6074\u001b[0m  2.6199\n",
      "      4        \u001b[36m0.5541\u001b[0m       \u001b[32m0.6948\u001b[0m        0.6092  2.6957\n",
      "      5        \u001b[36m0.5308\u001b[0m       0.6900        0.6140  2.3816\n",
      "      6        \u001b[36m0.5129\u001b[0m       0.6911        0.6272  2.4234\n",
      "      7        \u001b[36m0.5005\u001b[0m       0.6909        0.6575  2.3860\n",
      "      8        \u001b[36m0.4913\u001b[0m       0.6947        0.6844  2.3818\n",
      "      9        \u001b[36m0.4844\u001b[0m       0.6873        0.6834  2.4376\n",
      "     10        \u001b[36m0.4795\u001b[0m       0.6930        0.6950  2.3699\n",
      "     11        \u001b[36m0.4737\u001b[0m       0.6911        0.7284  2.4393\n",
      "     12        \u001b[36m0.4692\u001b[0m       0.6888        0.7671  2.3609\n",
      "     13        \u001b[36m0.4668\u001b[0m       0.6880        0.7596  2.5936\n",
      "     14        \u001b[36m0.4637\u001b[0m       0.6897        0.7363  2.5278\n",
      "     15        \u001b[36m0.4599\u001b[0m       0.6890        0.7702  2.4042\n",
      "     16        \u001b[36m0.4587\u001b[0m       0.6923        0.7966  2.4215\n",
      "     17        \u001b[36m0.4564\u001b[0m       0.6870        0.8189  2.3962\n",
      "     18        \u001b[36m0.4552\u001b[0m       0.6876        0.8434  2.4145\n",
      "     19        \u001b[36m0.4516\u001b[0m       0.6925        0.7784  2.3622\n",
      "     20        \u001b[36m0.4492\u001b[0m       0.6908        0.8241  2.3631\n",
      "3 enzyme 2.4577953033770967 2.0034480419323657 1.775189368228928 1.4764845116805598 2.2633173973505394\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6195\u001b[0m       \u001b[32m0.6628\u001b[0m        \u001b[35m0.6260\u001b[0m  2.3822\n",
      "      2        \u001b[36m0.5988\u001b[0m       0.6609        0.6354  2.3872\n",
      "      3        \u001b[36m0.5748\u001b[0m       0.6450        0.7094  2.3439\n",
      "      4        \u001b[36m0.5486\u001b[0m       0.6375        0.7160  2.3511\n",
      "      5        \u001b[36m0.5257\u001b[0m       0.6286        0.7824  2.4320\n",
      "      6        \u001b[36m0.5107\u001b[0m       0.6135        0.8375  2.3746\n",
      "      7        \u001b[36m0.5007\u001b[0m       0.6102        0.8868  2.4407\n",
      "      8        \u001b[36m0.4911\u001b[0m       0.6035        0.9752  2.3678\n",
      "      9        \u001b[36m0.4840\u001b[0m       0.6155        0.8796  2.3791\n",
      "     10        \u001b[36m0.4788\u001b[0m       0.5971        1.0211  3.0601\n",
      "     11        \u001b[36m0.4753\u001b[0m       0.6153        0.9042  2.9615\n",
      "     12        \u001b[36m0.4706\u001b[0m       0.6017        0.9992  2.7192\n",
      "     13        \u001b[36m0.4663\u001b[0m       0.5955        0.9986  2.4563\n",
      "     14        \u001b[36m0.4635\u001b[0m       0.6014        1.0318  2.4161\n",
      "     15        0.4640       0.5942        1.0018  2.6527\n",
      "     16        \u001b[36m0.4597\u001b[0m       0.6039        1.0098  2.4472\n",
      "     17        \u001b[36m0.4582\u001b[0m       0.6020        0.9995  2.5436\n",
      "     18        \u001b[36m0.4543\u001b[0m       0.6029        0.9935  2.4343\n",
      "     19        0.4545       0.5871        1.1633  2.4636\n",
      "     20        \u001b[36m0.4525\u001b[0m       0.5946        1.0506  2.4547\n",
      "4 enzyme 3.0748062899198123 2.4845427009456 2.296404354637154 2.157320246550877 2.685542456391836\n",
      "enzyme 0.6149612579839625 0.49690854018912 0.45928087092743086 0.4314640493101754 0.5371084912783672\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6275\u001b[0m       \u001b[32m0.6763\u001b[0m        \u001b[35m0.6264\u001b[0m  2.4666\n",
      "      2        \u001b[36m0.6169\u001b[0m       \u001b[32m0.6832\u001b[0m        \u001b[35m0.6161\u001b[0m  2.4747\n",
      "      3        \u001b[36m0.5954\u001b[0m       \u001b[32m0.6951\u001b[0m        \u001b[35m0.6038\u001b[0m  2.4163\n",
      "      4        \u001b[36m0.5633\u001b[0m       \u001b[32m0.6984\u001b[0m        \u001b[35m0.5937\u001b[0m  2.3960\n",
      "      5        \u001b[36m0.5305\u001b[0m       0.6972        \u001b[35m0.5883\u001b[0m  2.4095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        \u001b[36m0.5040\u001b[0m       0.6858        0.5987  2.4206\n",
      "      7        \u001b[36m0.4868\u001b[0m       0.6961        0.5992  2.3628\n",
      "      8        \u001b[36m0.4708\u001b[0m       \u001b[32m0.7040\u001b[0m        0.5929  2.3297\n",
      "      9        \u001b[36m0.4611\u001b[0m       0.7027        0.5886  2.3124\n",
      "     10        \u001b[36m0.4495\u001b[0m       \u001b[32m0.7116\u001b[0m        \u001b[35m0.5819\u001b[0m  2.3498\n",
      "     11        \u001b[36m0.4438\u001b[0m       \u001b[32m0.7232\u001b[0m        \u001b[35m0.5732\u001b[0m  2.4406\n",
      "     12        \u001b[36m0.4377\u001b[0m       0.7072        0.6077  2.4550\n",
      "     13        \u001b[36m0.4311\u001b[0m       0.7185        0.5808  2.4453\n",
      "     14        \u001b[36m0.4280\u001b[0m       0.7087        0.5951  2.4324\n",
      "     15        \u001b[36m0.4210\u001b[0m       0.7125        0.5921  2.4460\n",
      "     16        \u001b[36m0.4180\u001b[0m       0.7215        0.5823  2.3834\n",
      "     17        \u001b[36m0.4133\u001b[0m       0.7127        0.5878  2.2855\n",
      "     18        \u001b[36m0.4102\u001b[0m       0.7206        0.5965  2.4083\n",
      "     19        \u001b[36m0.4099\u001b[0m       0.7151        0.5943  2.2073\n",
      "     20        \u001b[36m0.4074\u001b[0m       0.7202        0.5946  2.5029\n",
      "0 pathway 0.6398499724949676 0.5764417535237869 0.488037109375 0.4114438612740558 0.5996700164991751\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6260\u001b[0m       \u001b[32m0.6756\u001b[0m        \u001b[35m0.6283\u001b[0m  2.4832\n",
      "      2        \u001b[36m0.6119\u001b[0m       \u001b[32m0.6815\u001b[0m        \u001b[35m0.6202\u001b[0m  2.8639\n",
      "      3        \u001b[36m0.5896\u001b[0m       \u001b[32m0.6957\u001b[0m        \u001b[35m0.6068\u001b[0m  2.4514\n",
      "      4        \u001b[36m0.5584\u001b[0m       \u001b[32m0.7019\u001b[0m        \u001b[35m0.5944\u001b[0m  2.3672\n",
      "      5        \u001b[36m0.5284\u001b[0m       \u001b[32m0.7036\u001b[0m        0.6027  2.3813\n",
      "      6        \u001b[36m0.5025\u001b[0m       \u001b[32m0.7050\u001b[0m        0.6148  2.3805\n",
      "      7        \u001b[36m0.4875\u001b[0m       \u001b[32m0.7144\u001b[0m        0.6023  2.3851\n",
      "      8        \u001b[36m0.4737\u001b[0m       0.7091        \u001b[35m0.5864\u001b[0m  2.4114\n",
      "      9        \u001b[36m0.4626\u001b[0m       0.6980        0.6068  2.4864\n",
      "     10        \u001b[36m0.4544\u001b[0m       \u001b[32m0.7149\u001b[0m        0.5922  2.7201\n",
      "     11        \u001b[36m0.4454\u001b[0m       0.7090        0.6310  2.4007\n",
      "     12        \u001b[36m0.4394\u001b[0m       0.7102        0.6344  2.3804\n",
      "     13        \u001b[36m0.4318\u001b[0m       \u001b[32m0.7230\u001b[0m        0.6027  2.4116\n",
      "     14        \u001b[36m0.4289\u001b[0m       0.7166        0.6429  2.3926\n",
      "     15        \u001b[36m0.4242\u001b[0m       0.7194        0.6077  2.3947\n",
      "     16        \u001b[36m0.4202\u001b[0m       0.7166        0.6386  2.4911\n",
      "     17        \u001b[36m0.4168\u001b[0m       \u001b[32m0.7237\u001b[0m        0.6493  2.4028\n",
      "     18        \u001b[36m0.4137\u001b[0m       0.7070        0.6321  2.5178\n",
      "     19        \u001b[36m0.4100\u001b[0m       0.7216        0.6347  2.5041\n",
      "     20        \u001b[36m0.4074\u001b[0m       0.7102        0.6500  2.6384\n",
      "1 pathway 1.2540347178322904 1.125678845908376 0.9209352816987598 0.7527014510651435 1.1914958241022449\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6251\u001b[0m       \u001b[32m0.6791\u001b[0m        \u001b[35m0.6259\u001b[0m  2.4361\n",
      "      2        \u001b[36m0.6126\u001b[0m       \u001b[32m0.6847\u001b[0m        \u001b[35m0.6152\u001b[0m  2.3703\n",
      "      3        \u001b[36m0.5864\u001b[0m       \u001b[32m0.6950\u001b[0m        \u001b[35m0.6025\u001b[0m  2.7614\n",
      "      4        \u001b[36m0.5501\u001b[0m       0.6816        0.6052  2.5548\n",
      "      5        \u001b[36m0.5177\u001b[0m       0.6799        0.6245  2.3761\n",
      "      6        \u001b[36m0.4953\u001b[0m       0.6712        0.6554  2.3677\n",
      "      7        \u001b[36m0.4798\u001b[0m       0.6566        0.6828  2.4037\n",
      "      8        \u001b[36m0.4673\u001b[0m       0.6580        0.7007  2.1172\n",
      "      9        \u001b[36m0.4564\u001b[0m       0.6820        0.6800  2.1170\n",
      "     10        \u001b[36m0.4498\u001b[0m       0.6873        0.6897  2.1015\n",
      "     11        \u001b[36m0.4429\u001b[0m       0.6742        0.7119  2.1552\n",
      "     12        \u001b[36m0.4356\u001b[0m       0.6676        0.7525  2.3155\n",
      "     13        \u001b[36m0.4320\u001b[0m       0.6774        0.7090  2.4104\n",
      "     14        \u001b[36m0.4260\u001b[0m       0.6835        0.7140  2.4367\n",
      "     15        \u001b[36m0.4220\u001b[0m       0.6752        0.7476  2.3770\n",
      "     16        \u001b[36m0.4180\u001b[0m       0.6809        0.7447  2.6101\n",
      "     17        \u001b[36m0.4143\u001b[0m       0.6813        0.7596  2.4644\n",
      "     18        \u001b[36m0.4118\u001b[0m       0.6949        0.7207  2.4302\n",
      "     19        \u001b[36m0.4073\u001b[0m       0.6808        0.7807  2.4298\n",
      "     20        \u001b[36m0.4049\u001b[0m       0.6877        0.7650  2.5262\n",
      "2 pathway 1.8922841301471964 1.6569285068686586 1.4290786041743297 1.2503859215807347 1.710547020839392\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6322\u001b[0m       \u001b[32m0.6755\u001b[0m        \u001b[35m0.6244\u001b[0m  2.3862\n",
      "      2        \u001b[36m0.6146\u001b[0m       \u001b[32m0.6821\u001b[0m        \u001b[35m0.6120\u001b[0m  2.4261\n",
      "      3        \u001b[36m0.5894\u001b[0m       0.6763        \u001b[35m0.6077\u001b[0m  2.4670\n",
      "      4        \u001b[36m0.5527\u001b[0m       \u001b[32m0.6868\u001b[0m        0.6093  2.3982\n",
      "      5        \u001b[36m0.5183\u001b[0m       \u001b[32m0.6878\u001b[0m        0.6281  2.3799\n",
      "      6        \u001b[36m0.4909\u001b[0m       0.6822        0.6595  2.2841\n",
      "      7        \u001b[36m0.4720\u001b[0m       0.6731        0.6892  2.3810\n",
      "      8        \u001b[36m0.4601\u001b[0m       0.6727        0.6943  2.6087\n",
      "      9        \u001b[36m0.4484\u001b[0m       0.6731        0.7070  2.9469\n",
      "     10        \u001b[36m0.4379\u001b[0m       0.6675        0.7439  2.5373\n",
      "     11        \u001b[36m0.4309\u001b[0m       0.6709        0.7356  2.4847\n",
      "     12        \u001b[36m0.4252\u001b[0m       0.6711        0.7759  2.4095\n",
      "     13        \u001b[36m0.4199\u001b[0m       0.6631        0.8118  2.4388\n",
      "     14        \u001b[36m0.4158\u001b[0m       0.6684        0.7782  2.3849\n",
      "     15        \u001b[36m0.4099\u001b[0m       0.6605        0.8318  2.4551\n",
      "     16        \u001b[36m0.4062\u001b[0m       0.6681        0.8220  2.4269\n",
      "     17        \u001b[36m0.4033\u001b[0m       0.6706        0.7985  2.4536\n",
      "     18        \u001b[36m0.4009\u001b[0m       0.6578        0.8332  2.4390\n",
      "     19        \u001b[36m0.3976\u001b[0m       0.6611        0.8531  2.4599\n",
      "     20        \u001b[36m0.3946\u001b[0m       0.6652        0.8401  2.4630\n",
      "3 pathway 2.4805582985919195 2.0862192215573314 1.846200221086863 1.6199444272923742 2.1892831893502573\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6331\u001b[0m       \u001b[32m0.6762\u001b[0m        \u001b[35m0.6207\u001b[0m  2.3952\n",
      "      2        \u001b[36m0.6105\u001b[0m       \u001b[32m0.6859\u001b[0m        \u001b[35m0.6129\u001b[0m  2.3981\n",
      "      3        \u001b[36m0.5803\u001b[0m       0.6735        0.6159  2.3924\n",
      "      4        \u001b[36m0.5412\u001b[0m       0.6557        0.6500  2.4809\n",
      "      5        \u001b[36m0.5068\u001b[0m       0.6505        0.6776  2.4954\n",
      "      6        \u001b[36m0.4824\u001b[0m       0.6379        0.7650  2.6066\n",
      "      7        \u001b[36m0.4668\u001b[0m       0.6397        0.7680  2.3439\n",
      "      8        \u001b[36m0.4549\u001b[0m       0.6437        0.7862  2.3897\n",
      "      9        \u001b[36m0.4425\u001b[0m       0.6334        0.8761  2.4001\n",
      "     10        \u001b[36m0.4347\u001b[0m       0.6420        0.8253  2.4079\n",
      "     11        \u001b[36m0.4287\u001b[0m       0.6422        0.8186  2.4142\n",
      "     12        \u001b[36m0.4233\u001b[0m       0.6357        0.8819  2.3759\n",
      "     13        \u001b[36m0.4183\u001b[0m       0.6330        0.9369  2.3951\n",
      "     14        \u001b[36m0.4130\u001b[0m       0.6366        0.9120  2.6365\n",
      "     15        \u001b[36m0.4085\u001b[0m       0.6417        0.9144  2.7437\n",
      "     16        \u001b[36m0.4047\u001b[0m       0.6380        0.9996  2.4164\n",
      "     17        \u001b[36m0.4033\u001b[0m       0.6342        0.9580  2.3495\n",
      "     18        \u001b[36m0.4010\u001b[0m       0.6388        0.9537  2.4365\n",
      "     19        \u001b[36m0.3966\u001b[0m       0.6280        1.1121  2.4386\n",
      "     20        \u001b[36m0.3934\u001b[0m       0.6407        0.9941  2.3428\n",
      "4 pathway 3.091421361125197 2.5263391462265874 2.3331219419882876 2.1458810267160056 2.6425786741369155\n",
      "pathway 0.6182842722250393 0.5052678292453174 0.4666243883976575 0.4291762053432011 0.5285157348273831\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6228\u001b[0m       \u001b[32m0.6845\u001b[0m        \u001b[35m0.6129\u001b[0m  2.4942\n",
      "      2        \u001b[36m0.5625\u001b[0m       \u001b[32m0.7046\u001b[0m        \u001b[35m0.5793\u001b[0m  2.6191\n",
      "      3        \u001b[36m0.5038\u001b[0m       \u001b[32m0.7204\u001b[0m        \u001b[35m0.5581\u001b[0m  2.3857\n",
      "      4        \u001b[36m0.4571\u001b[0m       0.7195        0.5616  2.4288\n",
      "      5        \u001b[36m0.4246\u001b[0m       \u001b[32m0.7266\u001b[0m        0.5671  2.3670\n",
      "      6        \u001b[36m0.4008\u001b[0m       \u001b[32m0.7270\u001b[0m        0.5830  2.3579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        \u001b[36m0.3838\u001b[0m       \u001b[32m0.7281\u001b[0m        0.5991  2.4610\n",
      "      8        \u001b[36m0.3702\u001b[0m       0.7262        0.6127  2.3689\n",
      "      9        \u001b[36m0.3596\u001b[0m       \u001b[32m0.7286\u001b[0m        0.6316  2.4990\n",
      "     10        \u001b[36m0.3518\u001b[0m       0.7266        0.6597  2.4105\n",
      "     11        \u001b[36m0.3430\u001b[0m       \u001b[32m0.7316\u001b[0m        0.6822  2.6709\n",
      "     12        \u001b[36m0.3386\u001b[0m       \u001b[32m0.7371\u001b[0m        0.6382  2.6573\n",
      "     13        \u001b[36m0.3330\u001b[0m       0.7292        0.6828  2.4281\n",
      "     14        \u001b[36m0.3276\u001b[0m       0.7360        0.6699  2.5127\n",
      "     15        \u001b[36m0.3232\u001b[0m       0.7348        0.6624  2.6641\n",
      "     16        \u001b[36m0.3190\u001b[0m       0.7329        0.6800  2.8693\n",
      "     17        \u001b[36m0.3163\u001b[0m       0.7341        0.7011  2.4765\n",
      "     18        \u001b[36m0.3115\u001b[0m       \u001b[32m0.7378\u001b[0m        0.6896  2.4722\n",
      "     19        \u001b[36m0.3071\u001b[0m       0.7359        0.7107  2.7106\n",
      "     20        \u001b[36m0.3058\u001b[0m       0.7365        0.7224  2.7110\n",
      "0 indication 0.67995296812423 0.6022465644631242 0.5608983267552393 0.5191931666152104 0.6098887814313346\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6302\u001b[0m       \u001b[32m0.6785\u001b[0m        \u001b[35m0.6181\u001b[0m  2.4068\n",
      "      2        \u001b[36m0.5780\u001b[0m       \u001b[32m0.7299\u001b[0m        \u001b[35m0.5429\u001b[0m  2.3919\n",
      "      3        \u001b[36m0.5173\u001b[0m       \u001b[32m0.7586\u001b[0m        \u001b[35m0.5071\u001b[0m  2.4367\n",
      "      4        \u001b[36m0.4703\u001b[0m       \u001b[32m0.7653\u001b[0m        \u001b[35m0.4963\u001b[0m  2.3315\n",
      "      5        \u001b[36m0.4359\u001b[0m       \u001b[32m0.7686\u001b[0m        0.4974  2.9020\n",
      "      6        \u001b[36m0.4114\u001b[0m       0.7626        0.5070  2.3935\n",
      "      7        \u001b[36m0.3928\u001b[0m       0.7616        0.5198  2.3409\n",
      "      8        \u001b[36m0.3799\u001b[0m       0.7548        0.5305  2.5176\n",
      "      9        \u001b[36m0.3695\u001b[0m       0.7615        0.5355  2.4520\n",
      "     10        \u001b[36m0.3607\u001b[0m       0.7604        0.5459  2.4961\n",
      "     11        \u001b[36m0.3518\u001b[0m       0.7613        0.5506  2.3932\n",
      "     12        \u001b[36m0.3468\u001b[0m       0.7628        0.5429  2.4260\n",
      "     13        \u001b[36m0.3389\u001b[0m       0.7564        0.5796  2.4239\n",
      "     14        \u001b[36m0.3347\u001b[0m       0.7610        0.5688  2.3935\n",
      "     15        \u001b[36m0.3297\u001b[0m       0.7561        0.5789  2.5120\n",
      "     16        \u001b[36m0.3265\u001b[0m       0.7576        0.5771  2.4192\n",
      "     17        \u001b[36m0.3235\u001b[0m       0.7622        0.5817  2.4424\n",
      "     18        \u001b[36m0.3190\u001b[0m       0.7585        0.6102  2.4113\n",
      "     19        \u001b[36m0.3169\u001b[0m       0.7561        0.6161  2.4508\n",
      "     20        \u001b[36m0.3137\u001b[0m       0.7615        0.6192  2.4696\n",
      "1 indication 1.396329252756001 1.2725758752007787 1.1760676221204758 1.1072347432335081 1.2548097746593256\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6229\u001b[0m       \u001b[32m0.6894\u001b[0m        \u001b[35m0.6004\u001b[0m  2.3877\n",
      "      2        \u001b[36m0.5614\u001b[0m       \u001b[32m0.7312\u001b[0m        \u001b[35m0.5459\u001b[0m  2.3356\n",
      "      3        \u001b[36m0.5033\u001b[0m       \u001b[32m0.7447\u001b[0m        \u001b[35m0.5279\u001b[0m  2.3991\n",
      "      4        \u001b[36m0.4593\u001b[0m       \u001b[32m0.7485\u001b[0m        0.5311  2.4019\n",
      "      5        \u001b[36m0.4263\u001b[0m       0.7380        0.5697  2.3900\n",
      "      6        \u001b[36m0.4050\u001b[0m       0.7333        0.5873  2.3988\n",
      "      7        \u001b[36m0.3888\u001b[0m       0.7332        0.5984  2.3893\n",
      "      8        \u001b[36m0.3732\u001b[0m       0.7223        0.6587  2.4037\n",
      "      9        \u001b[36m0.3631\u001b[0m       0.7235        0.6549  2.4754\n",
      "     10        \u001b[36m0.3550\u001b[0m       0.7216        0.7282  2.4193\n",
      "     11        \u001b[36m0.3475\u001b[0m       0.7265        0.7127  2.3911\n",
      "     12        \u001b[36m0.3408\u001b[0m       0.7245        0.7039  2.5264\n",
      "     13        \u001b[36m0.3355\u001b[0m       0.7218        0.7347  2.4457\n",
      "     14        \u001b[36m0.3292\u001b[0m       0.7216        0.7498  2.3734\n",
      "     15        \u001b[36m0.3227\u001b[0m       0.7215        0.7461  2.3972\n",
      "     16        \u001b[36m0.3205\u001b[0m       0.7176        0.7936  2.4095\n",
      "     17        \u001b[36m0.3165\u001b[0m       0.7163        0.8154  2.4422\n",
      "     18        \u001b[36m0.3121\u001b[0m       0.7227        0.8459  2.3917\n",
      "     19        \u001b[36m0.3093\u001b[0m       0.7248        0.8021  2.4713\n",
      "     20        \u001b[36m0.3056\u001b[0m       0.7284        0.7737  2.4294\n",
      "2 indication 2.094032944867516 1.875610002532368 1.7690786704632244 1.7176083153236594 1.831418757743129\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6290\u001b[0m       \u001b[32m0.6758\u001b[0m        \u001b[35m0.6209\u001b[0m  2.5585\n",
      "      2        \u001b[36m0.5865\u001b[0m       \u001b[32m0.7274\u001b[0m        \u001b[35m0.5517\u001b[0m  2.4222\n",
      "      3        \u001b[36m0.5116\u001b[0m       \u001b[32m0.7403\u001b[0m        \u001b[35m0.5396\u001b[0m  2.4965\n",
      "      4        \u001b[36m0.4575\u001b[0m       0.7368        0.5484  2.4654\n",
      "      5        \u001b[36m0.4177\u001b[0m       0.7293        0.5696  2.4205\n",
      "      6        \u001b[36m0.3877\u001b[0m       0.7246        0.5848  2.3849\n",
      "      7        \u001b[36m0.3700\u001b[0m       0.7207        0.6131  2.4166\n",
      "      8        \u001b[36m0.3521\u001b[0m       0.7058        0.6734  2.4419\n",
      "      9        \u001b[36m0.3420\u001b[0m       0.7133        0.6559  2.5131\n",
      "     10        \u001b[36m0.3306\u001b[0m       0.7112        0.6728  2.4818\n",
      "     11        \u001b[36m0.3256\u001b[0m       0.7003        0.7358  2.4833\n",
      "     12        \u001b[36m0.3187\u001b[0m       0.7042        0.7314  2.4875\n",
      "     13        \u001b[36m0.3121\u001b[0m       0.7043        0.7365  2.4689\n",
      "     14        \u001b[36m0.3063\u001b[0m       0.7023        0.7712  2.4605\n",
      "     15        \u001b[36m0.3015\u001b[0m       0.6963        0.7817  2.4018\n",
      "     16        \u001b[36m0.2978\u001b[0m       0.6965        0.8013  2.4497\n",
      "     17        \u001b[36m0.2955\u001b[0m       0.6996        0.8154  2.1596\n",
      "     18        \u001b[36m0.2905\u001b[0m       0.6978        0.8238  2.1347\n",
      "     19        \u001b[36m0.2858\u001b[0m       0.7021        0.7979  2.1318\n",
      "     20        \u001b[36m0.2849\u001b[0m       0.6947        0.8284  2.1068\n",
      "3 indication 2.74554387275137 2.414220985991516 2.2979847193692735 2.2462694247195634 2.3605699732355063\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6304\u001b[0m       \u001b[32m0.6759\u001b[0m        \u001b[35m0.6255\u001b[0m  2.1543\n",
      "      2        \u001b[36m0.5937\u001b[0m       \u001b[32m0.7398\u001b[0m        \u001b[35m0.5446\u001b[0m  2.1679\n",
      "      3        \u001b[36m0.5208\u001b[0m       \u001b[32m0.7508\u001b[0m        \u001b[35m0.5155\u001b[0m  2.1632\n",
      "      4        \u001b[36m0.4682\u001b[0m       0.7444        0.5284  2.1520\n",
      "      5        \u001b[36m0.4271\u001b[0m       0.7365        0.5445  2.1653\n",
      "      6        \u001b[36m0.3984\u001b[0m       0.7344        0.5713  2.1498\n",
      "      7        \u001b[36m0.3771\u001b[0m       0.7299        0.6129  2.1910\n",
      "      8        \u001b[36m0.3629\u001b[0m       0.7244        0.6159  2.1194\n",
      "      9        \u001b[36m0.3519\u001b[0m       0.7203        0.6505  2.1366\n",
      "     10        \u001b[36m0.3414\u001b[0m       0.7077        0.6997  2.1488\n",
      "     11        \u001b[36m0.3343\u001b[0m       0.7123        0.6864  2.1304\n",
      "     12        \u001b[36m0.3276\u001b[0m       0.7052        0.7298  2.2768\n",
      "     13        \u001b[36m0.3210\u001b[0m       0.7107        0.7410  2.3710\n",
      "     14        \u001b[36m0.3144\u001b[0m       0.7039        0.7586  2.4718\n",
      "     15        \u001b[36m0.3134\u001b[0m       0.7074        0.7385  2.3770\n",
      "     16        \u001b[36m0.3082\u001b[0m       0.7101        0.7726  2.3169\n",
      "     17        \u001b[36m0.3047\u001b[0m       0.7060        0.7989  2.3772\n",
      "     18        \u001b[36m0.3008\u001b[0m       0.7070        0.7853  2.4032\n",
      "     19        \u001b[36m0.2967\u001b[0m       0.7068        0.7817  2.9885\n",
      "     20        \u001b[36m0.2955\u001b[0m       0.7045        0.8544  2.9477\n",
      "4 indication 3.4040285383247837 2.979635855403786 2.834526702105345 2.7740586383877397 2.9061599296138443\n",
      "indication 0.6808057076649567 0.5959271710807572 0.566905340421069 0.5548117276775479 0.5812319859227688\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.5911\u001b[0m       \u001b[32m0.6794\u001b[0m        \u001b[35m0.6116\u001b[0m  2.4364\n",
      "      2        \u001b[36m0.5694\u001b[0m       \u001b[32m0.6837\u001b[0m        0.6137  2.4042\n",
      "      3        \u001b[36m0.5490\u001b[0m       \u001b[32m0.6865\u001b[0m        \u001b[35m0.6080\u001b[0m  2.3549\n",
      "      4        \u001b[36m0.5331\u001b[0m       \u001b[32m0.6990\u001b[0m        \u001b[35m0.5961\u001b[0m  2.9551\n",
      "      5        \u001b[36m0.5207\u001b[0m       \u001b[32m0.7093\u001b[0m        \u001b[35m0.5823\u001b[0m  2.3563\n",
      "      6        \u001b[36m0.5172\u001b[0m       0.6919        0.5931  2.5004\n",
      "      7        \u001b[36m0.5140\u001b[0m       0.6964        0.6027  2.3388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      8        \u001b[36m0.5133\u001b[0m       0.7005        0.5933  2.5307\n",
      "      9        \u001b[36m0.5123\u001b[0m       0.7003        0.5911  2.4712\n",
      "     10        \u001b[36m0.5081\u001b[0m       \u001b[32m0.7127\u001b[0m        \u001b[35m0.5646\u001b[0m  2.4324\n",
      "     11        0.5117       0.6973        0.5810  2.4438\n",
      "     12        0.5115       0.7076        0.5772  2.4181\n",
      "     13        0.5130       0.7039        0.5735  2.2822\n",
      "     14        0.5154       0.7023        \u001b[35m0.5641\u001b[0m  2.1435\n",
      "     15        0.5128       0.6939        0.5894  2.1431\n",
      "     16        0.5140       0.6924        0.6043  2.1318\n",
      "     17        0.5086       \u001b[32m0.7167\u001b[0m        \u001b[35m0.5549\u001b[0m  2.1232\n",
      "     18        \u001b[36m0.4973\u001b[0m       \u001b[32m0.7247\u001b[0m        0.5590  2.1570\n",
      "     19        0.4977       0.7071        0.5852  2.4698\n",
      "     20        0.5083       0.7119        0.5656  2.4867\n",
      "0 sideeffect 0.5697885735696794 0.587706488499169 0.2715924426450742 0.16568899866213851 0.7526881720430108\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6030\u001b[0m       \u001b[32m0.6854\u001b[0m        \u001b[35m0.5893\u001b[0m  2.4133\n",
      "      2        \u001b[36m0.5792\u001b[0m       \u001b[32m0.7206\u001b[0m        \u001b[35m0.5664\u001b[0m  2.3993\n",
      "      3        \u001b[36m0.5616\u001b[0m       \u001b[32m0.7255\u001b[0m        \u001b[35m0.5529\u001b[0m  2.3895\n",
      "      4        \u001b[36m0.5420\u001b[0m       \u001b[32m0.7298\u001b[0m        \u001b[35m0.5481\u001b[0m  2.3811\n",
      "      5        \u001b[36m0.5266\u001b[0m       \u001b[32m0.7356\u001b[0m        \u001b[35m0.5403\u001b[0m  2.4159\n",
      "      6        \u001b[36m0.5126\u001b[0m       \u001b[32m0.7423\u001b[0m        \u001b[35m0.5238\u001b[0m  2.4395\n",
      "      7        \u001b[36m0.5100\u001b[0m       0.7407        0.5316  2.4033\n",
      "      8        0.5127       0.7411        0.5312  2.4085\n",
      "      9        0.5121       0.7361        0.5309  2.4119\n",
      "     10        0.5128       0.7329        0.5427  2.4005\n",
      "     11        \u001b[36m0.5016\u001b[0m       0.7364        0.5337  2.3749\n",
      "     12        \u001b[36m0.5002\u001b[0m       0.7310        0.5373  2.4004\n",
      "     13        \u001b[36m0.4984\u001b[0m       0.7314        0.5361  2.3912\n",
      "     14        0.4994       0.7222        0.5613  2.3953\n",
      "     15        \u001b[36m0.4933\u001b[0m       0.7082        0.5924  2.3919\n",
      "     16        0.5029       0.7421        0.5332  2.4399\n",
      "     17        0.5095       0.7298        0.5854  2.4453\n",
      "     18        0.5084       0.7298        0.5689  2.3750\n",
      "     19        0.5125       0.7234        0.5479  2.3330\n",
      "     20        0.4983       0.7227        0.5389  2.4179\n",
      "1 sideeffect 1.2415320472983844 1.1896164520444734 0.8235247446986504 0.6926005968920448 1.332135885940702\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.5996\u001b[0m       \u001b[32m0.7043\u001b[0m        \u001b[35m0.5739\u001b[0m  2.4064\n",
      "      2        \u001b[36m0.5783\u001b[0m       \u001b[32m0.7170\u001b[0m        \u001b[35m0.5630\u001b[0m  2.4138\n",
      "      3        \u001b[36m0.5583\u001b[0m       \u001b[32m0.7210\u001b[0m        \u001b[35m0.5555\u001b[0m  2.4354\n",
      "      4        \u001b[36m0.5381\u001b[0m       \u001b[32m0.7225\u001b[0m        \u001b[35m0.5526\u001b[0m  2.4089\n",
      "      5        \u001b[36m0.5256\u001b[0m       0.6973        0.5974  2.3817\n",
      "      6        \u001b[36m0.5194\u001b[0m       \u001b[32m0.7239\u001b[0m        0.5594  2.3924\n",
      "      7        \u001b[36m0.5161\u001b[0m       0.7110        0.5924  2.4582\n",
      "      8        0.5200       \u001b[32m0.7299\u001b[0m        \u001b[35m0.5447\u001b[0m  2.4125\n",
      "      9        \u001b[36m0.5138\u001b[0m       \u001b[32m0.7325\u001b[0m        \u001b[35m0.5446\u001b[0m  2.3867\n",
      "     10        0.5166       0.7180        0.5636  2.3920\n",
      "     11        0.5168       0.7095        0.5836  2.3758\n",
      "     12        \u001b[36m0.5117\u001b[0m       0.7276        0.5447  2.4344\n",
      "     13        \u001b[36m0.5104\u001b[0m       0.7249        0.5584  2.4355\n",
      "     14        0.5118       0.7121        0.5902  2.3880\n",
      "     15        0.5126       0.7009        0.6429  2.4557\n",
      "     16        0.5165       \u001b[32m0.7355\u001b[0m        \u001b[35m0.5349\u001b[0m  2.4441\n",
      "     17        0.5188       0.7297        0.5406  2.4118\n",
      "     18        0.5107       0.7258        0.5454  2.3726\n",
      "     19        \u001b[36m0.5069\u001b[0m       0.7218        0.5570  2.9840\n",
      "     20        \u001b[36m0.5011\u001b[0m       0.7113        0.6050  2.6078\n",
      "2 sideeffect 1.8145693418425282 1.744059978439657 1.1111677841322554 0.8723885973036944 2.0510659270929654\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6007\u001b[0m       \u001b[32m0.6966\u001b[0m        \u001b[35m0.5856\u001b[0m  2.6103\n",
      "      2        \u001b[36m0.5743\u001b[0m       \u001b[32m0.7055\u001b[0m        \u001b[35m0.5784\u001b[0m  2.5613\n",
      "      3        \u001b[36m0.5517\u001b[0m       0.6972        0.6136  2.5621\n",
      "      4        \u001b[36m0.5295\u001b[0m       0.6962        0.6167  2.5768\n",
      "      5        \u001b[36m0.5160\u001b[0m       0.6988        0.6485  2.5074\n",
      "      6        \u001b[36m0.5029\u001b[0m       0.6941        0.6652  2.4065\n",
      "      7        0.5055       0.7018        0.6282  2.4442\n",
      "      8        0.5040       0.7017        0.6110  2.3472\n",
      "      9        0.5045       0.6837        0.6055  2.3683\n",
      "     10        0.5036       0.7031        0.5865  2.4364\n",
      "     11        0.5068       0.6973        0.6655  2.3835\n",
      "     12        \u001b[36m0.5019\u001b[0m       0.6966        0.6670  2.3817\n",
      "     13        \u001b[36m0.4983\u001b[0m       0.6892        0.7556  2.4086\n",
      "     14        0.5007       0.6933        0.6383  2.4701\n",
      "     15        0.4998       0.6850        0.6743  2.4526\n",
      "     16        0.4993       \u001b[32m0.7069\u001b[0m        0.6759  2.4080\n",
      "     17        0.5022       \u001b[32m0.7124\u001b[0m        0.5834  2.4147\n",
      "     18        0.5147       0.6886        0.7189  2.3393\n",
      "     19        0.5009       0.7028        0.6742  2.4492\n",
      "     20        \u001b[36m0.4960\u001b[0m       0.7074        0.6142  2.4299\n",
      "3 sideeffect 2.415644816272195 2.2661421204646683 1.5094664738489323 1.1711433570031902 2.6483910299736237\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6003\u001b[0m       \u001b[32m0.6981\u001b[0m        \u001b[35m0.5930\u001b[0m  2.4411\n",
      "      2        \u001b[36m0.5748\u001b[0m       0.6953        0.6044  2.4020\n",
      "      3        \u001b[36m0.5561\u001b[0m       0.6245        0.6479  2.4429\n",
      "      4        \u001b[36m0.5352\u001b[0m       0.6406        0.6492  2.3623\n",
      "      5        \u001b[36m0.5158\u001b[0m       0.4107        1.2208  2.3387\n",
      "      6        \u001b[36m0.5096\u001b[0m       0.5095        0.8542  2.4342\n",
      "      7        \u001b[36m0.5081\u001b[0m       0.4970        0.8178  2.3836\n",
      "      8        0.5106       0.4324        1.0394  2.3963\n",
      "      9        \u001b[36m0.5048\u001b[0m       0.4845        0.8268  2.4191\n",
      "     10        \u001b[36m0.4999\u001b[0m       0.4603        0.8444  2.8942\n",
      "     11        0.5062       0.5223        0.7734  2.5271\n",
      "     12        0.5056       0.3862        1.1924  2.3520\n",
      "     13        0.5033       0.3929        1.0928  2.3911\n",
      "     14        0.5162       0.4780        0.8723  2.3802\n",
      "     15        0.5161       0.5272        0.7878  2.4078\n",
      "     16        0.5125       0.4668        0.9741  2.4015\n",
      "     17        0.5132       0.5025        0.8432  2.3680\n",
      "     18        0.5106       0.4748        0.8913  2.4178\n",
      "     19        0.5115       0.4011        1.2274  2.3801\n",
      "     20        0.5061       0.4947        0.9328  2.4645\n",
      "4 sideeffect 3.0205693367537783 2.754116802984906 2.050346214352767 2.089319561202449 3.0317437006796526\n",
      "sideeffect 0.6041138673507557 0.5508233605969812 0.41006924287055335 0.4178639122404898 0.6063487401359305\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.5928\u001b[0m       \u001b[32m0.6923\u001b[0m        \u001b[35m0.5797\u001b[0m  2.3680\n",
      "      2        \u001b[36m0.5397\u001b[0m       \u001b[32m0.7137\u001b[0m        \u001b[35m0.5560\u001b[0m  2.4116\n",
      "      3        \u001b[36m0.5104\u001b[0m       \u001b[32m0.7454\u001b[0m        \u001b[35m0.5201\u001b[0m  2.4344\n",
      "      4        \u001b[36m0.4806\u001b[0m       \u001b[32m0.7553\u001b[0m        \u001b[35m0.5083\u001b[0m  2.4435\n",
      "      5        \u001b[36m0.4646\u001b[0m       \u001b[32m0.7582\u001b[0m        \u001b[35m0.4958\u001b[0m  2.4181\n",
      "      6        0.4677       0.7480        0.5028  2.3750\n",
      "      7        \u001b[36m0.4575\u001b[0m       0.7454        0.5025  2.5281\n",
      "      8        \u001b[36m0.4531\u001b[0m       0.7374        0.5096  2.4756\n",
      "      9        0.4569       \u001b[32m0.7626\u001b[0m        \u001b[35m0.4871\u001b[0m  2.3723\n",
      "     10        0.4562       0.7545        0.4941  2.3913\n",
      "     11        0.4588       0.7250        0.5333  2.4107\n",
      "     12        0.4602       \u001b[32m0.7708\u001b[0m        0.4896  2.3897\n",
      "     13        0.4574       0.7612        0.4917  2.3698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     14        0.4564       0.7417        0.5126  2.3727\n",
      "     15        0.4577       0.7629        0.4965  2.4033\n",
      "     16        \u001b[36m0.4493\u001b[0m       \u001b[32m0.7828\u001b[0m        \u001b[35m0.4691\u001b[0m  2.5715\n",
      "     17        0.4609       0.7583        0.4946  2.4154\n",
      "     18        \u001b[36m0.4472\u001b[0m       0.7622        0.4865  2.3995\n",
      "     19        \u001b[36m0.4441\u001b[0m       0.7509        0.4939  2.4213\n",
      "     20        0.4448       0.7769        0.4740  2.3767\n",
      "0 offsideeffect 0.7046175384741662 0.7048581484601263 0.5918828196521209 0.49902233199547186 0.7272045590881824\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.5917\u001b[0m       \u001b[32m0.6971\u001b[0m        \u001b[35m0.5747\u001b[0m  2.3743\n",
      "      2        \u001b[36m0.5545\u001b[0m       \u001b[32m0.7125\u001b[0m        \u001b[35m0.5586\u001b[0m  2.3808\n",
      "      3        \u001b[36m0.5275\u001b[0m       \u001b[32m0.7323\u001b[0m        \u001b[35m0.5367\u001b[0m  2.2965\n",
      "      4        \u001b[36m0.5026\u001b[0m       \u001b[32m0.7519\u001b[0m        \u001b[35m0.5071\u001b[0m  2.3740\n",
      "      5        \u001b[36m0.4891\u001b[0m       \u001b[32m0.7534\u001b[0m        \u001b[35m0.5015\u001b[0m  2.3901\n",
      "      6        \u001b[36m0.4849\u001b[0m       0.7499        \u001b[35m0.4919\u001b[0m  2.3891\n",
      "      7        \u001b[36m0.4777\u001b[0m       \u001b[32m0.7625\u001b[0m        \u001b[35m0.4820\u001b[0m  2.5859\n",
      "      8        \u001b[36m0.4750\u001b[0m       0.7464        0.4975  2.6285\n",
      "      9        \u001b[36m0.4721\u001b[0m       \u001b[32m0.7691\u001b[0m        \u001b[35m0.4722\u001b[0m  2.6190\n",
      "     10        \u001b[36m0.4707\u001b[0m       0.7254        0.5109  2.4551\n",
      "     11        0.4739       0.7520        0.5023  2.4384\n",
      "     12        0.4719       \u001b[32m0.7730\u001b[0m        \u001b[35m0.4681\u001b[0m  2.2392\n",
      "     13        \u001b[36m0.4679\u001b[0m       \u001b[32m0.7739\u001b[0m        \u001b[35m0.4668\u001b[0m  2.1566\n",
      "     14        \u001b[36m0.4598\u001b[0m       0.7638        0.4775  2.1910\n",
      "     15        \u001b[36m0.4575\u001b[0m       0.7733        0.4744  2.1616\n",
      "     16        0.4667       0.7687        0.4725  2.1226\n",
      "     17        0.4717       0.7644        0.4797  2.1666\n",
      "     18        0.4707       \u001b[32m0.7785\u001b[0m        0.4728  2.1440\n",
      "     19        0.4658       0.7583        0.4853  2.2018\n",
      "     20        0.4582       0.7605        0.4839  2.1651\n",
      "1 offsideeffect 1.411687243893341 1.3589793074366687 1.192328006073935 1.0542348461459299 1.3809062433242176\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6012\u001b[0m       \u001b[32m0.7012\u001b[0m        \u001b[35m0.5638\u001b[0m  2.1749\n",
      "      2        \u001b[36m0.5585\u001b[0m       \u001b[32m0.7286\u001b[0m        \u001b[35m0.5326\u001b[0m  2.1715\n",
      "      3        \u001b[36m0.5288\u001b[0m       \u001b[32m0.7321\u001b[0m        \u001b[35m0.5156\u001b[0m  2.1657\n",
      "      4        \u001b[36m0.4995\u001b[0m       0.7261        0.5359  2.1486\n",
      "      5        \u001b[36m0.4797\u001b[0m       \u001b[32m0.7434\u001b[0m        \u001b[35m0.4963\u001b[0m  2.1892\n",
      "      6        \u001b[36m0.4708\u001b[0m       \u001b[32m0.7487\u001b[0m        0.5157  2.1832\n",
      "      7        \u001b[36m0.4703\u001b[0m       \u001b[32m0.7598\u001b[0m        \u001b[35m0.4951\u001b[0m  2.1374\n",
      "      8        0.4735       0.7352        0.5210  2.1356\n",
      "      9        \u001b[36m0.4691\u001b[0m       \u001b[32m0.7622\u001b[0m        \u001b[35m0.4752\u001b[0m  2.1822\n",
      "     10        \u001b[36m0.4689\u001b[0m       0.7615        0.4839  2.2084\n",
      "     11        0.4700       0.7472        0.5078  2.2186\n",
      "     12        \u001b[36m0.4660\u001b[0m       0.7318        0.5313  2.1782\n",
      "     13        \u001b[36m0.4549\u001b[0m       0.7418        0.5238  2.2239\n",
      "     14        0.4565       0.7371        0.5496  2.1683\n",
      "     15        \u001b[36m0.4522\u001b[0m       0.7576        0.4993  2.1200\n",
      "     16        \u001b[36m0.4492\u001b[0m       0.7287        0.5631  2.1133\n",
      "     17        \u001b[36m0.4471\u001b[0m       0.7449        0.5235  2.1563\n",
      "     18        0.4475       0.7600        0.4875  2.2016\n",
      "     19        0.4486       0.7292        0.5735  2.1651\n",
      "     20        0.4485       0.7569        0.5090  2.1535\n",
      "2 offsideeffect 2.0571578835189572 2.081614069635804 1.65942364807452 1.3829371205104457 2.187675629532856\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6015\u001b[0m       \u001b[32m0.7215\u001b[0m        \u001b[35m0.5442\u001b[0m  2.1536\n",
      "      2        \u001b[36m0.5476\u001b[0m       \u001b[32m0.7327\u001b[0m        \u001b[35m0.5258\u001b[0m  2.1581\n",
      "      3        \u001b[36m0.5129\u001b[0m       \u001b[32m0.7368\u001b[0m        \u001b[35m0.5186\u001b[0m  2.1184\n",
      "      4        \u001b[36m0.4838\u001b[0m       0.7300        0.5289  2.1188\n",
      "      5        \u001b[36m0.4722\u001b[0m       0.7273        0.5383  2.1308\n",
      "      6        \u001b[36m0.4588\u001b[0m       0.7179        0.5887  2.1364\n",
      "      7        \u001b[36m0.4581\u001b[0m       0.7266        0.5532  2.1261\n",
      "      8        0.4599       0.7270        0.5507  2.1746\n",
      "      9        \u001b[36m0.4557\u001b[0m       0.7258        0.5670  2.1059\n",
      "     10        0.4639       \u001b[32m0.7371\u001b[0m        \u001b[35m0.5153\u001b[0m  2.1038\n",
      "     11        0.4590       0.7269        0.5551  2.0951\n",
      "     12        \u001b[36m0.4487\u001b[0m       0.7341        0.5297  2.1011\n",
      "     13        0.4580       \u001b[32m0.7412\u001b[0m        \u001b[35m0.5147\u001b[0m  2.1585\n",
      "     14        0.4647       0.7098        0.5983  2.1475\n",
      "     15        0.4610       0.7082        0.6021  2.1414\n",
      "     16        0.4533       0.7343        0.5428  2.1400\n",
      "     17        0.4493       0.7391        0.5574  2.1559\n",
      "     18        0.4525       0.7131        0.6101  2.1394\n",
      "     19        0.4671       0.7350        0.5270  2.1466\n",
      "     20        0.4625       0.7078        0.6069  2.1533\n",
      "3 offsideeffect 2.631888806754937 2.658233532859734 1.9628828926073232 1.5792940207883093 2.855275349616831\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.5960\u001b[0m       \u001b[32m0.7093\u001b[0m        \u001b[35m0.5512\u001b[0m  2.1516\n",
      "      2        \u001b[36m0.5481\u001b[0m       \u001b[32m0.7285\u001b[0m        \u001b[35m0.5420\u001b[0m  2.1479\n",
      "      3        \u001b[36m0.5146\u001b[0m       0.7134        0.5524  2.1030\n",
      "      4        \u001b[36m0.4840\u001b[0m       0.6993        0.5660  2.1023\n",
      "      5        \u001b[36m0.4676\u001b[0m       \u001b[32m0.7403\u001b[0m        \u001b[35m0.5196\u001b[0m  2.1186\n",
      "      6        \u001b[36m0.4671\u001b[0m       0.6258        0.6623  2.1187\n",
      "      7        \u001b[36m0.4631\u001b[0m       \u001b[32m0.7587\u001b[0m        \u001b[35m0.4879\u001b[0m  2.1107\n",
      "      8        \u001b[36m0.4612\u001b[0m       0.7025        0.5636  2.1069\n",
      "      9        0.4662       0.7500        0.5004  2.1686\n",
      "     10        0.4826       0.6770        0.5622  2.2725\n",
      "     11        0.4793       0.6470        0.5956  2.2330\n",
      "     12        0.4704       0.6442        0.5998  2.1859\n",
      "     13        0.4671       0.5930        0.6577  2.1144\n",
      "     14        \u001b[36m0.4559\u001b[0m       0.6433        0.6087  2.1149\n",
      "     15        0.4637       0.7017        0.5471  2.1862\n",
      "     16        0.4667       0.6568        0.5940  2.1465\n",
      "     17        0.4667       0.5929        0.6377  2.1439\n",
      "     18        0.4747       0.5247        0.6957  2.9226\n",
      "     19        0.4728       0.6067        0.6436  2.5968\n",
      "     20        0.4726       0.5785        0.6741  2.5299\n",
      "4 offsideeffect 3.301622481683549 3.2358740302921825 2.5511833651509113 2.508277141414081 3.2857202921510478\n",
      "offsideeffect 0.6603244963367099 0.6471748060584365 0.5102366730301823 0.5016554282828162 0.6571440584302095\n"
     ]
    }
   ],
   "source": [
    "do_prepare_data = False\n",
    "do_train_model = True\n",
    "kfold_nsplits = 5\n",
    "similaritiesToRun = df_paperIndividualScores['Similarity']\n",
    "# similaritiesToRun = [\"sideeffect\"]\n",
    "\n",
    "for similarity in similaritiesToRun:\n",
    "    input_fea = pathInput+DS1_path+\"/\" + similarity + \"_Jacarrd_sim.csv\"\n",
    "    input_lab = pathInput+DS1_path+\"/drug_drug_matrix.csv\"\n",
    "    dataPicklePath = pathPickles+\"data_X_y_\" + similarity + \"_Jaccard.p\"\n",
    "    \n",
    "    # Prepare data if not available\n",
    "    if do_prepare_data:\n",
    "        X,y = prepare_data(input_fea, input_lab, seperate = False)\n",
    "\n",
    "        with open(dataPicklePath, 'wb') as f:\n",
    "            pickle.dump([X, y], f)\n",
    "\n",
    "    # Load X,y and split in to train, test\n",
    "    with open(dataPicklePath, 'rb') as f:\n",
    "        X, y = pickle.load(f)\n",
    "    \n",
    "    X = X.astype(np.float32)\n",
    "    y = y.astype(np.int64)    \n",
    "    \n",
    "    \n",
    "    # Define model\n",
    "    D_in, H1, H2, D_out, drop = X.shape[1], 300, 400, 2, 0.5\n",
    "    str_hidden_layers_params = \"-H1-\" + str(H1) + \"-H2-\" + str(H2)\n",
    "    callbacks = []\n",
    "    \n",
    "    AUROC, AUPR, F1, Rec, Prec = 0,0,0,0,0\n",
    "    kFoldSplit = getStratifiedKFoldSplit(X,y,n_splits=kfold_nsplits)\n",
    "    for i, indices in enumerate(kFoldSplit):\n",
    "        train_index = indices[0]\n",
    "        test_index = indices[1]\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "        # Create Network Classifier\n",
    "        Xy_test = skorch.dataset.Dataset(X_test, y_test)\n",
    "        net = getNDDClassifier(D_in, H1, H2, D_out, drop, Xy_test)        \n",
    "        \n",
    "        # Fit and save OR load model\n",
    "        modelPicklePath = pathPickles+\"model_params/model_params_fold\" + str(i) + \"_\" + str_hidden_layers_params+ \"_\" + similarity + \".p\"\n",
    "        if do_train_model:\n",
    "            net.fit(X_train, y_train)\n",
    "            net.save_params(f_params=modelPicklePath)\n",
    "        else:\n",
    "            net.initialize()  # This is important!\n",
    "            net.load_params(f_params=modelPicklePath)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = net.predict(X_test)\n",
    "        lr_probs = soft(net.forward(X_test))[:,1]\n",
    "        lr_precision, lr_recall, _ = precision_recall_curve(y_test, lr_probs)\n",
    "\n",
    "        AUROC += roc_auc_score(y_test, y_pred)\n",
    "        AUPR += auc(lr_recall, lr_precision)\n",
    "        F1 += f1_score(y_test, y_pred)\n",
    "        Rec += recall_score(y_test, y_pred)\n",
    "        Prec += precision_score(y_test, y_pred)\n",
    "        \n",
    "        print(i, similarity, AUROC, AUPR, F1, Rec, Prec)\n",
    "        \n",
    "    \n",
    "    AUROC, AUPR, F1, Rec, Prec = avgMetrics(AUROC, AUPR, F1, Rec, Prec, kfold_nsplits)\n",
    "    print(similarity, AUROC, AUPR, F1, Rec, Prec)\n",
    "    \n",
    "    # Fill replicated metrics\n",
    "    updateSimilarityDF(df_replicatedIndividualScores, similarity, AUROC, AUPR, F1, Rec, Prec)\n",
    "    \n",
    "# Write CSV\n",
    "writeReplicatedIndividualScoresCSV(net, df_replicatedIndividualScores, pathRuns, str_hidden_layers_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare to Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Similarity    AUC   AUPR  F-measure  Recall  Precision\n",
      "0           chem  0.631  0.455      0.527   0.899      0.373\n",
      "1         target  0.787  0.642      0.617   0.721      0.540\n",
      "2    transporter  0.682  0.568      0.519   0.945      0.358\n",
      "3         enzyme  0.734  0.599      0.552   0.579      0.529\n",
      "4        pathway  0.767  0.623      0.587   0.650      0.536\n",
      "5     indication  0.802  0.654      0.632   0.740      0.551\n",
      "6     sideeffect  0.778  0.601      0.619   0.748      0.528\n",
      "7  offsideeffect  0.782  0.606      0.617   0.764      0.517\n"
     ]
    }
   ],
   "source": [
    "print(df_paperIndividualScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Similarity    AUC   AUPR  F-measure  Recall  Precision\n",
      "0           chem  0.514  0.368      0.137   0.152      0.551\n",
      "1         target  0.634  0.527      0.494   0.453      0.561\n",
      "2    transporter  0.605  0.484      0.427   0.358      0.558\n",
      "3         enzyme  0.615  0.497      0.459   0.431      0.537\n",
      "4        pathway  0.618  0.505      0.467   0.429      0.529\n",
      "5     indication  0.681  0.596      0.567   0.555      0.581\n",
      "6     sideeffect  0.604  0.551      0.410   0.418      0.606\n",
      "7  offsideeffect  0.660  0.647      0.510   0.502      0.657\n"
     ]
    }
   ],
   "source": [
    "print(df_replicatedIndividualScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "diff_metrics = ['AUC', 'AUPR', 'F-measure', 'Recall', 'Precision']\n",
    "df_diff = df_paperIndividualScores[diff_metrics] - df_replicatedIndividualScores[diff_metrics]\n",
    "df_diff_abs = df_diff.abs()\n",
    "df_diff_percent = (df_diff_abs / df_paperIndividualScores[diff_metrics]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC</th>\n",
       "      <th>AUPR</th>\n",
       "      <th>F-measure</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.117</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.747</td>\n",
       "      <td>-0.178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.153</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.268</td>\n",
       "      <td>-0.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.077</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.587</td>\n",
       "      <td>-0.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.119</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.148</td>\n",
       "      <td>-0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.149</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.121</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.185</td>\n",
       "      <td>-0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.174</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.330</td>\n",
       "      <td>-0.078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.122</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.262</td>\n",
       "      <td>-0.140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     AUC   AUPR  F-measure  Recall  Precision\n",
       "0  0.117  0.087      0.390   0.747     -0.178\n",
       "1  0.153  0.115      0.123   0.268     -0.021\n",
       "2  0.077  0.084      0.092   0.587     -0.200\n",
       "3  0.119  0.102      0.093   0.148     -0.008\n",
       "4  0.149  0.118      0.120   0.221      0.007\n",
       "5  0.121  0.058      0.065   0.185     -0.030\n",
       "6  0.174  0.050      0.209   0.330     -0.078\n",
       "7  0.122 -0.041      0.107   0.262     -0.140"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fba72801470>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from seaborn import heatmap\n",
    "heatmap(df_diff, yticklabels=df_paperIndividualScores[\"Similarity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fbab8513940>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "heatmap(df_diff_abs, yticklabels=df_paperIndividualScores[\"Similarity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fbab8482a20>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "heatmap(df_diff_percent, yticklabels=df_paperIndividualScores[\"Similarity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0453187"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(df_paperIndividualScores[diff_metrics],\n",
    "                   df_replicatedIndividualScores[diff_metrics])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
