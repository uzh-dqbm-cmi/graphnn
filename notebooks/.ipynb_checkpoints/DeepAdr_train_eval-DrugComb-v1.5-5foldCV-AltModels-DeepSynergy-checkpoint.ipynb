{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import ogb\n",
    "from tqdm import tqdm\n",
    "import hiplot as hip\n",
    "from copy import deepcopy\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import Subset, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cluster/home/skyriakos/chemprop_run/git/notebooks\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')\n",
    "import deepadr\n",
    "from deepadr.dataset import *\n",
    "from deepadr.utilities import *\n",
    "from deepadr.run_workflow import *\n",
    "from deepadr.chemfeatures import *\n",
    "from deepadr.hyphelperflat import *\n",
    "# from deepadr.model_gnn import GCN as testGCN\n",
    "from deepadr.model_gnn_ogb import GNN, DeepAdr_SiameseTrf, ExpressionNN\n",
    "# from deepadr.model_attn_siamese import *\n",
    "from ogb.graphproppred import Evaluator\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import Draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tdc.single_pred import Tox\n",
    "# from tdc.multi_pred import DDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata_dir = '../data/raw/'\n",
    "processed_dir = '../data/processed/'\n",
    "up_dir = '..'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of GPUs available: 1\n",
      "cuda:0, name:GeForce GTX 1080 Ti\n",
      "total memory available: 10.91650390625 GB\n",
      "total memory allocated on device: 0.0 GB\n",
      "max memory allocated on device: 0.0 GB\n",
      "total memory cached on device: 0.0 GB\n",
      "max memory cached  on device: 0.0 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_available_cuda_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gpu = torch.cuda.device_count()\n",
    "n_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_cpu = get_device(to_gpu=False)\n",
    "# device_gpu = get_device(True, index=0)\n",
    "\n",
    "# fdtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 1.9.1\n",
      "CUDA: 11.1\n",
      "3.9.9 | packaged by conda-forge | (main, Dec 20 2021, 02:41:03) \n",
      "[GCC 9.4.0]\n"
     ]
    }
   ],
   "source": [
    "print(\"torch:\", torch.__version__)\n",
    "print(\"CUDA:\", torch.version.cuda)\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(torch.cuda.memory_summary(device=device_gpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = 'total_thresh' #'total_thresh'\n",
    "score_val = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v_1: GNN\n",
    "# v_2: Alt Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TDC Tox\n",
    "# DSdataset_name = 'SynergxZloewe' #'OncoPolyPharmacology' #'DrugComb'\n",
    "DSdataset_name = f'DrugComb_{score}_{score_val}' #'DrugComb'\n",
    "\n",
    "\n",
    "#fname_suffix = ds_config[\"fname_suffix\"]\n",
    "# similarity_types = ['chem']\n",
    "# kernel_option = 'sqeuclidean'\n",
    "data_fname = 'data_v2'\n",
    "# interact_matfname = ds_config[\"interact_matfname\"]\n",
    "# exp_iden = 'simtypeall'\n",
    "# ddi_interaction_labels_pth = ds_config[\"ddi_interaction_labels_pth\"]\n",
    "\n",
    "# up_dir, processed_dir, DSdataset_name, data_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path_current_dir /cluster/home/skyriakos/chemprop_run/git/deepadr\n",
      "path_current_dir /cluster/home/skyriakos/chemprop_run/git/deepadr\n",
      "path_current_dir /cluster/home/skyriakos/chemprop_run/git/deepadr\n",
      "path_current_dir /cluster/home/skyriakos/chemprop_run/git/deepadr\n",
      "/cluster/home/skyriakos/chemprop_run/git/data/processed/DrugComb_total_thresh_4/data_v2\n"
     ]
    }
   ],
   "source": [
    "targetdata_dir = create_directory(os.path.join(processed_dir, DSdataset_name, data_fname))\n",
    "targetdata_dir_raw = create_directory(os.path.join(targetdata_dir, \"raw\"))\n",
    "targetdata_dir_processed = create_directory(os.path.join(targetdata_dir, \"processed\"))\n",
    "targetdata_dir_exp = create_directory(os.path.join(targetdata_dir, \"experiments\"))\n",
    "# # ReaderWriter.dump_data(dpartitions, os.path.join(targetdata_dir, 'data_partitions.pkl'))\n",
    "print(targetdata_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "xFlat = ReaderWriter.read_data(os.path.join(targetdata_dir_raw, 'X_flat.pkl'))\n",
    "y = ReaderWriter.read_data(os.path.join(targetdata_dir_raw, 'y.pkl'))\n",
    "expression = ReaderWriter.read_data(os.path.join(targetdata_dir_raw, 'expression.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25757, 18])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xFlatMat = torch.stack([torch.cat(i) for i in list(xFlat.values())])\n",
    "xFlatMat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1661,  0.2518,  1.3373,  ..., -0.4085,  0.8581,  0.8979],\n",
       "        [ 0.1661,  0.2518,  1.3373,  ..., -0.4085,  0.8581,  0.8979],\n",
       "        [ 0.1661,  0.2518,  1.3373,  ..., -0.4085,  0.8581,  0.8979],\n",
       "        ...,\n",
       "        [-0.0373, -1.4322,  1.5411,  ..., -1.2711,  1.2013,  0.9537],\n",
       "        [-0.0373, -1.4322,  1.5411,  ..., -1.2711,  1.2013,  0.9537],\n",
       "        [-0.0373, -1.4322,  1.5411,  ..., -1.2711,  1.2013,  0.9537]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25757, 926])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.cat([xFlatMat, torch.tensor(expression)], dim=1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # Make sure to first run the \"data_generation\" notebook first\n",
    "\n",
    "# dataset = MoleculeDataset(root=targetdata_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print()\n",
    "# print(f'Dataset: {dataset}:')\n",
    "# print('====================')\n",
    "# print(f'Number of graphs: {len(dataset)}')\n",
    "# print(f'Number of features: {dataset.num_features}')\n",
    "# print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "# # data0 = dataset[0]  # Get the first graph object.\n",
    "\n",
    "# # print()\n",
    "# # print(data)\n",
    "# # print('=============================================================')\n",
    "\n",
    "# # # Gather some statistics about the first graph.\n",
    "# # print(f'Number of nodes: {data.num_nodes}')\n",
    "# # print(f'Number of edges: {data.num_edges}')\n",
    "# # print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "# # print(f'Contains isolated nodes: {data.contains_isolated_nodes()}')\n",
    "# # print(f'Contains self-loops: {data.contains_self_loops()}')\n",
    "# # print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data0.expression.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data0.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.tensors[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used_dataset = dataset\n",
    "\n",
    "# If you want to use a smaller subset of the dataset for testing\n",
    "# smaller_dataset_len = int(len(dataset)/1)\n",
    "# used_dataset = dataset[:smaller_dataset_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(used_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dataset.data.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25757, 926])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25757,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fold_partitions = get_stratified_partitions(dataset.data.y,\n",
    "#                                             num_folds=5,\n",
    "#                                             valid_set_portion=0.1,\n",
    "#                                             random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_num: 0\n",
      "train data\n",
      "class: 0 norm count: 0.5764128559102675\n",
      "class: 1 norm count: 0.4235871440897325\n",
      "validation data\n",
      "class: 0 norm count: 0.5764192139737991\n",
      "class: 1 norm count: 0.42358078602620086\n",
      "test data\n",
      "class: 0 norm count: 0.5764751552795031\n",
      "class: 1 norm count: 0.4235248447204969\n",
      "\n",
      "-------------------------\n",
      "fold_num: 1\n",
      "train data\n",
      "class: 0 norm count: 0.5764128559102675\n",
      "class: 1 norm count: 0.4235871440897325\n",
      "validation data\n",
      "class: 0 norm count: 0.5764192139737991\n",
      "class: 1 norm count: 0.42358078602620086\n",
      "test data\n",
      "class: 0 norm count: 0.5764751552795031\n",
      "class: 1 norm count: 0.4235248447204969\n",
      "\n",
      "-------------------------\n",
      "fold_num: 2\n",
      "train data\n",
      "class: 0 norm count: 0.5764356969533567\n",
      "class: 1 norm count: 0.4235643030466433\n",
      "validation data\n",
      "class: 0 norm count: 0.5764192139737991\n",
      "class: 1 norm count: 0.42358078602620086\n",
      "test data\n",
      "class: 0 norm count: 0.5763929334109882\n",
      "class: 1 norm count: 0.4236070665890118\n",
      "\n",
      "-------------------------\n",
      "fold_num: 3\n",
      "train data\n",
      "class: 0 norm count: 0.5764356969533567\n",
      "class: 1 norm count: 0.4235643030466433\n",
      "validation data\n",
      "class: 0 norm count: 0.5764192139737991\n",
      "class: 1 norm count: 0.42358078602620086\n",
      "test data\n",
      "class: 0 norm count: 0.5763929334109882\n",
      "class: 1 norm count: 0.4236070665890118\n",
      "\n",
      "-------------------------\n",
      "fold_num: 4\n",
      "train data\n",
      "class: 0 norm count: 0.5764356969533567\n",
      "class: 1 norm count: 0.4235643030466433\n",
      "validation data\n",
      "class: 0 norm count: 0.5764192139737991\n",
      "class: 1 norm count: 0.42358078602620086\n",
      "test data\n",
      "class: 0 norm count: 0.5763929334109882\n",
      "class: 1 norm count: 0.4236070665890118\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "fold_partitions = get_stratified_partitions(y,\n",
    "                                            num_folds=5,\n",
    "                                            valid_set_portion=0.1,\n",
    "                                            random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_val_test_frac = [0.7, 0.1, 0.2]\n",
    "# assert sum(train_val_test_frac) == 1\n",
    "\n",
    "# torch.manual_seed(42)\n",
    "# used_dataset = used_dataset.shuffle()\n",
    "\n",
    "# num_train = round(train_val_test_frac[0] * len(used_dataset)) \n",
    "# num_trainval = round((train_val_test_frac[0]+train_val_test_frac[1]) * len(used_dataset))\n",
    "\n",
    "# train_dataset = used_dataset[:num_train]\n",
    "# val_dataset = used_dataset[num_train:num_trainval]\n",
    "# test_dataset = used_dataset[num_trainval:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = Subset(used_dataset, fold_partitions[0]['train'])\n",
    "# val_dataset = Subset(used_dataset, fold_partitions[0]['validation'])\n",
    "# test_dataset = Subset(used_dataset, fold_partitions[0]['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Number of training graphs: {len(train_dataset)}')\n",
    "# print(f'Number of val graphs: {len(val_dataset)}')\n",
    "# print(f'Number of test graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 18544\n",
      "Number of validation graphs: 2061\n",
      "Number of testing graphs: 5152\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training graphs: \"+ str(len(fold_partitions[0]['train'])))\n",
    "print(\"Number of validation graphs: \"+ str(len(fold_partitions[0]['validation'])))\n",
    "print(\"Number of testing graphs: \"+ str(len(fold_partitions[0]['test'])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functools\n",
    "\n",
    "# def compose(*functions):\n",
    "#     return functools.reduce(lambda f, g: lambda x: f(g(x)), functions, lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_np = x.numpy()\n",
    "y_np = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = compose(scaler.fit_transform, np.tanh, scaler.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_np_norm = pipeline(x_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "926"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepsynergy_input_size = x_np_norm.shape[1]\n",
    "deepsynergy_input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(range(len(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x2ba9e70f6220>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = TensorDataset(torch.tensor(x_np_norm),torch.tensor(y), torch.tensor(ids))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Synergy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training params total_thresh 4\n",
    "tp = {\n",
    "    \"batch_size\" : 300,\n",
    "    \"num_epochs\" : 100,\n",
    "    \n",
    "    \"emb_dim\" : 300,\n",
    "    \"gnn_type\" : \"gatv2\",\n",
    "    \"num_layer\" : 5,\n",
    "    \"graph_pooling\" : \"mean\", #attention\n",
    "    \n",
    "    \"input_embed_dim\" : None,\n",
    "    \"gene_embed_dim\": 1,\n",
    "    \"num_attn_heads\" : 2,\n",
    "    \"num_transformer_units\" : 1,\n",
    "    \"p_dropout\" : 0.3,\n",
    "#     \"nonlin_func\" : nn.ReLU(),\n",
    "    \"mlp_embed_factor\" : 2,\n",
    "    \"pooling_mode\" : 'attn',\n",
    "    \"dist_opt\" : 'cosine',\n",
    "\n",
    "    \"base_lr\" : 3e-5, #3e-4\n",
    "    \"max_lr_mul\": 5,\n",
    "    \"l2_reg\" : 1e-5,\n",
    "    \"loss_w\" : 1.,\n",
    "    \"margin_v\" : 1.,\n",
    "\n",
    "    \"expression_dim\" : 64,\n",
    "    \"expression_input_size\" : 908,\n",
    "    \"exp_H1\" : 500,\n",
    "    \"exp_H2\" : 400\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "926"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp['deepsynergy_input_size'] = deepsynergy_input_size\n",
    "tp['deepsynergy_input_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training params total_thresh 3\n",
    "# tp = {\n",
    "#     \"batch_size\" : 300,\n",
    "#     \"num_epochs\" : 200,\n",
    "    \n",
    "#     \"emb_dim\" : 300,\n",
    "#     \"gnn_type\" : \"gatv2\",\n",
    "#     \"num_layer\" : 5,\n",
    "#     \"graph_pooling\" : \"mean\", #attention\n",
    "    \n",
    "#     \"input_embed_dim\" : None,\n",
    "#     \"gene_embed_dim\": 1,\n",
    "#     \"num_attn_heads\" : 2,\n",
    "#     \"num_transformer_units\" : 1,\n",
    "#     \"p_dropout\" : 0.3,\n",
    "# #     \"nonlin_func\" : nn.ReLU(),\n",
    "#     \"mlp_embed_factor\" : 2,\n",
    "#     \"pooling_mode\" : 'attn',\n",
    "#     \"dist_opt\" : 'cosine',\n",
    "\n",
    "#     \"base_lr\" : 3e-5, #3e-4\n",
    "#     \"max_lr_mul\": 10,\n",
    "#     \"l2_reg\" : 1e-8,\n",
    "#     \"loss_w\" : 0.3,\n",
    "#     \"margin_v\" : 1.,\n",
    "\n",
    "#     \"expression_dim\" : 64,\n",
    "#     \"expression_input_size\" : 908,\n",
    "#     \"exp_H1\" : 500,\n",
    "#     \"exp_H2\" : 400\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targetdata_dir_exp = create_directory(os.path.join(targetdata_dir, \"experiments\"))\n",
    "# targetdata_dir_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device_gpu = get_device(True, index=0)\n",
    "# exp_dir = create_directory(os.path.join(targetdata_dir_exp, \"exp0\"))\n",
    "# tphp = generate_tp_hp(tp, hyperparam_space[0], hp_names)\n",
    "# partition = fold_partitions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_exp(tphp, device_gpu, exp_dir,partition=fold_partitions[0], used_dataset=used_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spawn_q_process(q_process):\n",
    "    print(\">>> spawning hyperparam search process\")\n",
    "    q_process.start()\n",
    "    \n",
    "def join_q_process(q_process):\n",
    "    q_process.join()\n",
    "    print(\"<<< joined hyperparam search process\")\n",
    "    \n",
    "def create_q_process(queue, used_dataset, gpu_num, tphp, exp_dir, partition): #\n",
    "#     fold_gpu_map = {0:gpu_num}\n",
    "    return mp.Process(target=deepadr.hyphelperflat.run_exp_flat, args=(queue, used_dataset, gpu_num, tphp, exp_dir, partition)) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 2022-03-24_13-44-17\n",
      "path_current_dir /cluster/home/skyriakos/chemprop_run/git/deepadr\n",
      "path_current_dir /cluster/home/skyriakos/chemprop_run/git/deepadr\n",
      ">>> spawning hyperparam search process\n",
      "gpu: cuda:0\n",
      "DS model:\n",
      " DeepSynergy(\n",
      "  (fc1): Linear(in_features=926, out_features=8192, bias=True)\n",
      "  (fc2): Linear(in_features=8192, out_features=4096, bias=True)\n",
      "  (fc3): Linear(in_features=4096, out_features=2, bias=True)\n",
      "  (drop_in): Dropout(p=0.2, inplace=False)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (log_softmax): LogSoftmax(dim=-1)\n",
      ")\n",
      "=====Epoch 0\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 62/62 [00:01<00:00, 37.54it/s]\n",
      "Iteration:  19%|█▉        | 12/62 [00:00<00:00, 119.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 62/62 [00:00<00:00, 136.17it/s]\n",
      "Iteration: 100%|██████████| 62/62 [00:00<00:00, 141.88it/s]\n",
      "Iteration: 100%|██████████| 62/62 [00:00<00:00, 121.87it/s]\n",
      "Iteration:   0%|          | 0/62 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train':  best_epoch_indx:0\n",
      " auc:0.7268136175020405 \n",
      " apur:0.6891809227474259 \n",
      " f1:0.6049950049950049 \n",
      " precision:0.6343575418994414 \n",
      " recall:0.578230426479949 \n",
      ", 'Validation':  best_epoch_indx:0\n",
      " auc:0.7268136175020405 \n",
      " apur:0.6891809227474259 \n",
      " f1:0.6049950049950049 \n",
      " precision:0.6343575418994414 \n",
      " recall:0.578230426479949 \n",
      ", 'Test':  best_epoch_indx:0\n",
      " auc:0.7268136175020405 \n",
      " apur:0.6891809227471782 \n",
      " f1:0.6049950049950049 \n",
      " precision:0.6343575418994414 \n",
      " recall:0.578230426479949 \n",
      "}\n",
      "=====Epoch 1\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 62/62 [00:01<00:00, 46.70it/s]\n",
      "Iteration:   0%|          | 0/62 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 62/62 [00:00<00:00, 138.14it/s]\n",
      "Iteration: 100%|██████████| 62/62 [00:00<00:00, 143.01it/s]\n",
      "Iteration: 100%|██████████| 62/62 [00:00<00:00, 143.14it/s]\n",
      "Iteration:   0%|          | 0/62 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train':  best_epoch_indx:1\n",
      " auc:0.7476875904537638 \n",
      " apur:0.7101914200083672 \n",
      " f1:0.6103654759446624 \n",
      " precision:0.6643691938867246 \n",
      " recall:0.5644812221514959 \n",
      ", 'Validation':  best_epoch_indx:1\n",
      " auc:0.7476875904537638 \n",
      " apur:0.7101914200083672 \n",
      " f1:0.6103654759446624 \n",
      " precision:0.6643691938867246 \n",
      " recall:0.5644812221514959 \n",
      ", 'Test':  best_epoch_indx:1\n",
      " auc:0.7476875904537638 \n",
      " apur:0.7101914200083672 \n",
      " f1:0.6103654759446624 \n",
      " precision:0.6643691938867246 \n",
      " recall:0.5644812221514959 \n",
      "}\n",
      "=====Epoch 2\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 62/62 [00:01<00:00, 44.76it/s]\n",
      "Iteration:   0%|          | 0/62 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 62/62 [00:00<00:00, 135.73it/s]\n",
      "Iteration: 100%|██████████| 62/62 [00:00<00:00, 140.35it/s]\n",
      "Iteration: 100%|██████████| 62/62 [00:00<00:00, 142.77it/s]\n",
      "Iteration:   0%|          | 0/62 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train':  best_epoch_indx:2\n",
      " auc:0.7598700937607619 \n",
      " apur:0.7235106449102122 \n",
      " f1:0.6425129841687004 \n",
      " precision:0.6317991631799164 \n",
      " recall:0.6535964353914704 \n",
      ", 'Validation':  best_epoch_indx:2\n",
      " auc:0.7598700937607619 \n",
      " apur:0.7235106449102122 \n",
      " f1:0.6425129841687004 \n",
      " precision:0.6317991631799164 \n",
      " recall:0.6535964353914704 \n",
      ", 'Test':  best_epoch_indx:2\n",
      " auc:0.7598700937607619 \n",
      " apur:0.7235106449102122 \n",
      " f1:0.6425129841687004 \n",
      " precision:0.6317991631799164 \n",
      " recall:0.6535964353914704 \n",
      "}\n",
      "=====Epoch 3\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 62/62 [00:01<00:00, 46.66it/s]\n",
      "Iteration:   0%|          | 0/62 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 62/62 [00:00<00:00, 120.07it/s]\n",
      "Iteration: 100%|██████████| 62/62 [00:00<00:00, 141.79it/s]\n",
      "Iteration: 100%|██████████| 62/62 [00:00<00:00, 143.37it/s]\n",
      "Iteration:   0%|          | 0/62 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train':  best_epoch_indx:3\n",
      " auc:0.7616538867926057 \n",
      " apur:0.724917501097381 \n",
      " f1:0.6196793808734108 \n",
      " precision:0.677648481184827 \n",
      " recall:0.5708465945257798 \n",
      ", 'Validation':  best_epoch_indx:3\n",
      " auc:0.7616538867926055 \n",
      " apur:0.724917501097381 \n",
      " f1:0.6196793808734108 \n",
      " precision:0.677648481184827 \n",
      " recall:0.5708465945257798 \n",
      ", 'Test':  best_epoch_indx:3\n",
      " auc:0.7616538867926055 \n",
      " apur:0.724917501097381 \n",
      " f1:0.6196793808734108 \n",
      " precision:0.677648481184827 \n",
      " recall:0.5708465945257798 \n",
      "}\n",
      "=====Epoch 4\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 62/62 [00:01<00:00, 46.63it/s]\n",
      "Iteration:   0%|          | 0/62 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 62/62 [00:00<00:00, 136.31it/s]\n",
      "Iteration: 100%|██████████| 62/62 [00:00<00:00, 115.29it/s]\n",
      "Iteration: 100%|██████████| 62/62 [00:00<00:00, 143.52it/s]\n",
      "Iteration:   0%|          | 0/62 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train':  best_epoch_indx:4\n",
      " auc:0.7746924609253736 \n",
      " apur:0.7377701558472822 \n",
      " f1:0.6514475194490499 \n",
      " precision:0.6526127507346365 \n",
      " recall:0.6502864417568428 \n",
      ", 'Validation':  best_epoch_indx:4\n",
      " auc:0.7746924609253736 \n",
      " apur:0.7377701558472822 \n",
      " f1:0.6514475194490499 \n",
      " precision:0.6526127507346365 \n",
      " recall:0.6502864417568428 \n",
      ", 'Test':  best_epoch_indx:4\n",
      " auc:0.7746924609253736 \n",
      " apur:0.7377701558472822 \n",
      " f1:0.6514475194490499 \n",
      " precision:0.6526127507346365 \n",
      " recall:0.6502864417568428 \n",
      "}\n",
      "=====Epoch 5\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 62/62 [00:01<00:00, 46.63it/s]\n",
      "Iteration:   0%|          | 0/62 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 62/62 [00:00<00:00, 137.60it/s]\n",
      "Iteration: 100%|██████████| 62/62 [00:00<00:00, 142.04it/s]\n",
      "Iteration: 100%|██████████| 62/62 [00:00<00:00, 124.11it/s]\n",
      "Iteration:   0%|          | 0/62 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train':  best_epoch_indx:5\n",
      " auc:0.781329926319728 \n",
      " apur:0.7464614559449834 \n",
      " f1:0.6634140238944142 \n",
      " precision:0.6521151008362026 \n",
      " recall:0.67511139401655 \n",
      ", 'Validation':  best_epoch_indx:5\n",
      " auc:0.781329926319728 \n",
      " apur:0.7464614559449834 \n",
      " f1:0.6634140238944142 \n",
      " precision:0.6521151008362026 \n",
      " recall:0.67511139401655 \n",
      ", 'Test':  best_epoch_indx:5\n",
      " auc:0.781329926319728 \n",
      " apur:0.746461455942819 \n",
      " f1:0.6634140238944142 \n",
      " precision:0.6521151008362026 \n",
      " recall:0.67511139401655 \n",
      "}\n",
      "=====Epoch 6\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 62/62 [00:01<00:00, 46.64it/s]\n",
      "Iteration:   0%|          | 0/62 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 62/62 [00:00<00:00, 137.57it/s]\n",
      "Iteration: 100%|██████████| 62/62 [00:00<00:00, 143.62it/s]\n",
      "Iteration: 100%|██████████| 62/62 [00:00<00:00, 142.49it/s]\n",
      "Iteration:   0%|          | 0/62 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train':  best_epoch_indx:6\n",
      " auc:0.780754368980431 \n",
      " apur:0.7454306977437108 \n",
      " f1:0.6586946944426962 \n",
      " precision:0.6513567338810057 \n",
      " recall:0.6661998726925525 \n",
      ", 'Validation':  best_epoch_indx:6\n",
      " auc:0.780754368980431 \n",
      " apur:0.7454306977437108 \n",
      " f1:0.6586946944426962 \n",
      " precision:0.6513567338810057 \n",
      " recall:0.6661998726925525 \n",
      ", 'Test':  best_epoch_indx:6\n",
      " auc:0.7807543630253628 \n",
      " apur:0.7454306919846682 \n",
      " f1:0.6586946944426962 \n",
      " precision:0.6513567338810057 \n",
      " recall:0.6661998726925525 \n",
      "}\n",
      "=====Epoch 7\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 62/62 [00:01<00:00, 44.83it/s]\n",
      "Iteration:   0%|          | 0/62 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 62/62 [00:00<00:00, 138.19it/s]\n",
      "Iteration: 100%|██████████| 62/62 [00:00<00:00, 143.20it/s]\n",
      "Iteration: 100%|██████████| 62/62 [00:00<00:00, 143.00it/s]\n",
      "Iteration:   0%|          | 0/62 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train':  best_epoch_indx:7\n",
      " auc:0.7933563353796733 \n",
      " apur:0.7591433617719621 \n",
      " f1:0.6643795714933004 \n",
      " precision:0.6757966815907295 \n",
      " recall:0.653341820496499 \n",
      ", 'Validation':  best_epoch_indx:7\n",
      " auc:0.7933563353796733 \n",
      " apur:0.7591433617719621 \n",
      " f1:0.6643795714933004 \n",
      " precision:0.6757966815907295 \n",
      " recall:0.653341820496499 \n",
      ", 'Test':  best_epoch_indx:7\n",
      " auc:0.7933563353796733 \n",
      " apur:0.7591433617719621 \n",
      " f1:0.6643795714933004 \n",
      " precision:0.6757966815907295 \n",
      " recall:0.653341820496499 \n",
      "}\n",
      "=====Epoch 8\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 62/62 [00:01<00:00, 46.68it/s]\n",
      "Iteration:   0%|          | 0/62 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 62/62 [00:00<00:00, 120.84it/s]\n",
      "Iteration: 100%|██████████| 62/62 [00:00<00:00, 142.20it/s]\n",
      "Iteration: 100%|██████████| 62/62 [00:00<00:00, 142.90it/s]\n",
      "Iteration:   0%|          | 0/62 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train':  best_epoch_indx:8\n",
      " auc:0.7982746142768353 \n",
      " apur:0.7655470581032704 \n",
      " f1:0.6839712918660287 \n",
      " precision:0.6450084602368866 \n",
      " recall:0.7279439847231063 \n",
      ", 'Validation':  best_epoch_indx:8\n",
      " auc:0.7982746142768353 \n",
      " apur:0.7655470581024904 \n",
      " f1:0.6839712918660287 \n",
      " precision:0.6450084602368866 \n",
      " recall:0.7279439847231063 \n",
      ", 'Test':  best_epoch_indx:8\n",
      " auc:0.7982746142768353 \n",
      " apur:0.7655470581032704 \n",
      " f1:0.6839712918660287 \n",
      " precision:0.6450084602368866 \n",
      " recall:0.7279439847231063 \n",
      "}\n",
      "=====Epoch 9\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 62/62 [00:01<00:00, 46.65it/s]\n",
      "Iteration:   0%|          | 0/62 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 62/62 [00:00<00:00, 138.30it/s]\n",
      "Iteration: 100%|██████████| 62/62 [00:00<00:00, 124.17it/s]\n",
      "Iteration: 100%|██████████| 62/62 [00:00<00:00, 142.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train':  best_epoch_indx:9\n",
      " auc:0.8073128118110917 \n",
      " apur:0.774425161799104 \n",
      " f1:0.6756210471409011 \n",
      " precision:0.6924619085805934 \n",
      " recall:0.6595798854232973 \n",
      ", 'Validation':  best_epoch_indx:9\n",
      " auc:0.8073128118110917 \n",
      " apur:0.774425161799104 \n",
      " f1:0.6756210471409011 \n",
      " precision:0.6924619085805934 \n",
      " recall:0.6595798854232973 \n",
      ", 'Test':  best_epoch_indx:9\n",
      " auc:0.8073128118110917 \n",
      " apur:0.774425161799104 \n",
      " f1:0.6756210471409011 \n",
      " precision:0.6924619085805934 \n",
      " recall:0.6595798854232973 \n",
      "}\n",
      "Finished training!\n",
      "<<< joined hyperparam search process\n",
      "released_gpu_num: 0\n"
     ]
    }
   ],
   "source": [
    "import torch.multiprocessing as mp\n",
    "mp.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "queue = mp.Queue()\n",
    "q_processes = []\n",
    "\n",
    "# partition = fold_partitions[0]\n",
    "time_stamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "print(\"Start: \" + time_stamp)\n",
    "\n",
    "for q_i in range(min(n_gpu, len(fold_partitions))):\n",
    "#     device_gpu = get_device(True, index=q_i)\n",
    "    partition = fold_partitions[q_i]\n",
    "    exp_dir = create_directory(os.path.join(targetdata_dir_exp, \"fold_\"+str(q_i)+\"_\"+time_stamp))\n",
    "    create_directory(os.path.join(exp_dir, \"predictions\"))\n",
    "\n",
    "#     tphp = generate_tp_hp(tp, hyperparam_space[q_i], hp_names)\n",
    "    \n",
    "    q_process = create_q_process(queue, dataset, q_i, tp, exp_dir, partition)\n",
    "    q_processes.append(q_process)\n",
    "    spawn_q_process(q_process)\n",
    "\n",
    "spawned_processes = n_gpu\n",
    "    \n",
    "# for q_i in range(len(hyperparam_space)):\n",
    "for q_i in range(min(n_gpu, len(fold_partitions))):\n",
    "    join_q_process(q_processes[q_i])\n",
    "    released_gpu_num = queue.get()\n",
    "    print(\"released_gpu_num:\", released_gpu_num)\n",
    "#     if(spawned_processes < len(hyperparam_space)):\n",
    "# #         device_gpu = get_device(True, index=q_i)\n",
    "#         time_stamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "#         exp_dir = create_directory(os.path.join(targetdata_dir_exp, \"exp_\"+str(q_i)+\"_\"+time_stamp))\n",
    "#         tphp = generate_tp_hp(tp, hyperparam_space[q_i], hp_names)\n",
    "\n",
    "#         q_process = create_q_process(queue, used_dataset, released_gpu_num, tphp, exp_dir, partition)\n",
    "#         q_processes.append(q_process)\n",
    "#         spawn_q_process(q_process)\n",
    "#         spawned_processes = spawned_processes + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End: 2022-03-24_13-44-58\n"
     ]
    }
   ],
   "source": [
    "print(\"End: \" + datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
