{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "# import ogb\n",
    "from tqdm import tqdm\n",
    "# import hiplot as hip\n",
    "from copy import deepcopy\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/data/chemprop_run/git/notebooks/AltModels\n",
      "/opt/data/chemprop_run/git\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "# cwd_parent = os.path.abspath(os.path.join(cwd, os.pardir))\n",
    "cwd_parent = os.path.abspath(os.path.join(cwd, '../../'))\n",
    "print(cwd_parent)\n",
    "\n",
    "sys.path.append(cwd_parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepadr\n",
    "from deepadr.rapidsutilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata_dir = '../data/raw/'\n",
    "processed_dir = '../data/processed/'\n",
    "up_dir = '..'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = 'total_thresh'\n",
    "score_val = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DSdataset_name = f'DrugComb_{score}_{score_val}'\n",
    "\n",
    "# v_1: GNN\n",
    "# v_2: Alt Models (Baseline)\n",
    "data_fname = 'data_v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path_current_dir /opt/data/chemprop_run/git/deepadr\n",
      "path_current_dir /opt/data/chemprop_run/git/deepadr\n",
      "path_current_dir /opt/data/chemprop_run/git/deepadr\n",
      "path_current_dir /opt/data/chemprop_run/git/deepadr\n"
     ]
    }
   ],
   "source": [
    "targetdata_dir = create_directory(os.path.join(processed_dir, DSdataset_name, data_fname))\n",
    "targetdata_dir_raw = create_directory(os.path.join(targetdata_dir, \"raw\"))\n",
    "targetdata_dir_processed = create_directory(os.path.join(targetdata_dir, \"processed\"))\n",
    "targetdata_dir_exp = create_directory(os.path.join(targetdata_dir, \"experiments\"))\n",
    "# print(targetdata_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ReaderWriter.read_data(os.path.join(targetdata_dir_raw, 'X_flat_np.pkl'))\n",
    "y = ReaderWriter.read_data(os.path.join(targetdata_dir_raw, 'y.pkl'))\n",
    "# expression = ReaderWriter.read_data(os.path.join(targetdata_dir_raw, 'expression.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_num: 0\n",
      "train data\n",
      "class: 0 norm count: 0.5764128559102675\n",
      "class: 1 norm count: 0.4235871440897325\n",
      "validation data\n",
      "class: 0 norm count: 0.5764192139737991\n",
      "class: 1 norm count: 0.42358078602620086\n",
      "test data\n",
      "class: 0 norm count: 0.5764751552795031\n",
      "class: 1 norm count: 0.4235248447204969\n",
      "\n",
      "-------------------------\n",
      "fold_num: 1\n",
      "train data\n",
      "class: 0 norm count: 0.5764128559102675\n",
      "class: 1 norm count: 0.4235871440897325\n",
      "validation data\n",
      "class: 0 norm count: 0.5764192139737991\n",
      "class: 1 norm count: 0.42358078602620086\n",
      "test data\n",
      "class: 0 norm count: 0.5764751552795031\n",
      "class: 1 norm count: 0.4235248447204969\n",
      "\n",
      "-------------------------\n",
      "fold_num: 2\n",
      "train data\n",
      "class: 0 norm count: 0.5764356969533567\n",
      "class: 1 norm count: 0.4235643030466433\n",
      "validation data\n",
      "class: 0 norm count: 0.5764192139737991\n",
      "class: 1 norm count: 0.42358078602620086\n",
      "test data\n",
      "class: 0 norm count: 0.5763929334109882\n",
      "class: 1 norm count: 0.4236070665890118\n",
      "\n",
      "-------------------------\n",
      "fold_num: 3\n",
      "train data\n",
      "class: 0 norm count: 0.5764356969533567\n",
      "class: 1 norm count: 0.4235643030466433\n",
      "validation data\n",
      "class: 0 norm count: 0.5764192139737991\n",
      "class: 1 norm count: 0.42358078602620086\n",
      "test data\n",
      "class: 0 norm count: 0.5763929334109882\n",
      "class: 1 norm count: 0.4236070665890118\n",
      "\n",
      "-------------------------\n",
      "fold_num: 4\n",
      "train data\n",
      "class: 0 norm count: 0.5764356969533567\n",
      "class: 1 norm count: 0.4235643030466433\n",
      "validation data\n",
      "class: 0 norm count: 0.5764192139737991\n",
      "class: 1 norm count: 0.42358078602620086\n",
      "test data\n",
      "class: 0 norm count: 0.5763929334109882\n",
      "class: 1 norm count: 0.4236070665890118\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "fold_partitions = get_stratified_partitions(y,\n",
    "                                            num_folds=5,\n",
    "                                            valid_set_portion=0.1,\n",
    "                                            random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 18544\n",
      "Number of validation graphs: 2061\n",
      "Number of testing graphs: 5152\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training graphs: \"+ str(len(fold_partitions[0]['train'])))\n",
    "print(\"Number of validation graphs: \"+ str(len(fold_partitions[0]['validation'])))\n",
    "print(\"Number of testing graphs: \"+ str(len(fold_partitions[0]['test'])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alt Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "# from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rapids Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "from cuml.linear_model import LogisticRegression as cuLogReg\n",
    "from cuml.explainer import KernelExplainer, PermutationExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_np = x\n",
    "y_np = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_np_scale = scaler.fit_transform(x_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/data/chemprop_run/git/data/processed/DrugComb_total_thresh_4/data_v2/experiments'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetdata_dir_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LogisticRegression(max_iter=10000,\n",
    "#                                       random_state=42,\n",
    "#                                       solver=\"lbfgs\",\n",
    "#                                       penalty='l2',\n",
    "#                                       l1_ratio=None,\n",
    "#                                       verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cuLogReg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 2022-06-20_17-42-19\n",
      "path_current_dir /opt/data/chemprop_run/git/deepadr\n",
      "path_current_dir /opt/data/chemprop_run/git/deepadr\n",
      "End: 2022-06-20_17-42-21\n"
     ]
    }
   ],
   "source": [
    "time_stamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "print(\"Start: \" + time_stamp)\n",
    "\n",
    "q_i = 0\n",
    "\n",
    "# for q_i in range(len(fold_partitions)):\n",
    "partition = fold_partitions[q_i]\n",
    "\n",
    "ids_train = partition['train']\n",
    "ids_test = partition['test']\n",
    "\n",
    "part_train_x = np.take(x_np_scale, ids_train, axis=0)\n",
    "part_train_y = np.take(y_np, ids_train, axis=0)\n",
    "\n",
    "part_test_x = np.take(x_np_scale, ids_test, axis=0)\n",
    "part_test_y = np.take(y_np, ids_test, axis=0)\n",
    "\n",
    "cu_train_x = cudf.DataFrame(part_train_x)\n",
    "cu_train_y = cudf.Series(part_train_y)\n",
    "\n",
    "cu_test_x = cudf.DataFrame(part_test_x)\n",
    "cu_test_y = cudf.Series(part_test_y)\n",
    "\n",
    "exp_dir = create_directory(os.path.join(targetdata_dir_exp, \"fold_\"+str(q_i)+\"_\"+time_stamp))\n",
    "create_directory(os.path.join(exp_dir, \"predictions\"))\n",
    "\n",
    "model_fit = model.fit(cu_train_x, cu_train_y)\n",
    "\n",
    "# prob_scores_arr = model_fit.predict_proba(cu_test_x)\n",
    "# pred_class = model_fit.predict(cu_test_x)\n",
    "# ref_class = cu_test_y\n",
    "# epoch = 0\n",
    "# dsettype = 'test'\n",
    "\n",
    "# dset_perf = perfmetric_report(pred_class, ref_class, prob_scores_arr[:,1], epoch,\n",
    "#                           outlog = os.path.join(exp_dir, dsettype + \".log\"))\n",
    "\n",
    "# predictions_df = build_predictions_df(ids_test, ref_class, pred_class, prob_scores_arr)\n",
    "# predictions_df.to_csv(os.path.join(exp_dir, 'predictions', f'epoch_{epoch}_predictions_{dsettype}.csv'))\n",
    "    \n",
    "print(\"End: \" + datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_stamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "# print(\"Start: \" + time_stamp)\n",
    "\n",
    "# q_i = 0\n",
    "\n",
    "# # for q_i in range(len(fold_partitions)):\n",
    "# partition = fold_partitions[q_i]\n",
    "\n",
    "# ids_train = partition['train']\n",
    "# ids_test = partition['test']\n",
    "\n",
    "# part_train_x = np.take(x_np_scale, ids_train, axis=0)\n",
    "# part_train_y = np.take(y_np, ids_train, axis=0)\n",
    "\n",
    "# part_test_x = np.take(x_np_scale, ids_test, axis=0)\n",
    "# part_test_y = np.take(y_np, ids_test, axis=0)\n",
    "\n",
    "# exp_dir = create_directory(os.path.join(targetdata_dir_exp, \"fold_\"+str(q_i)+\"_\"+time_stamp))\n",
    "# create_directory(os.path.join(exp_dir, \"predictions\"))\n",
    "\n",
    "# model_fit = model.fit(part_train_x, part_train_y)\n",
    "\n",
    "# prob_scores_arr = model_fit.predict_proba(part_test_x)\n",
    "# pred_class = model_fit.predict(part_test_x)\n",
    "# ref_class = part_test_y\n",
    "# epoch = 0\n",
    "# dsettype = 'test'\n",
    "\n",
    "# dset_perf = perfmetric_report(pred_class, ref_class, prob_scores_arr[:,1], epoch,\n",
    "#                           outlog = os.path.join(exp_dir, dsettype + \".log\"))\n",
    "\n",
    "# predictions_df = build_predictions_df(ids_test, ref_class, pred_class, prob_scores_arr)\n",
    "# predictions_df.to_csv(os.path.join(exp_dir, 'predictions', f'epoch_{epoch}_predictions_{dsettype}.csv'))\n",
    "    \n",
    "# print(\"End: \" + datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_stamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "# print(\"Start: \" + time_stamp)\n",
    "\n",
    "# for q_i in range(len(fold_partitions)):\n",
    "#     partition = fold_partitions[q_i]\n",
    "    \n",
    "#     ids_train = partition['train']\n",
    "#     ids_test = partition['test']\n",
    "    \n",
    "#     part_train_x = np.take(x_np_scale, ids_train, axis=0)\n",
    "#     part_train_y = np.take(y_np, ids_train, axis=0)\n",
    "    \n",
    "#     part_test_x = np.take(x_np_scale, ids_test, axis=0)\n",
    "#     part_test_y = np.take(y_np, ids_test, axis=0)\n",
    "    \n",
    "#     exp_dir = create_directory(os.path.join(targetdata_dir_exp, \"fold_\"+str(q_i)+\"_\"+time_stamp))\n",
    "#     create_directory(os.path.join(exp_dir, \"predictions\"))\n",
    "    \n",
    "#     model_fit = model.fit(part_train_x, part_train_y)\n",
    "    \n",
    "#     prob_scores_arr = model_fit.predict_proba(part_test_x)\n",
    "#     pred_class = model_fit.predict(part_test_x)\n",
    "#     ref_class = part_test_y\n",
    "#     epoch = 0\n",
    "#     dsettype = 'test'\n",
    "\n",
    "#     dset_perf = perfmetric_report(pred_class, ref_class, prob_scores_arr[:,1], epoch,\n",
    "#                               outlog = os.path.join(exp_dir, dsettype + \".log\"))\n",
    "    \n",
    "#     predictions_df = build_predictions_df(ids_test, ref_class, pred_class, prob_scores_arr)\n",
    "#     predictions_df.to_csv(os.path.join(exp_dir, 'predictions', f'epoch_{epoch}_predictions_{dsettype}.csv'))\n",
    "    \n",
    "# print(\"End: \" + datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cu_explainer = KernelExplainer(\n",
    "#     model=model.predict,\n",
    "#     data=cu_train_x,\n",
    "#     is_gpu_model=True,\n",
    "#     random_state=42)\n",
    "\n",
    "cu_explainer = PermutationExplainer(\n",
    "    model=model.predict,\n",
    "    data=cu_train_x,\n",
    "    is_gpu_model=True,\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "std::bad_alloc: out_of_memory: CUDA error at: /workspace/.conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:70: cudaErrorMemoryAllocation out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cu_shap_values \u001b[38;5;241m=\u001b[39m \u001b[43mcu_explainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcu_test_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m cu_shap_values\n",
      "File \u001b[0;32mcuml/explainer/permutation_shap.pyx:256\u001b[0m, in \u001b[0;36mcuml.explainer.permutation_shap.PermutationExplainer.shap_values\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcuml/explainer/base.pyx:274\u001b[0m, in \u001b[0;36mcuml.explainer.base.SHAPBase._explain\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/envs/rapids-core/lib/python3.9/site-packages/cupy/_creation/basic.py:209\u001b[0m, in \u001b[0;36mzeros\u001b[0;34m(shape, dtype, order)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mzeros\u001b[39m(shape, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns a new array of given shape and dtype, filled with zeros.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    207\u001b[0m \n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mcupy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m     a\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mmemset_async(\u001b[38;5;241m0\u001b[39m, a\u001b[38;5;241m.\u001b[39mnbytes)\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "File \u001b[0;32mcupy/_core/core.pyx:167\u001b[0m, in \u001b[0;36mcupy._core.core.ndarray.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/cuda/memory.pyx:718\u001b[0m, in \u001b[0;36mcupy.cuda.memory.alloc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/envs/rapids-core/lib/python3.9/site-packages/rmm/rmm.py:226\u001b[0m, in \u001b[0;36mrmm_cupy_allocator\u001b[0;34m(nbytes)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo module named \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcupy\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    225\u001b[0m stream \u001b[38;5;241m=\u001b[39m Stream(obj\u001b[38;5;241m=\u001b[39mcupy\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mget_current_stream())\n\u001b[0;32m--> 226\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[43mlibrmm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDeviceBuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m dev_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m buf\u001b[38;5;241m.\u001b[39mptr \u001b[38;5;28;01melse\u001b[39;00m cupy\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mget_device_id()\n\u001b[1;32m    228\u001b[0m mem \u001b[38;5;241m=\u001b[39m cupy\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mUnownedMemory(\n\u001b[1;32m    229\u001b[0m     ptr\u001b[38;5;241m=\u001b[39mbuf\u001b[38;5;241m.\u001b[39mptr, size\u001b[38;5;241m=\u001b[39mbuf\u001b[38;5;241m.\u001b[39msize, owner\u001b[38;5;241m=\u001b[39mbuf, device_id\u001b[38;5;241m=\u001b[39mdev_id\n\u001b[1;32m    230\u001b[0m )\n",
      "File \u001b[0;32mdevice_buffer.pyx:88\u001b[0m, in \u001b[0;36mrmm._lib.device_buffer.DeviceBuffer.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: std::bad_alloc: out_of_memory: CUDA error at: /workspace/.conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:70: cudaErrorMemoryAllocation out of memory"
     ]
    }
   ],
   "source": [
    "cu_shap_values = cu_explainer.shap_values(cu_test_x)\n",
    "cu_shap_values  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_stamp = \"2022-06-20_15-29-16\"\n",
    "\n",
    "import glob\n",
    "exp_dirs = glob.glob(targetdata_dir_exp+\"/fold_*_\"+time_stamp)\n",
    "len(exp_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = []\n",
    "\n",
    "for edir in exp_dirs:\n",
    "    fold = edir.split(\"/\")[-1].split('_')[1]\n",
    "    with open(os.path.join(edir, \"test.log\")) as f:\n",
    "        lines = f.read().splitlines()\n",
    "        folds.append([fold, float(lines[18]), float(lines[22])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUPR</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.828877</td>\n",
       "      <td>0.811873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.842180</td>\n",
       "      <td>0.825904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.830765</td>\n",
       "      <td>0.813108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.836757</td>\n",
       "      <td>0.819945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.835450</td>\n",
       "      <td>0.819092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          AUPR       AUC\n",
       "Fold                    \n",
       "3     0.828877  0.811873\n",
       "4     0.842180  0.825904\n",
       "2     0.830765  0.813108\n",
       "1     0.836757  0.819945\n",
       "0     0.835450  0.819092"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_folds = pd.DataFrame(data=folds, columns=[\"Fold\",\"AUPR\", \"AUC\"]).set_index(\"Fold\")\n",
    "# df_folds[\"Fscore\"] = df_folds.apply(lambda x: (2*x[0]*x[1])/(x[0]+x[1]), axis=1) # harmonic mean of AUC, AUPR\n",
    "df_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8348058 , 0.81798434])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_folds.mean(axis=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8263094695742067"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_score(*df_folds.mean(axis=0).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
