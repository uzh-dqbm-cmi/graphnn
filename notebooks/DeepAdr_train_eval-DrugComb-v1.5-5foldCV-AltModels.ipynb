{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import ogb\n",
    "from tqdm import tqdm\n",
    "import hiplot as hip\n",
    "from copy import deepcopy\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import Subset, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cluster/home/skyriakos/chemprop_run/git/notebooks\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')\n",
    "import deepadr\n",
    "from deepadr.dataset import *\n",
    "from deepadr.utilities import *\n",
    "from deepadr.run_workflow import *\n",
    "from deepadr.chemfeatures import *\n",
    "from deepadr.hyphelper import *\n",
    "# from deepadr.model_gnn import GCN as testGCN\n",
    "from deepadr.model_gnn_ogb import GNN, DeepAdr_SiameseTrf, ExpressionNN\n",
    "# from deepadr.model_attn_siamese import *\n",
    "from ogb.graphproppred import Evaluator\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import Draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tdc.single_pred import Tox\n",
    "# from tdc.multi_pred import DDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata_dir = '../data/raw/'\n",
    "processed_dir = '../data/processed/'\n",
    "up_dir = '..'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of GPUs available: 8\n",
      "cuda:0, name:GeForce GTX 1080 Ti\n",
      "total memory available: 10.91650390625 GB\n",
      "total memory allocated on device: 0.0 GB\n",
      "max memory allocated on device: 0.0 GB\n",
      "total memory cached on device: 0.0 GB\n",
      "max memory cached  on device: 0.0 GB\n",
      "\n",
      "cuda:1, name:GeForce GTX 1080 Ti\n",
      "total memory available: 10.91650390625 GB\n",
      "total memory allocated on device: 0.0 GB\n",
      "max memory allocated on device: 0.0 GB\n",
      "total memory cached on device: 0.0 GB\n",
      "max memory cached  on device: 0.0 GB\n",
      "\n",
      "cuda:2, name:GeForce GTX 1080 Ti\n",
      "total memory available: 10.91650390625 GB\n",
      "total memory allocated on device: 0.0 GB\n",
      "max memory allocated on device: 0.0 GB\n",
      "total memory cached on device: 0.0 GB\n",
      "max memory cached  on device: 0.0 GB\n",
      "\n",
      "cuda:3, name:GeForce GTX 1080 Ti\n",
      "total memory available: 10.91650390625 GB\n",
      "total memory allocated on device: 0.0 GB\n",
      "max memory allocated on device: 0.0 GB\n",
      "total memory cached on device: 0.0 GB\n",
      "max memory cached  on device: 0.0 GB\n",
      "\n",
      "cuda:4, name:GeForce GTX 1080 Ti\n",
      "total memory available: 10.91650390625 GB\n",
      "total memory allocated on device: 0.0 GB\n",
      "max memory allocated on device: 0.0 GB\n",
      "total memory cached on device: 0.0 GB\n",
      "max memory cached  on device: 0.0 GB\n",
      "\n",
      "cuda:5, name:GeForce GTX 1080 Ti\n",
      "total memory available: 10.91650390625 GB\n",
      "total memory allocated on device: 0.0 GB\n",
      "max memory allocated on device: 0.0 GB\n",
      "total memory cached on device: 0.0 GB\n",
      "max memory cached  on device: 0.0 GB\n",
      "\n",
      "cuda:6, name:GeForce GTX 1080 Ti\n",
      "total memory available: 10.91650390625 GB\n",
      "total memory allocated on device: 0.0 GB\n",
      "max memory allocated on device: 0.0 GB\n",
      "total memory cached on device: 0.0 GB\n",
      "max memory cached  on device: 0.0 GB\n",
      "\n",
      "cuda:7, name:GeForce GTX 1080 Ti\n",
      "total memory available: 10.91650390625 GB\n",
      "total memory allocated on device: 0.0 GB\n",
      "max memory allocated on device: 0.0 GB\n",
      "total memory cached on device: 0.0 GB\n",
      "max memory cached  on device: 0.0 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_available_cuda_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gpu = torch.cuda.device_count()\n",
    "n_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_cpu = get_device(to_gpu=False)\n",
    "# device_gpu = get_device(True, index=0)\n",
    "\n",
    "# fdtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 1.9.1\n",
      "CUDA: 11.1\n",
      "3.9.9 | packaged by conda-forge | (main, Dec 20 2021, 02:41:03) \n",
      "[GCC 9.4.0]\n"
     ]
    }
   ],
   "source": [
    "print(\"torch:\", torch.__version__)\n",
    "print(\"CUDA:\", torch.version.cuda)\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(torch.cuda.memory_summary(device=device_gpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = 'total_thresh'\n",
    "score_val = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v_1: GNN\n",
    "# v_2: Alt Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TDC Tox\n",
    "# DSdataset_name = 'SynergxZloewe' #'OncoPolyPharmacology' #'DrugComb'\n",
    "DSdataset_name = f'DrugComb_{score}_{score_val}' #'DrugComb'\n",
    "\n",
    "\n",
    "#fname_suffix = ds_config[\"fname_suffix\"]\n",
    "# similarity_types = ['chem']\n",
    "# kernel_option = 'sqeuclidean'\n",
    "data_fname = 'data_v2'\n",
    "# interact_matfname = ds_config[\"interact_matfname\"]\n",
    "# exp_iden = 'simtypeall'\n",
    "# ddi_interaction_labels_pth = ds_config[\"ddi_interaction_labels_pth\"]\n",
    "\n",
    "# up_dir, processed_dir, DSdataset_name, data_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path_current_dir /cluster/home/skyriakos/chemprop_run/git/deepadr\n",
      "path_current_dir /cluster/home/skyriakos/chemprop_run/git/deepadr\n",
      "path_current_dir /cluster/home/skyriakos/chemprop_run/git/deepadr\n",
      "path_current_dir /cluster/home/skyriakos/chemprop_run/git/deepadr\n",
      "/cluster/home/skyriakos/chemprop_run/git/data/processed/DrugComb_total_thresh_3/data_v2\n"
     ]
    }
   ],
   "source": [
    "targetdata_dir = create_directory(os.path.join(processed_dir, DSdataset_name, data_fname))\n",
    "targetdata_dir_raw = create_directory(os.path.join(targetdata_dir, \"raw\"))\n",
    "targetdata_dir_processed = create_directory(os.path.join(targetdata_dir, \"processed\"))\n",
    "targetdata_dir_exp = create_directory(os.path.join(targetdata_dir, \"experiments\"))\n",
    "# # ReaderWriter.dump_data(dpartitions, os.path.join(targetdata_dir, 'data_partitions.pkl'))\n",
    "print(targetdata_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "xFlat = ReaderWriter.read_data(os.path.join(targetdata_dir_raw, 'X_flat.pkl'))\n",
    "y = ReaderWriter.read_data(os.path.join(targetdata_dir_raw, 'y.pkl'))\n",
    "expression = ReaderWriter.read_data(os.path.join(targetdata_dir_raw, 'expression.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64140, 18])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xFlatMat = torch.stack([torch.cat(i) for i in list(xFlat.values())])\n",
    "xFlatMat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1790,  0.2520,  1.3661,  ..., -0.4165,  0.9541,  0.9106],\n",
       "        [ 0.1790,  0.2520,  1.3661,  ..., -0.4165,  0.9541,  0.9106],\n",
       "        [ 0.1790,  0.2520,  1.3661,  ..., -0.4165,  0.9541,  0.9106],\n",
       "        ...,\n",
       "        [-0.0309, -1.4168,  1.5472,  ..., -1.2610,  1.2777,  0.9653],\n",
       "        [-0.0309, -1.4168,  1.5472,  ..., -1.2610,  1.2777,  0.9653],\n",
       "        [-0.0309, -1.4168,  1.5472,  ..., -1.2610,  1.2777,  0.9653]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64140, 926])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.cat([xFlatMat, torch.tensor(expression)], dim=1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # Make sure to first run the \"data_generation\" notebook first\n",
    "\n",
    "# dataset = MoleculeDataset(root=targetdata_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print()\n",
    "# print(f'Dataset: {dataset}:')\n",
    "# print('====================')\n",
    "# print(f'Number of graphs: {len(dataset)}')\n",
    "# print(f'Number of features: {dataset.num_features}')\n",
    "# print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "# # data0 = dataset[0]  # Get the first graph object.\n",
    "\n",
    "# # print()\n",
    "# # print(data)\n",
    "# # print('=============================================================')\n",
    "\n",
    "# # # Gather some statistics about the first graph.\n",
    "# # print(f'Number of nodes: {data.num_nodes}')\n",
    "# # print(f'Number of edges: {data.num_edges}')\n",
    "# # print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "# # print(f'Contains isolated nodes: {data.contains_isolated_nodes()}')\n",
    "# # print(f'Contains self-loops: {data.contains_self_loops()}')\n",
    "# # print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data0.expression.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data0.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.tensors[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used_dataset = dataset\n",
    "\n",
    "# If you want to use a smaller subset of the dataset for testing\n",
    "# smaller_dataset_len = int(len(dataset)/1)\n",
    "# used_dataset = dataset[:smaller_dataset_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(used_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dataset.data.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fold_partitions = get_stratified_partitions(dataset.data.y,\n",
    "#                                             num_folds=5,\n",
    "#                                             valid_set_portion=0.1,\n",
    "#                                             random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_num: 0\n",
      "train data\n",
      "class: 0 norm count: 0.6346470333477696\n",
      "class: 1 norm count: 0.3653529666522304\n",
      "validation data\n",
      "class: 0 norm count: 0.6346453624318005\n",
      "class: 1 norm count: 0.3653546375681995\n",
      "test data\n",
      "class: 0 norm count: 0.6347053320860617\n",
      "class: 1 norm count: 0.3652946679139383\n",
      "\n",
      "-------------------------\n",
      "fold_num: 1\n",
      "train data\n",
      "class: 0 norm count: 0.6346470333477696\n",
      "class: 1 norm count: 0.3653529666522304\n",
      "validation data\n",
      "class: 0 norm count: 0.6346453624318005\n",
      "class: 1 norm count: 0.3653546375681995\n",
      "test data\n",
      "class: 0 norm count: 0.6347053320860617\n",
      "class: 1 norm count: 0.3652946679139383\n",
      "\n",
      "-------------------------\n",
      "fold_num: 2\n",
      "train data\n",
      "class: 0 norm count: 0.634668687743612\n",
      "class: 1 norm count: 0.36533131225638804\n",
      "validation data\n",
      "class: 0 norm count: 0.6346453624318005\n",
      "class: 1 norm count: 0.3653546375681995\n",
      "test data\n",
      "class: 0 norm count: 0.6346273776114749\n",
      "class: 1 norm count: 0.3653726223885251\n",
      "\n",
      "-------------------------\n",
      "fold_num: 3\n",
      "train data\n",
      "class: 0 norm count: 0.634668687743612\n",
      "class: 1 norm count: 0.36533131225638804\n",
      "validation data\n",
      "class: 0 norm count: 0.6346453624318005\n",
      "class: 1 norm count: 0.3653546375681995\n",
      "test data\n",
      "class: 0 norm count: 0.6346273776114749\n",
      "class: 1 norm count: 0.3653726223885251\n",
      "\n",
      "-------------------------\n",
      "fold_num: 4\n",
      "train data\n",
      "class: 0 norm count: 0.634668687743612\n",
      "class: 1 norm count: 0.36533131225638804\n",
      "validation data\n",
      "class: 0 norm count: 0.6346453624318005\n",
      "class: 1 norm count: 0.3653546375681995\n",
      "test data\n",
      "class: 0 norm count: 0.6346273776114749\n",
      "class: 1 norm count: 0.3653726223885251\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "fold_partitions = get_stratified_partitions(y,\n",
    "                                            num_folds=5,\n",
    "                                            valid_set_portion=0.1,\n",
    "                                            random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_val_test_frac = [0.7, 0.1, 0.2]\n",
    "# assert sum(train_val_test_frac) == 1\n",
    "\n",
    "# torch.manual_seed(42)\n",
    "# used_dataset = used_dataset.shuffle()\n",
    "\n",
    "# num_train = round(train_val_test_frac[0] * len(used_dataset)) \n",
    "# num_trainval = round((train_val_test_frac[0]+train_val_test_frac[1]) * len(used_dataset))\n",
    "\n",
    "# train_dataset = used_dataset[:num_train]\n",
    "# val_dataset = used_dataset[num_train:num_trainval]\n",
    "# test_dataset = used_dataset[num_trainval:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = Subset(used_dataset, fold_partitions[0]['train'])\n",
    "# val_dataset = Subset(used_dataset, fold_partitions[0]['validation'])\n",
    "# test_dataset = Subset(used_dataset, fold_partitions[0]['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Number of training graphs: {len(train_dataset)}')\n",
    "# print(f'Number of val graphs: {len(val_dataset)}')\n",
    "# print(f'Number of test graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 46180\n",
      "Number of validation graphs: 5132\n",
      "Number of testing graphs: 12828\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training graphs: \"+ str(len(fold_partitions[0]['train'])))\n",
    "print(\"Number of validation graphs: \"+ str(len(fold_partitions[0]['validation'])))\n",
    "print(\"Number of testing graphs: \"+ str(len(fold_partitions[0]['test'])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alt Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_np = x.numpy()\n",
    "y_np = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_np_scale = scaler.fit_transform(x_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part_train_x = np.take(x_np_scale, fold_partitions[0]['train'], axis=0)\n",
    "# part_train_y = np.take(y_np, fold_partitions[0]['train'], axis=0)\n",
    "# part_train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part_test_x = np.take(x_np_scale, fold_partitions[0]['test'], axis=0)\n",
    "# part_test_y = np.take(y_np, fold_partitions[0]['test'], axis=0)\n",
    "# part_test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# clf = LogisticRegression(max_iter=10000, random_state=42, solver=\"sag\").fit(part_train_x, part_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/cluster/home/skyriakos/chemprop_run/git/data/processed/DrugComb_total_thresh_3/data_v2/experiments'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetdata_dir_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=10000,\n",
    "                                      random_state=42,\n",
    "                                      solver=\"lbfgs\",\n",
    "                                      penalty='l2',\n",
    "                                      l1_ratio=None,\n",
    "                                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 2022-03-24_16-02-38\n",
      "path_current_dir /cluster/home/skyriakos/chemprop_run/git/deepadr\n",
      "path_current_dir /cluster/home/skyriakos/chemprop_run/git/deepadr\n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          927     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  3.20095D+04    |proj g|=  6.21800D+03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  2.73016D+04    |proj g|=  8.28918D+01\n",
      "\n",
      "At iterate  100    f=  2.72535D+04    |proj g|=  2.50565D+01\n",
      "\n",
      "At iterate  150    f=  2.72456D+04    |proj g|=  1.76240D+01\n",
      "\n",
      "At iterate  200    f=  2.72443D+04    |proj g|=  6.11669D+00\n",
      "\n",
      "At iterate  250    f=  2.72437D+04    |proj g|=  4.42540D+00\n",
      "\n",
      "At iterate  300    f=  2.72433D+04    |proj g|=  4.34195D+00\n",
      "\n",
      "At iterate  350    f=  2.72432D+04    |proj g|=  2.23140D+00\n",
      "\n",
      "At iterate  400    f=  2.72432D+04    |proj g|=  1.14586D+00\n",
      "\n",
      "At iterate  450    f=  2.72431D+04    |proj g|=  8.71583D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  927    473    511      1     0     0   1.769D+00   2.724D+04\n",
      "  F =   27243.137023162879     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   32.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path_current_dir /cluster/home/skyriakos/chemprop_run/git/deepadr\n",
      "path_current_dir /cluster/home/skyriakos/chemprop_run/git/deepadr\n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          927     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  3.20095D+04    |proj g|=  6.21800D+03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  2.72825D+04    |proj g|=  7.67635D+01\n",
      "\n",
      "At iterate  100    f=  2.72335D+04    |proj g|=  2.98864D+01\n",
      "\n",
      "At iterate  150    f=  2.72249D+04    |proj g|=  1.69856D+01\n",
      "\n",
      "At iterate  200    f=  2.72237D+04    |proj g|=  7.04273D+00\n",
      "\n",
      "At iterate  250    f=  2.72232D+04    |proj g|=  1.07921D+01\n",
      "\n",
      "At iterate  300    f=  2.72228D+04    |proj g|=  5.24106D+00\n",
      "\n",
      "At iterate  350    f=  2.72227D+04    |proj g|=  2.40418D+00\n",
      "\n",
      "At iterate  400    f=  2.72226D+04    |proj g|=  2.25580D+00\n",
      "\n",
      "At iterate  450    f=  2.72226D+04    |proj g|=  1.60986D+00\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  927    498    536      1     0     0   2.103D+00   2.722D+04\n",
      "  F =   27222.592164797625     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   33.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path_current_dir /cluster/home/skyriakos/chemprop_run/git/deepadr\n",
      "path_current_dir /cluster/home/skyriakos/chemprop_run/git/deepadr\n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          927     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  3.20095D+04    |proj g|=  6.21900D+03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  2.72606D+04    |proj g|=  9.75772D+01\n",
      "\n",
      "At iterate  100    f=  2.72060D+04    |proj g|=  2.09136D+01\n",
      "\n",
      "At iterate  150    f=  2.71975D+04    |proj g|=  1.02692D+01\n",
      "\n",
      "At iterate  200    f=  2.71962D+04    |proj g|=  4.45816D+00\n",
      "\n",
      "At iterate  250    f=  2.71956D+04    |proj g|=  7.12198D+00\n",
      "\n",
      "At iterate  300    f=  2.71951D+04    |proj g|=  6.35656D+00\n",
      "\n",
      "At iterate  350    f=  2.71949D+04    |proj g|=  2.16474D+00\n",
      "\n",
      "At iterate  400    f=  2.71949D+04    |proj g|=  1.50990D+00\n",
      "\n",
      "At iterate  450    f=  2.71949D+04    |proj g|=  1.12388D+00\n",
      "\n",
      "At iterate  500    f=  2.71949D+04    |proj g|=  6.52214D-01\n",
      "\n",
      "At iterate  550    f=  2.71949D+04    |proj g|=  9.22492D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  927    583    625      1     0     0   6.812D-01   2.719D+04\n",
      "  F =   27194.848593395065     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   40.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path_current_dir /cluster/home/skyriakos/chemprop_run/git/deepadr\n",
      "path_current_dir /cluster/home/skyriakos/chemprop_run/git/deepadr\n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          927     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  3.20095D+04    |proj g|=  6.21900D+03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  2.72772D+04    |proj g|=  6.36407D+01\n",
      "\n",
      "At iterate  100    f=  2.72336D+04    |proj g|=  2.92986D+01\n",
      "\n",
      "At iterate  150    f=  2.72264D+04    |proj g|=  1.80911D+01\n",
      "\n",
      "At iterate  200    f=  2.72248D+04    |proj g|=  6.41399D+00\n",
      "\n",
      "At iterate  250    f=  2.72243D+04    |proj g|=  6.06359D+00\n",
      "\n",
      "At iterate  300    f=  2.72240D+04    |proj g|=  3.20023D+00\n",
      "\n",
      "At iterate  350    f=  2.72239D+04    |proj g|=  1.83276D+00\n",
      "\n",
      "At iterate  400    f=  2.72239D+04    |proj g|=  2.89800D+00\n",
      "\n",
      "At iterate  450    f=  2.72238D+04    |proj g|=  1.07058D+00\n",
      "\n",
      "At iterate  500    f=  2.72238D+04    |proj g|=  1.33180D+00\n",
      "\n",
      "At iterate  550    f=  2.72238D+04    |proj g|=  8.17805D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  927    557    596      1     0     0   8.869D-01   2.722D+04\n",
      "  F =   27223.784496266155     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   38.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path_current_dir /cluster/home/skyriakos/chemprop_run/git/deepadr\n",
      "path_current_dir /cluster/home/skyriakos/chemprop_run/git/deepadr\n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          927     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  3.20095D+04    |proj g|=  6.21900D+03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  2.71983D+04    |proj g|=  7.07238D+01\n",
      "\n",
      "At iterate  100    f=  2.71511D+04    |proj g|=  2.45174D+01\n",
      "\n",
      "At iterate  150    f=  2.71439D+04    |proj g|=  1.77910D+01\n",
      "\n",
      "At iterate  200    f=  2.71426D+04    |proj g|=  7.11643D+00\n",
      "\n",
      "At iterate  250    f=  2.71422D+04    |proj g|=  3.90215D+00\n",
      "\n",
      "At iterate  300    f=  2.71419D+04    |proj g|=  4.48465D+00\n",
      "\n",
      "At iterate  350    f=  2.71417D+04    |proj g|=  5.01694D+00\n",
      "\n",
      "At iterate  400    f=  2.71416D+04    |proj g|=  4.02970D+00\n",
      "\n",
      "At iterate  450    f=  2.71416D+04    |proj g|=  8.77997D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  927    490    520      1     0     0   5.029D-01   2.714D+04\n",
      "  F =   27141.601167351328     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   31.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End: 2022-03-24_16-05-39\n"
     ]
    }
   ],
   "source": [
    "time_stamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "print(\"Start: \" + time_stamp)\n",
    "\n",
    "for q_i in range(len(fold_partitions)):\n",
    "# for q_i in range(1):\n",
    "\n",
    "    partition = fold_partitions[q_i]\n",
    "    \n",
    "    ids_train = partition['train']\n",
    "    ids_test = partition['test']\n",
    "    \n",
    "    part_train_x = np.take(x_np_scale, ids_train, axis=0)\n",
    "    part_train_y = np.take(y_np, ids_train, axis=0)\n",
    "    \n",
    "    part_test_x = np.take(x_np_scale, ids_test, axis=0)\n",
    "    part_test_y = np.take(y_np, ids_test, axis=0)\n",
    "    \n",
    "    exp_dir = create_directory(os.path.join(targetdata_dir_exp, \"fold_\"+str(q_i)+\"_\"+time_stamp))\n",
    "    create_directory(os.path.join(exp_dir, \"predictions\"))\n",
    "    \n",
    "    model_fit = model.fit(part_train_x, part_train_y)\n",
    "    \n",
    "    prob_scores_arr = model_fit.predict_proba(part_test_x)\n",
    "    pred_class = model_fit.predict(part_test_x)\n",
    "    ref_class = part_test_y\n",
    "    epoch = 0\n",
    "    dsettype = 'test'\n",
    "\n",
    "    dset_perf = perfmetric_report(pred_class, ref_class, prob_scores_arr[:,1], epoch,\n",
    "                              outlog = os.path.join(exp_dir, dsettype + \".log\"))\n",
    "    \n",
    "    predictions_df = build_predictions_df(ids_test, ref_class, pred_class, prob_scores_arr)\n",
    "    predictions_df.to_csv(os.path.join(exp_dir, 'predictions', f'epoch_{epoch}_predictions_{dsettype}.csv'))\n",
    "    \n",
    "print(\"End: \" + datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time_stamp = \"2022-03-10_18-42-45\"\n",
    "\n",
    "import glob\n",
    "exp_dirs = glob.glob(targetdata_dir_exp+\"/fold_*_\"+time_stamp)\n",
    "len(exp_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = []\n",
    "\n",
    "for edir in exp_dirs:\n",
    "    fold = edir.split(\"/\")[-1].split('_')[1]\n",
    "    with open(os.path.join(edir, \"test.log\")) as f:\n",
    "        lines = f.read().splitlines()\n",
    "#         print(lines[17:23])\n",
    "        folds.append([fold, float(lines[18]), float(lines[22])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUPR</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.599707</td>\n",
       "      <td>0.701583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.594751</td>\n",
       "      <td>0.696580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.588012</td>\n",
       "      <td>0.697524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.593316</td>\n",
       "      <td>0.699101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.590234</td>\n",
       "      <td>0.692597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          AUPR       AUC\n",
       "Fold                    \n",
       "0     0.599707  0.701583\n",
       "1     0.594751  0.696580\n",
       "2     0.588012  0.697524\n",
       "3     0.593316  0.699101\n",
       "4     0.590234  0.692597"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_folds = pd.DataFrame(data=folds, columns=[\"Fold\",\"AUPR\", \"AUC\"]).set_index(\"Fold\")\n",
    "# df_folds[\"Fscore\"] = df_folds.apply(lambda x: (2*x[0]*x[1])/(x[0]+x[1]), axis=1) # harmonic mean of AUC, AUPR\n",
    "df_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5932038 , 0.69747682])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_folds.mean(axis=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6411282414758441"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_score(*df_folds.mean(axis=0).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
