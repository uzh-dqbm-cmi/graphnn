{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import ogb\n",
    "from tqdm import tqdm\n",
    "import hiplot as hip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cluster/home/skyriakos/chemprop_run/git/notebooks\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')\n",
    "import deepadr\n",
    "from deepadr.dataset import *\n",
    "from deepadr.utilities import *\n",
    "from deepadr.run_workflow import *\n",
    "from deepadr.chemfeatures import *\n",
    "# from deepadr.model_gnn import GCN as testGCN\n",
    "from deepadr.model_gnn_ogb import GNN, DeepAdr_SiameseTrf, ExpressionNN\n",
    "# from deepadr.model_attn_siamese import *\n",
    "from ogb.graphproppred import Evaluator\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import Draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tdc.single_pred import Tox\n",
    "# from tdc.multi_pred import DDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata_dir = '../data/raw/'\n",
    "processed_dir = '../data/processed/'\n",
    "up_dir = '..'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of GPUs available: 1\n",
      "cuda:0, name:GeForce GTX 1080 Ti\n",
      "total memory available: 10.91650390625 GB\n",
      "total memory allocated on device: 0.0 GB\n",
      "max memory allocated on device: 0.0 GB\n",
      "total memory cached on device: 0.0 GB\n",
      "max memory cached  on device: 0.0 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_available_cuda_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gpu = torch.cuda.device_count()\n",
    "n_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_cpu = get_device(to_gpu=False)\n",
    "device_gpu = get_device(True, index=0)\n",
    "\n",
    "fdtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 1.9.1\n",
      "CUDA: 11.1\n"
     ]
    }
   ],
   "source": [
    "print(\"torch:\", torch.__version__)\n",
    "print(\"CUDA:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(torch.cuda.memory_summary(device=device_gpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TDC Tox\n",
    "DSdataset_name = 'MatchMakerDrugComb' #'OncoPolyPharmacology' #'DrugComb'\n",
    "\n",
    "#fname_suffix = ds_config[\"fname_suffix\"]\n",
    "similarity_types = ['chem']\n",
    "kernel_option = 'sqeuclidean'\n",
    "data_fname = 'data_v1'\n",
    "# interact_matfname = ds_config[\"interact_matfname\"]\n",
    "# exp_iden = 'simtypeall'\n",
    "# ddi_interaction_labels_pth = ds_config[\"ddi_interaction_labels_pth\"]\n",
    "\n",
    "# up_dir, processed_dir, DSdataset_name, data_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path_current_dir /cluster/home/skyriakos/chemprop_run/git/deepadr\n",
      "path_current_dir /cluster/home/skyriakos/chemprop_run/git/deepadr\n",
      "path_current_dir /cluster/home/skyriakos/chemprop_run/git/deepadr\n",
      "/cluster/home/skyriakos/chemprop_run/git/data/processed/MatchMakerDrugComb/data_v1\n"
     ]
    }
   ],
   "source": [
    "targetdata_dir = create_directory(os.path.join(processed_dir, DSdataset_name, data_fname))\n",
    "targetdata_dir_raw = create_directory(os.path.join(targetdata_dir, \"raw\"))\n",
    "targetdata_dir_processed = create_directory(os.path.join(targetdata_dir, \"processed\"))\n",
    "# # ReaderWriter.dump_data(dpartitions, os.path.join(targetdata_dir, 'data_partitions.pkl'))\n",
    "print(targetdata_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.8 ms, sys: 730 ms, total: 745 ms\n",
      "Wall time: 1.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Make sure to first run the \"data_generation\" notebook first\n",
    "\n",
    "dataset = MoleculeDataset(root=targetdata_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: MoleculeDataset(96671):\n",
      "====================\n",
      "Number of graphs: 96671\n",
      "Number of features: 9\n",
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "data0 = dataset[0]  # Get the first graph object.\n",
    "\n",
    "# print()\n",
    "# print(data)\n",
    "# print('=============================================================')\n",
    "\n",
    "# # Gather some statistics about the first graph.\n",
    "# print(f'Number of nodes: {data.num_nodes}')\n",
    "# print(f'Number of edges: {data.num_edges}')\n",
    "# print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "# print(f'Contains isolated nodes: {data.contains_isolated_nodes()}')\n",
    "# print(f'Contains self-loops: {data.contains_self_loops()}')\n",
    "# print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PairData(edge_index_a=[2, 68], x_a=[31, 9], edge_attr_a=[68, 3], edge_index_b=[2, 76], x_b=[35, 9], edge_attr_b=[76, 3], id=[1], y=[1], expression=[1, 972])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 972])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data0.expression.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0], dtype=torch.int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data0.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96671"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used_dataset = dataset\n",
    "\n",
    "# If you want to use a smaller subset of the dataset for testing\n",
    "smaller_dataset_len = int(len(dataset)/1)\n",
    "used_dataset = dataset[:smaller_dataset_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_num: 0\n",
      "train data\n",
      "class: 0 norm count: 0.9410649119278182\n",
      "class: 1 norm count: 0.058935088072181833\n",
      "validation data\n",
      "class: 0 norm count: 0.9410395655546936\n",
      "class: 1 norm count: 0.05896043444530644\n",
      "test data\n",
      "class: 0 norm count: 0.9410395655546936\n",
      "class: 1 norm count: 0.05896043444530644\n",
      "\n",
      "-------------------------\n",
      "fold_num: 1\n",
      "train data\n",
      "class: 0 norm count: 0.9410513914630116\n",
      "class: 1 norm count: 0.05894860853698835\n",
      "validation data\n",
      "class: 0 norm count: 0.9410395655546936\n",
      "class: 1 norm count: 0.05896043444530644\n",
      "test data\n",
      "class: 0 norm count: 0.941088238336609\n",
      "class: 1 norm count: 0.05891176166339092\n",
      "\n",
      "-------------------------\n",
      "fold_num: 2\n",
      "train data\n",
      "class: 0 norm count: 0.9410513914630116\n",
      "class: 1 norm count: 0.05894860853698835\n",
      "validation data\n",
      "class: 0 norm count: 0.9410395655546936\n",
      "class: 1 norm count: 0.05896043444530644\n",
      "test data\n",
      "class: 0 norm count: 0.941088238336609\n",
      "class: 1 norm count: 0.05891176166339092\n",
      "\n",
      "-------------------------\n",
      "fold_num: 3\n",
      "train data\n",
      "class: 0 norm count: 0.9410657586598279\n",
      "class: 1 norm count: 0.05893424134017212\n",
      "validation data\n",
      "class: 0 norm count: 0.9410395655546936\n",
      "class: 1 norm count: 0.05896043444530644\n",
      "test data\n",
      "class: 0 norm count: 0.9410365159822075\n",
      "class: 1 norm count: 0.05896348401779249\n",
      "\n",
      "-------------------------\n",
      "fold_num: 4\n",
      "train data\n",
      "class: 0 norm count: 0.9410657586598279\n",
      "class: 1 norm count: 0.05893424134017212\n",
      "validation data\n",
      "class: 0 norm count: 0.9410395655546936\n",
      "class: 1 norm count: 0.05896043444530644\n",
      "test data\n",
      "class: 0 norm count: 0.9410365159822075\n",
      "class: 1 norm count: 0.05896348401779249\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "fold_partitions = get_stratified_partitions(used_dataset.data.y[:smaller_dataset_len],\n",
    "                                            num_folds=5, valid_set_portion=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_val_test_frac = [0.7, 0.1, 0.2]\n",
    "# assert sum(train_val_test_frac) == 1\n",
    "\n",
    "# torch.manual_seed(42)\n",
    "# used_dataset = used_dataset.shuffle()\n",
    "\n",
    "# num_train = round(train_val_test_frac[0] * len(used_dataset)) \n",
    "# num_trainval = round((train_val_test_frac[0]+train_val_test_frac[1]) * len(used_dataset))\n",
    "\n",
    "# train_dataset = used_dataset[:num_train]\n",
    "# val_dataset = used_dataset[num_train:num_trainval]\n",
    "# test_dataset = used_dataset[num_trainval:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Subset(used_dataset, fold_partitions[0]['train'])\n",
    "val_dataset = Subset(used_dataset, fold_partitions[0]['validation'])\n",
    "test_dataset = Subset(used_dataset, fold_partitions[0]['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 69602\n",
      "Number of val graphs: 7734\n",
      "Number of test graphs: 19335\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of val graphs: {len(val_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(dataset.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training params\n",
    "tp = {\n",
    "    \"batch_size\" : 512,\n",
    "    \"num_epochs\" : 100,\n",
    "    \n",
    "    \"emb_dim\" : 300,\n",
    "    \"gnn_type\" : \"gatv2\",\n",
    "    \"num_layer\" : 5,\n",
    "    \"graph_pooling\" : \"mean\", #attention\n",
    "    \n",
    "    \"input_embed_dim\" : None,\n",
    "    \"num_attn_heads\" : 2,\n",
    "    \"num_transformer_units\" : 1,\n",
    "    \"p_dropout\" : 0.3,\n",
    "    \"nonlin_func\" : nn.ReLU(),\n",
    "    \"mlp_embed_factor\" : 2,\n",
    "    \"pooling_mode\" : 'attn',\n",
    "    \"dist_opt\" : 'cosine',\n",
    "\n",
    "    \"base_lr\" : 3e-5, #3e-4\n",
    "    \"l2_reg\" : 1e-7,\n",
    "    \"loss_w\" : 0.1,\n",
    "    \"margin_v\" : 1.,\n",
    "\n",
    "    \"expression_dim\" : 64,\n",
    "    \"expression_input_size\" : 972,\n",
    "    \"exp_H1\" : 500,\n",
    "    \"exp_H2\" : 400\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=tp[\"batch_size\"], shuffle=True, follow_batch=['x_a', 'x_b'])\n",
    "valid_loader = DataLoader(val_dataset, batch_size=tp[\"batch_size\"], shuffle=False, follow_batch=['x_a', 'x_b'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=tp[\"batch_size\"], shuffle=False, follow_batch=['x_a', 'x_b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_model = GNN(gnn_type = tp[\"gnn_type\"], \n",
    "#                 num_tasks = dataset.num_classes, \n",
    "                num_layer = tp[\"num_layer\"], \n",
    "                emb_dim = tp[\"emb_dim\"], \n",
    "                drop_ratio = 0.5, \n",
    "                JK = \"last\",\n",
    "                graph_pooling = tp[\"graph_pooling\"],\n",
    "                virtual_node = False,\n",
    "                with_edge_attr=False).to(device=device_gpu, dtype=fdtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = DeepAdr_Transformer(input_size=tp[\"expression_input_size\"],\n",
    "                                        input_embed_dim=tp[\"input_embed_dim\"],\n",
    "                                        num_attn_heads=tp[\"num_attn_heads\"],\n",
    "                                        mlp_embed_factor=tp[\"mlp_embed_factor\"],\n",
    "                                        nonlin_func=tp[\"nonlin_func\"],\n",
    "                                        pdropout=tp[\"p_dropout\"],\n",
    "                                        num_transformer_units=tp[\"num_transformer_units\"],\n",
    "                                        pooling_mode=tp[\"pooling_mode\"]).to(device=device_gpu, dtype=fdtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expression_model = ExpressionNN(D_in=tp[\"expression_input_size\"],\n",
    "#                                 H1=tp[\"exp_H1\"], H2=tp[\"exp_H2\"],\n",
    "#                                 D_out=tp[\"expression_dim\"], drop=0.5).to(device=device_gpu, dtype=fdtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated\n",
      "num classes: 2\n"
     ]
    }
   ],
   "source": [
    "siamese_model = DeepAdr_SiameseTrf(input_dim=tp[\"emb_dim\"],\n",
    "                                   dist=tp[\"dist_opt\"],\n",
    "                                   expression_dim=tp[\"expression_input_size\"],\n",
    "                                   num_classes=dataset.num_classes).to(device=device_gpu, dtype=fdtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lasso_input_dim = 2*tp[\"emb_dim\"]+1+expression_input_size\n",
    "# lasso_out_dim = dataset.num_classes\n",
    "\n",
    "# lassonet_model = LassoNet(lasso_input_dim, lasso_out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models_param = list(gnn_model.parameters()) + list(transformer_model.parameters()) + list(siamese_model.parameters()) + list(expression_model.parameters())\n",
    "models_param = list(gnn_model.parameters()) + list(transformer_model.parameters()) + list(siamese_model.parameters())\n",
    "\n",
    "\n",
    "model_name = \"ogb\"\n",
    "models = [(gnn_model, f'{model_name}_GNN'),\n",
    "          (transformer_model, f'{model_name}_Transformer'),\n",
    "          (siamese_model, f'{model_name}_Siamese'),\n",
    "#           (expression_model, f'{model_name}_Expression'),\n",
    "#           (lassonet_model, f'{model_name}_LassoNet')\n",
    "         ]\n",
    "#models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5313, 8.4829], device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_weights = ReaderWriter.read_data(os.path.join(targetdata_dir_raw, 'y_weights.pkl'))\n",
    "class_weights = torch.tensor(y_weights).type(fdtype).to(device_gpu)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Javascript\n",
    "# display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
    "\n",
    "num_iter = len(train_loader)  # num_train_samples/batch_size\n",
    "c_step_size = int(np.ceil(5*num_iter))  # this should be 2-10 times num_iter\n",
    "\n",
    "base_lr = tp['base_lr']\n",
    "max_lr = 10*base_lr  # 3-5 times base_lr\n",
    "optimizer = torch.optim.Adam(models_param, weight_decay=tp[\"l2_reg\"], lr=base_lr)\n",
    "cyc_scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, step_size_up=c_step_size,\n",
    "                                                mode='triangular', cycle_momentum=False)\n",
    "# optimizer = torch.optim.Adam(models_param, lr=0.001)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# loss_nlll = torch.nn.NLLLoss(weight=class_weights, reduction='mean')  # negative log likelihood loss\n",
    "loss_nlll = torch.nn.NLLLoss(weight=class_weights, reduction='mean')  # negative log likelihood loss\n",
    "loss_contrastive = ContrastiveLoss(0.5, reduction='mean')\n",
    "# loss_mse = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "\n",
    "\n",
    "# evaluator = Evaluator(DSdataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    for m, m_name in models:\n",
    "        m.train()\n",
    "\n",
    "        #            for i_batch, samples_batch in enumerate(data_loader):\n",
    "\n",
    "#     for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "    for i_batch, batch in enumerate(tqdm(train_loader, desc=\"Iteration\")):\n",
    "        batch = batch.to(device_gpu)\n",
    "\n",
    "#         print(\"running batch:\", i_batch)\n",
    "        # x, edge_index, edge_attr, batch\n",
    "        h_a = gnn_model(batch.x_a, batch.edge_index_a, batch.edge_attr_a, batch.x_a_batch)\n",
    "        h_b = gnn_model(batch.x_b, batch.edge_index_b, batch.edge_attr_b, batch.x_b_batch)\n",
    "        \n",
    "#         print(\"h_a shape:\", h_a.shape)\n",
    "        \n",
    "#         z_a, fattn_w_scores_a = transformer_model(h_a)\n",
    "#         z_b, fattn_w_scores_b = transformer_model(h_b)\n",
    "        \n",
    "#         z_e = expression_model(batch.expression.type(fdtype))\n",
    "        \n",
    "        transformer_input = torch.unsqueeze(batch.expression.type(fdtype), dim=1)\n",
    "        z_e, fattn_w_scores_e = transformer_model(transformer_input)\n",
    "        \n",
    "#         print(\"h_a shape:\", h_a.shape)\n",
    "#         print(\"h_b shape:\", h_b.shape)\n",
    "#         print(\"transformer_input shape:\", transformer_input.shape)\n",
    "#         print(\"z_e shape:\", z_e.shape)\n",
    "        \n",
    "#         print(\"h_a dtype:\", h_a.dtype)\n",
    "#         print(\"h_b dtype:\", h_b.dtype)\n",
    "#         print(\"z_e dtype:\", z_e.dtype)\n",
    "        \n",
    "#         if (i_batch == 0):\n",
    "#             print(torch.cuda.memory_summary(device=device_gpu))\n",
    "        \n",
    "        logsoftmax_scores, dist = siamese_model(h_a, h_b, z_e)\n",
    "#         out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "#         loss = criterion(out, samples_batch.y)  # Compute the loss.\n",
    "#         print(pd.Series(batch.y.cpu()).value_counts())\n",
    "        cl = loss_nlll(logsoftmax_scores, batch.y.type(torch.long))            \n",
    "        dl = loss_contrastive(dist.reshape(-1), batch.y.type(fdtype))          \n",
    "        loss = tp[\"loss_w\"]*cl + (1-tp[\"loss_w\"])*dl\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        cyc_scheduler.step() # after each batch step the scheduler\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def eval(loader, dsettype):\n",
    "    for m, m_name in models:\n",
    "        m.eval()\n",
    "        \n",
    "    pred_class = []\n",
    "    ref_class = []\n",
    "    prob_scores = []\n",
    "    \n",
    "#     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "    for i_batch, batch in enumerate(tqdm(loader, desc=\"Iteration\")):\n",
    "        batch = batch.to(device_gpu)\n",
    "#         out = model(data.x, data.edge_index, data.batch)  \n",
    "        h_a = gnn_model(batch.x_a, batch.edge_index_a, batch.edge_attr_a, batch.x_a_batch)\n",
    "        h_b = gnn_model(batch.x_b, batch.edge_index_b, batch.edge_attr_b, batch.x_b_batch)\n",
    "        \n",
    "#         z_a, fattn_w_scores_a = transformer_model(h_a)\n",
    "#         z_b, fattn_w_scores_b = transformer_model(h_b)\n",
    "        \n",
    "#         z_e = expression_model(batch.expression.type(fdtype))\n",
    "\n",
    "        transformer_input = torch.unsqueeze(batch.expression.type(fdtype), dim=1)\n",
    "        z_e, fattn_w_scores_e = transformer_model(transformer_input)\n",
    "\n",
    "        \n",
    "#         logsoftmax_scores, dist = deepadr_siamese(z_a, z_b)\n",
    "        \n",
    "#         __, y_pred_clss = torch.max(logsoftmax_scores, -1)\n",
    "#         y_pred_prob  = torch.exp(logsoftmax_scores.detach().cpu()).numpy()\n",
    "\n",
    "#         pred_class.extend(y_pred_clss.view(-1).tolist())\n",
    "#         ref_class.extend(batch.y.view(-1).tolist())\n",
    "# #         prob_scores.append(y_pred_prob)\n",
    "#         prob_scores.extend(y_pred_prob.view(-1).tolist())\n",
    "    \n",
    "        logsoftmax_scores, dist = siamese_model(h_a, h_b, z_e)\n",
    "\n",
    "        __, y_pred_clss = torch.max(logsoftmax_scores, -1)\n",
    "\n",
    "        y_pred_prob  = torch.exp(logsoftmax_scores.detach().cpu()).numpy()\n",
    "\n",
    "        pred_class.extend(y_pred_clss.view(-1).tolist())\n",
    "        ref_class.extend(batch.y.view(-1).tolist())\n",
    "        prob_scores.append(y_pred_prob)\n",
    "\n",
    "    prob_scores_arr = np.concatenate(prob_scores, axis=0)\n",
    "#     print(\"pred_class\", pred_class)\n",
    "#     print(\"ref_class\", ref_class)\n",
    "    modelscore = perfmetric_report(pred_class, ref_class, prob_scores_arr[:,1], epoch,\n",
    "                                  outlog = os.path.join(targetdata_dir_processed, dsettype + \".log\"))\n",
    "#     modelscore = perfmetric_report(pred_class, ref_class, prob_scores, epoch, \"\")        \n",
    "    \n",
    "#         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "#         correct += int((pred == samples_batch.y).sum())  # Check against ground-truth labels.\n",
    "#     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "    return modelscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_curve = []\n",
    "test_curve = []\n",
    "train_curve = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Epoch 0\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 136/136 [01:00<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 136/136 [00:12<00:00, 10.75it/s]\n",
      "Iteration: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:02<00:00,  7.70it/s]\n",
      "Iteration: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 38/38 [00:05<00:00,  6.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train':  best_epoch_indx:0\n",
      " auc:0.7014396812576996 \n",
      " apur:0.19556815370129405 \n",
      " f1:0.16164315763424894 \n",
      " precision:0.08995262752010576 \n",
      " recall:0.7961969770843491 \n",
      ", 'Validation':  best_epoch_indx:0\n",
      " auc:0.7078563491030406 \n",
      " apur:0.23096240871395757 \n",
      " f1:0.1652523447967843 \n",
      " precision:0.0919940328194928 \n",
      " recall:0.8114035087719298 \n",
      ", 'Test':  best_epoch_indx:0\n",
      " auc:0.6939782714549496 \n",
      " apur:0.1862753885468076 \n",
      " f1:0.160078173580883 \n",
      " precision:0.08905802115251557 \n",
      " recall:0.7903508771929825 \n",
      "}\n",
      "=====Epoch 1\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 136/136 [00:31<00:00,  4.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 136/136 [00:12<00:00, 11.04it/s]\n",
      "Iteration: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:01<00:00, 11.78it/s]\n",
      "Iteration:  95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████          | 36/38 [00:03<00:00, 10.05it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6781/1708039914.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(loader, dsettype)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m#     for data in loader:  # Iterate in batches over the training/test dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Iteration\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m#         out = model(data.x, data.edge_index, data.batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/graphnn/lib/python3.9/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/graphnn/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/graphnn/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/graphnn/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/graphnn/lib/python3.9/site-packages/torch_geometric/loader/dataloader.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0melem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHeteroData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             return Batch.from_data_list(batch, self.follow_batch,\n\u001b[0m\u001b[1;32m     20\u001b[0m                                         self.exclude_keys)\n\u001b[1;32m     21\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/graphnn/lib/python3.9/site-packages/torch_geometric/data/batch.py\u001b[0m in \u001b[0;36mfrom_data_list\u001b[0;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m     67\u001b[0m         Will exclude any keys given in :obj:`exclude_keys`.\"\"\"\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         batch, slice_dict, inc_dict = collate(\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mdata_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/graphnn/lib/python3.9/site-packages/torch_geometric/data/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;31m# Collate attributes into a unified representation:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             value, slices, incs = _collate(attr, values, data_list, stores,\n\u001b[0m\u001b[1;32m     86\u001b[0m                                            increment)\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/graphnn/lib/python3.9/site-packages/torch_geometric/data/collate.py\u001b[0m in \u001b[0;36m_collate\u001b[0;34m(key, values, data_list, stores, increment)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcat_dim\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for epoch in range(tp[\"num_epochs\"]):\n",
    "# for epoch in range(60,70):\n",
    "    print(\"=====Epoch {}\".format(epoch))\n",
    "    print('Training...')\n",
    "#     train(model, device, train_loader, optimizer, dataset.task_type)\n",
    "    train()\n",
    "\n",
    "    print('Evaluating...')\n",
    "#     train_perf = eval(model, device, train_loader, evaluator)\n",
    "#     valid_perf = eval(model, device, valid_loader, evaluator)\n",
    "#     test_perf = eval(model, device, test_loader, evaluator)\n",
    "    train_perf = eval(train_loader, dsettype=\"train\")\n",
    "    valid_perf = eval(valid_loader, dsettype=\"valid\")\n",
    "    test_perf = eval(test_loader, dsettype=\"test\")\n",
    "\n",
    "    print({'Train': train_perf, 'Validation': valid_perf, 'Test': test_perf})\n",
    "\n",
    "    train_curve.append(train_perf.s_aupr)\n",
    "    valid_curve.append(valid_perf.s_aupr)\n",
    "    test_curve.append(test_perf.s_aupr)\n",
    "\n",
    "# if 'classification' in dataset.task_type:\n",
    "best_val_epoch = np.argmax(np.array(valid_curve))\n",
    "best_train = max(train_curve)\n",
    "# else:\n",
    "#     best_val_epoch = np.argmin(np.array(valid_curve))\n",
    "#     best_train = min(train_curve)\n",
    "\n",
    "print('Finished training!')\n",
    "print('Best validation score: {}'.format(valid_curve[best_val_epoch]))\n",
    "print('Test score: {}'.format(test_curve[best_val_epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_curves = pd.DataFrame(np.array([train_curve, valid_curve, test_curve]).T)\n",
    "df_curves.columns = ['train', 'valid', 'test']\n",
    "df_curves.index.name = \"epoch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='epoch'>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(data=df_curves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1862753885468076]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>valid</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.195568</td>\n",
       "      <td>0.230962</td>\n",
       "      <td>0.186275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          train     valid      test\n",
       "epoch                              \n",
       "0      0.195568  0.230962  0.186275"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
